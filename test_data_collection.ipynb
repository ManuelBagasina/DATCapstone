{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from fredapi import Fred\n",
    "import praw\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(firm):\n",
    "    # Get the firm's full name\n",
    "    def get_firm_name(ticker):\n",
    "        stock = yf.Ticker(ticker)\n",
    "        return stock.info[\"longName\"]\n",
    "\n",
    "    firm_name = get_firm_name(firm)\n",
    "    firm_name = re.sub(r'[^A-Za-z0-9 ]+', '', firm_name).split(' ')[0]\n",
    "\n",
    "    ##### Historical Stock Price #####\n",
    "    stock = yf.Ticker(firm)\n",
    "    data = stock.history(period=\"5y\")\n",
    "    df_stock = pd.DataFrame(data)\n",
    "    df_stock.index = df_stock.index.date\n",
    "    df_stock.index.name = None\n",
    "    print(f\"{firm}: Historical Stock Price collected\")\n",
    "\n",
    "    ##### Macroeconomic Indicators #####\n",
    "    api_key = \"d6ed01a1d424d730c0a92819f41f4c79\"\n",
    "    fred = Fred(api_key=api_key)\n",
    "\n",
    "    indicators = {\n",
    "        \"GDP (Billions USD)\": \"GDP\",\n",
    "        \"Unemployment Rate (%)\": \"UNRATE\",\n",
    "        \"Producer Price Index (PPI)\": \"PPIACO\",\n",
    "        \"Retail Sales (Millions USD)\": \"RSAFS\",\n",
    "        \"Industrial Production Index\": \"INDPRO\",\n",
    "        \"Housing Starts (Thousands)\": \"HOUST\",\n",
    "        \"Personal Consumption Expenditures (PCE)\": \"PCE\",\n",
    "        \"Trade Balance (Billions USD)\": \"BOPGSTB\",\n",
    "        \"M2 Money Supply (Billions USD)\": \"M2\",\n",
    "        \"Consumer Confidence Index\": \"UMCSENT\",\n",
    "    }\n",
    "\n",
    "    data = {name: fred.get_series(code) for name, code in indicators.items()}\n",
    "    df_macro = pd.DataFrame(data)\n",
    "    print(f\"{firm}: Macroeconomic Indicators collected\")\n",
    "\n",
    "    ##### Microeconomic Indicators #####\n",
    "    keep_cols = [\n",
    "        \"Total Revenue\", \"Operating Income\", \"Net Income\", \"EBITDA\", \"Gross Profit\", \"Interest Expense\",\n",
    "        \"Total Assets\", \"Total Liabilities Net Minority Interest\", \"Stockholders Equity\",\n",
    "        \"Cash And Cash Equivalents\", \"Accounts Receivable\", \"Inventory\", \"Current Assets\", \n",
    "        \"Current Liabilities\", \"Long Term Debt\",\n",
    "        \"Operating Cash Flow\", \"Capital Expenditure\", \"Free Cash Flow\", \n",
    "        \"Depreciation And Amortization\", \"Repurchase Of Capital Stock\"\n",
    "    ]\n",
    "    \n",
    "    def select_columns(df):\n",
    "        return df[[col for col in keep_cols if col in df.columns]]\n",
    "    \n",
    "    financials = select_columns(stock.financials.T)\n",
    "    balance_sheet = select_columns(stock.balance_sheet.T)\n",
    "    cash_flow = select_columns(stock.cashflow.T)\n",
    "    \n",
    "    df_micro = pd.concat([financials, balance_sheet, cash_flow], axis=1)\n",
    "    \n",
    "    print(f\"{firm}: Microeconomic Indicators collected\")\n",
    "\n",
    "    ##### Reddit (동기 방식) #####\n",
    "    def fetch_reddit_sync(firm_name, subreddit_name, sort_method=\"new\", limit=500):\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=\"ardOQiL60Y2K7KF0V_WMGA\",\n",
    "            client_secret=\"oyNdfuaDlVeSwV7qmgSRP5bFcYru-Q\",\n",
    "            user_agent=\"my_reddit_scraper\"\n",
    "        )\n",
    "\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        posts = subreddit.search(firm_name, limit=limit, sort=sort_method, time_filter='all')\n",
    "\n",
    "        data = []\n",
    "        for post in posts:\n",
    "            data.append({\n",
    "                \"Date Posted\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d'),\n",
    "                \"Title\": post.title,\n",
    "                \"Body\": post.selftext,\n",
    "                \"Vote\": post.score,\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    subreddit_lists = [\"wallstreetbets\", \"stocks\", \"investing\", \"StockMarket\"]\n",
    "    dataframes = []\n",
    "    for subreddit in subreddit_lists:\n",
    "        df = fetch_reddit_sync(\"$\" + firm, subreddit)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    df_reddit = pd.concat(dataframes, ignore_index=True)\n",
    "    df_reddit.drop_duplicates(inplace=True)\n",
    "    df_reddit = df_reddit.sort_values(by='Date Posted', ascending=False).reset_index(drop=True)\n",
    "    df_reddit['Date Posted'] = pd.to_datetime(df_reddit['Date Posted'])\n",
    "    df_reddit.set_index('Date Posted', inplace=True)\n",
    "    df_reddit.index.name = None\n",
    "    print(f\"{firm}: Reddit Data collected\")\n",
    "\n",
    "    ##### Merge stock, macro, micro #####\n",
    "    df_stock.index = pd.to_datetime(df_stock.index)\n",
    "    df_stock = df_stock.reset_index().rename(columns={\"index\": \"Date\"})\n",
    "\n",
    "    df_macro.index = pd.to_datetime(df_macro.index)\n",
    "    df_macro = df_macro.sort_index().ffill().dropna()\n",
    "    df_macro = df_macro.resample(\"D\").ffill().reset_index().rename(columns={\"index\": \"Date\"})\n",
    "\n",
    "    df_micro.index = pd.to_datetime(df_micro.index)\n",
    "    df_micro = df_micro.sort_index()\n",
    "    df_micro = df_micro.drop(df_micro.index[0])\n",
    "    df_micro.ffill(inplace=True)\n",
    "    df_micro = df_micro.resample(\"D\").ffill().reset_index().rename(columns={\"index\": \"Date\"})\n",
    "\n",
    "    # Expand df_micro to 2024-12-31\n",
    "    last_date = df_micro[\"Date\"].max()\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), end=df_stock['Date'].max(), freq=\"D\")\n",
    "    future_df = pd.DataFrame({\"Date\": future_dates})\n",
    "    df_micro = pd.concat([df_micro, future_df], ignore_index=True).ffill()\n",
    "\n",
    "    merged_df = df_stock.merge(df_macro, on='Date', how='inner')\n",
    "    merged_df = merged_df.merge(df_micro, on='Date', how='inner')\n",
    "    merged_df.set_index('Date', inplace=True)\n",
    "    merged_df.reset_index(inplace=True)\n",
    "\n",
    "    # Filtering and ratios\n",
    "    # merged_df = merged_df[(merged_df[\"Date\"] >= \"2021-09-30\") & (merged_df[\"Date\"] <= \"2024-12-31\")]\n",
    "    merged_df['ROA'] = merged_df['Net Income'] / merged_df['Total Assets']\n",
    "    merged_df['Current_Ratio'] = merged_df['Current Assets'] / merged_df['Current Liabilities']\n",
    "\n",
    "    ##### Word Embdeeing & Sentiment analysis #####\n",
    "\n",
    "    # Load FinBERT models\n",
    "    model_name = \"yiyanghkust/finbert-tone\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    embedding_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "    # 예시: 데이터 불러오기\n",
    "    df = df_reddit  # 또는 DataFrame이 이미 있다면 생략\n",
    "\n",
    "    # 텍스트 결합\n",
    "    df[\"text\"] = df[\"Title\"].fillna(\"\") + \" \" + df[\"Body\"].fillna(\"\")\n",
    "\n",
    "    # 결과 저장용 리스트\n",
    "    sentiment_list = []\n",
    "    embedding_list = []\n",
    "\n",
    "    # GPU 사용 가능할 시\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sentiment_model.to(device)\n",
    "    embedding_model.to(device)\n",
    "\n",
    "    # 인퍼런스 함수\n",
    "    def process(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 감성\n",
    "            sent_out = sentiment_model(**inputs)\n",
    "            probs = F.softmax(sent_out.logits, dim=-1)[0].cpu().numpy()\n",
    "            sentiment = dict(zip(labels, probs))\n",
    "\n",
    "            # 임베딩\n",
    "            emb_out = embedding_model(**inputs)\n",
    "            cls_vec = emb_out.last_hidden_state[:, 0, :].squeeze().cpu().numpy()  # CLS 토큰\n",
    "\n",
    "        return sentiment, cls_vec\n",
    "\n",
    "    # tqdm으로 진행상황 보기\n",
    "    for text in df[\"text\"]:\n",
    "        sentiment, embedding = process(text)\n",
    "        sentiment_list.append(sentiment)\n",
    "        embedding_list.append(embedding)\n",
    "\n",
    "    # 결과 추가\n",
    "    df[\"sentiment\"] = sentiment_list\n",
    "    df[\"embedding\"] = embedding_list\n",
    "    print(f'{firm}: Word Embedding & Sentiment Analysis completed')\n",
    "    \n",
    "    # 1. 임베딩 분해\n",
    "    embedding_array = np.vstack(df[\"embedding\"].values)\n",
    "    embedding_df = pd.DataFrame(embedding_array, columns=[f\"emb_{i}\" for i in range(embedding_array.shape[1])])\n",
    "\n",
    "    # 2. 감성 점수 분해\n",
    "    sentiment_df = pd.DataFrame(df[\"sentiment\"].tolist())\n",
    "\n",
    "    # 3. Vote 가져오기\n",
    "    vote_col = df[\"Vote\"].reset_index(drop=True)\n",
    "\n",
    "    # 4. 날짜 (혹은 Title에서 날짜 파싱)\n",
    "    df[\"Date\"] = pd.to_datetime(df.index) \n",
    "    # 5. 모두 합치기\n",
    "    features_df = pd.concat([df[[\"Date\"]].reset_index(drop=True), vote_col, sentiment_df, embedding_df], axis=1)\n",
    "    features_df.rename(columns={\"Vote\": \"vote\"}, inplace=True)\n",
    "\n",
    "    final_df = features_df.groupby(\"Date\").agg({\n",
    "        \"vote\": \"sum\",               # 그날 받은 총 vote 수\n",
    "        \"negative\": \"mean\",          # 그날의 평균 부정 감성\n",
    "        \"neutral\": \"mean\",\n",
    "        \"positive\": \"mean\",\n",
    "        **{f\"emb_{i}\": \"mean\" for i in range(768)}  # 임베딩 768차원 평균\n",
    "    }).reset_index()\n",
    "\n",
    "    reddit_cols = ['vote', 'negative', 'neutral', 'positive'] + [f\"emb_{i}\" for i in range(768)]\n",
    "    if final_df.empty:\n",
    "        final_df = pd.DataFrame(columns=[\"Date\"] + reddit_cols)\n",
    "    else:\n",
    "        for col in reddit_cols:\n",
    "            if col not in final_df.columns:\n",
    "                final_df[col] = np.nan\n",
    "\n",
    "    final_df[\"Date\"] = pd.to_datetime(final_df[\"Date\"]).dt.date\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date']).dt.date\n",
    "    \n",
    "    df = merged_df.merge(final_df, on='Date', how = 'left')\n",
    "    \n",
    "    df[\"Target_1day\"] = df[\"Close\"].shift(-1) - df['Close']\n",
    "    df[\"Target_1week\"] = df[\"Close\"].shift(-5) - df['Close']\n",
    "    df[\"Target_1month\"] = df[\"Close\"].shift(-20) - df['Close']\n",
    "    df[\"Target_1year\"] = df[\"Close\"].shift(-250) - df['Close']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(firm):\n",
    "    # Get the firm's full name\n",
    "    def get_firm_name(ticker):\n",
    "        stock = yf.Ticker(ticker)\n",
    "        return stock.info[\"longName\"]\n",
    "\n",
    "    firm_name = get_firm_name(firm)\n",
    "    firm_name = re.sub(r'[^A-Za-z0-9 ]+', '', firm_name).split(' ')[0]\n",
    "\n",
    "    ##### Historical Stock Price #####\n",
    "    stock = yf.Ticker(firm)\n",
    "    data = stock.history(period=\"5y\")\n",
    "    df_stock = pd.DataFrame(data)\n",
    "    df_stock.index = df_stock.index.date\n",
    "    df_stock.index.name = None\n",
    "    print(f\"{firm}: Historical Stock Price collected\")\n",
    "\n",
    "    ##### Macroeconomic Indicators #####\n",
    "    api_key = \"d6ed01a1d424d730c0a92819f41f4c79\"\n",
    "    fred = Fred(api_key=api_key)\n",
    "\n",
    "    indicators = {\n",
    "        \"GDP (Billions USD)\": \"GDP\",\n",
    "        \"Unemployment Rate (%)\": \"UNRATE\",\n",
    "        \"Producer Price Index (PPI)\": \"PPIACO\",\n",
    "        \"Retail Sales (Millions USD)\": \"RSAFS\",\n",
    "        \"Industrial Production Index\": \"INDPRO\",\n",
    "        \"Housing Starts (Thousands)\": \"HOUST\",\n",
    "        \"Personal Consumption Expenditures (PCE)\": \"PCE\",\n",
    "        \"Trade Balance (Billions USD)\": \"BOPGSTB\",\n",
    "        \"M2 Money Supply (Billions USD)\": \"M2\",\n",
    "        \"Consumer Confidence Index\": \"UMCSENT\",\n",
    "    }\n",
    "\n",
    "    data = {name: fred.get_series(code) for name, code in indicators.items()}\n",
    "    df_macro = pd.DataFrame(data)\n",
    "    print(f\"{firm}: Macroeconomic Indicators collected\")\n",
    "\n",
    "    ##### Microeconomic Indicators #####\n",
    "    keep_cols = [\n",
    "        \"Total Revenue\", \"Operating Income\", \"Net Income\", \"EBITDA\", \"Gross Profit\", \"Interest Expense\",\n",
    "        \"Total Assets\", \"Total Liabilities Net Minority Interest\", \"Stockholders Equity\",\n",
    "        \"Cash And Cash Equivalents\", \"Accounts Receivable\", \"Inventory\", \"Current Assets\", \n",
    "        \"Current Liabilities\", \"Long Term Debt\",\n",
    "        \"Operating Cash Flow\", \"Capital Expenditure\", \"Free Cash Flow\", \n",
    "        \"Depreciation And Amortization\", \"Repurchase Of Capital Stock\"\n",
    "    ]\n",
    "    \n",
    "    def select_columns(df):\n",
    "        return df[[col for col in keep_cols if col in df.columns]]\n",
    "    \n",
    "    financials = select_columns(stock.financials.T)\n",
    "    balance_sheet = select_columns(stock.balance_sheet.T)\n",
    "    cash_flow = select_columns(stock.cashflow.T)\n",
    "    \n",
    "    df_micro = pd.concat([financials, balance_sheet, cash_flow], axis=1)\n",
    "    \n",
    "    print(f\"{firm}: Microeconomic Indicators collected\")\n",
    "\n",
    "    ##### Reddit (동기 방식) #####\n",
    "    def fetch_reddit_sync(firm_name, subreddit_name, sort_method=\"new\", limit=500):\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=\"ardOQiL60Y2K7KF0V_WMGA\",\n",
    "            client_secret=\"oyNdfuaDlVeSwV7qmgSRP5bFcYru-Q\",\n",
    "            user_agent=\"my_reddit_scraper\"\n",
    "        )\n",
    "\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        posts = subreddit.search(firm_name, limit=limit, sort=sort_method, time_filter='all')\n",
    "\n",
    "        data = []\n",
    "        for post in posts:\n",
    "            data.append({\n",
    "                \"Date Posted\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d'),\n",
    "                \"Title\": post.title,\n",
    "                \"Body\": post.selftext,\n",
    "                \"Vote\": post.score,\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    subreddit_lists = [\"wallstreetbets\", \"stocks\", \"investing\", \"StockMarket\"]\n",
    "    dataframes = []\n",
    "    for subreddit in subreddit_lists:\n",
    "        df = fetch_reddit_sync(\"$\" + firm, subreddit)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    df_reddit = pd.concat(dataframes, ignore_index=True)\n",
    "    df_reddit.drop_duplicates(inplace=True)\n",
    "    df_reddit = df_reddit.sort_values(by='Date Posted', ascending=False).reset_index(drop=True)\n",
    "    df_reddit['Date Posted'] = pd.to_datetime(df_reddit['Date Posted'])\n",
    "    df_reddit.set_index('Date Posted', inplace=True)\n",
    "    df_reddit.index.name = None\n",
    "    print(f\"{firm}: Reddit Data collected\")\n",
    "\n",
    "    ##### Merge stock, macro, micro #####\n",
    "    df_stock.index = pd.to_datetime(df_stock.index)\n",
    "    df_stock = df_stock.reset_index().rename(columns={\"index\": \"Date\"})\n",
    "\n",
    "    df_macro.index = pd.to_datetime(df_macro.index)\n",
    "    df_macro = df_macro.sort_index().ffill().dropna()\n",
    "    df_macro = df_macro.resample(\"D\").ffill().reset_index().rename(columns={\"index\": \"Date\"})\n",
    "\n",
    "    df_micro.index = pd.to_datetime(df_micro.index)\n",
    "    df_micro = df_micro.sort_index()\n",
    "    df_micro = df_micro.drop(df_micro.index[0])\n",
    "    df_micro.ffill(inplace=True)\n",
    "    df_micro = df_micro.resample(\"D\").ffill().reset_index().rename(columns={\"index\": \"Date\"})\n",
    "\n",
    "    # Expand df_micro to 2024-12-31\n",
    "    last_date = df_micro[\"Date\"].max()\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), end=df_stock['Date'].max(), freq=\"D\")\n",
    "    future_df = pd.DataFrame({\"Date\": future_dates})\n",
    "    df_micro = pd.concat([df_micro, future_df], ignore_index=True).ffill()\n",
    "\n",
    "    merged_df = df_stock.merge(df_macro, on='Date', how='inner')\n",
    "    merged_df = merged_df.merge(df_micro, on='Date', how='inner')\n",
    "    merged_df.set_index('Date', inplace=True)\n",
    "    merged_df.reset_index(inplace=True)\n",
    "\n",
    "    # Filtering and ratios\n",
    "    # merged_df = merged_df[(merged_df[\"Date\"] >= \"2021-09-30\") & (merged_df[\"Date\"] <= \"2024-12-31\")]\n",
    "    merged_df['ROA'] = merged_df['Net Income'] / merged_df['Total Assets']\n",
    "    merged_df['Current_Ratio'] = merged_df['Current Assets'] / merged_df['Current Liabilities']\n",
    "\n",
    "    ##### Word Embdeeing & Sentiment analysis #####\n",
    "\n",
    "    # Load FinBERT models\n",
    "    model_name = \"yiyanghkust/finbert-tone\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    embedding_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "    # 예시: 데이터 불러오기\n",
    "    df = df_reddit  # 또는 DataFrame이 이미 있다면 생략\n",
    "\n",
    "    # 텍스트 결합\n",
    "    df[\"text\"] = df[\"Title\"].fillna(\"\") + \" \" + df[\"Body\"].fillna(\"\")\n",
    "\n",
    "    # 결과 저장용 리스트\n",
    "    sentiment_list = []\n",
    "    embedding_list = []\n",
    "\n",
    "    # GPU 사용 가능할 시\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sentiment_model.to(device)\n",
    "    embedding_model.to(device)\n",
    "\n",
    "    # 인퍼런스 함수\n",
    "    def process(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 감성\n",
    "            sent_out = sentiment_model(**inputs)\n",
    "            probs = F.softmax(sent_out.logits, dim=-1)[0].cpu().numpy()\n",
    "            sentiment = dict(zip(labels, probs))\n",
    "\n",
    "            # 임베딩\n",
    "            emb_out = embedding_model(**inputs)\n",
    "            cls_vec = emb_out.last_hidden_state[:, 0, :].squeeze().cpu().numpy()  # CLS 토큰\n",
    "\n",
    "        return sentiment, cls_vec\n",
    "\n",
    "    # tqdm으로 진행상황 보기\n",
    "    for text in df[\"text\"]:\n",
    "        sentiment, embedding = process(text)\n",
    "        sentiment_list.append(sentiment)\n",
    "        embedding_list.append(embedding)\n",
    "\n",
    "    # 결과 추가\n",
    "    df[\"sentiment\"] = sentiment_list\n",
    "    df[\"embedding\"] = embedding_list\n",
    "    print(f'{firm}: Word Embedding & Sentiment Analysis completed')\n",
    "    \n",
    "    # 1. 임베딩 분해\n",
    "    embedding_array = np.vstack(df[\"embedding\"].values)\n",
    "    embedding_df = pd.DataFrame(embedding_array, columns=[f\"emb_{i}\" for i in range(embedding_array.shape[1])])\n",
    "\n",
    "    # 2. 감성 점수 분해\n",
    "    sentiment_df = pd.DataFrame(df[\"sentiment\"].tolist())\n",
    "\n",
    "    # 3. Vote 가져오기\n",
    "    vote_col = df[\"Vote\"].reset_index(drop=True)\n",
    "\n",
    "    # 4. 날짜 (혹은 Title에서 날짜 파싱)\n",
    "    df[\"Date\"] = pd.to_datetime(df.index) \n",
    "    # 5. 모두 합치기\n",
    "    features_df = pd.concat([df[[\"Date\"]].reset_index(drop=True), vote_col, sentiment_df, embedding_df], axis=1)\n",
    "    features_df.rename(columns={\"Vote\": \"vote\"}, inplace=True)\n",
    "\n",
    "    final_df = features_df.groupby(\"Date\").agg({\n",
    "        \"vote\": \"sum\",               # 그날 받은 총 vote 수\n",
    "        \"negative\": \"mean\",          # 그날의 평균 부정 감성\n",
    "        \"neutral\": \"mean\",\n",
    "        \"positive\": \"mean\",\n",
    "        **{f\"emb_{i}\": \"mean\" for i in range(768)}  # 임베딩 768차원 평균\n",
    "    }).reset_index()\n",
    "\n",
    "    reddit_cols = ['vote', 'negative', 'neutral', 'positive'] + [f\"emb_{i}\" for i in range(768)]\n",
    "    if final_df.empty:\n",
    "        final_df = pd.DataFrame(columns=[\"Date\"] + reddit_cols)\n",
    "    else:\n",
    "        for col in reddit_cols:\n",
    "            if col not in final_df.columns:\n",
    "                final_df[col] = np.nan\n",
    "\n",
    "    final_df[\"Date\"] = pd.to_datetime(final_df[\"Date\"]).dt.date\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date']).dt.date\n",
    "    \n",
    "    df = merged_df.merge(final_df, on='Date', how = 'left')\n",
    "    \n",
    "    df[\"Target_1day\"] = df[\"Close\"].shift(-1) - df['Close']\n",
    "    df[\"Target_1week\"] = df[\"Close\"].shift(-5) - df['Close']\n",
    "    df[\"Target_1month\"] = df[\"Close\"].shift(-20) - df['Close']\n",
    "    df[\"Target_1year\"] = df[\"Close\"].shift(-250) - df['Close']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "firms = ['AAPL', 'NVDA', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'AVGO', 'ORCL', 'AMD']\n",
    "\n",
    "for firm in firms:\n",
    "    df = collect_data(firm)\n",
    "    df['Ticker'] = firm\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "df = df.sort_values(['Date', 'Ticker'])\n",
    "df = df.reset_index(drop = True)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dat490",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
