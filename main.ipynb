{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManuelBagasina/DATCapstone/blob/Jhee/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install asyncpraw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo_S6oYZTzYm",
        "outputId": "09b06542-675a-4df4-da99-da21f68a851f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting asyncpraw\n",
            "  Downloading asyncpraw-7.8.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting aiofiles (from asyncpraw)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (3.11.12)\n",
            "Collecting aiosqlite<=0.17.0 (from asyncpraw)\n",
            "  Downloading aiosqlite-0.17.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting asyncprawcore<3,>=2.4 (from asyncpraw)\n",
            "  Downloading asyncprawcore-2.4.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting update_checker>=0.18 (from asyncpraw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (1.18.3)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from update_checker>=0.18->asyncpraw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (2025.1.31)\n",
            "Downloading asyncpraw-7.8.1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosqlite-0.17.0-py3-none-any.whl (15 kB)\n",
            "Downloading asyncprawcore-2.4.0-py3-none-any.whl (19 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: aiosqlite, aiofiles, update_checker, asyncprawcore, asyncpraw\n",
            "Successfully installed aiofiles-24.1.0 aiosqlite-0.17.0 asyncpraw-7.8.1 asyncprawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic Libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#Yahoo Finance\n",
        "import yfinance as yf"
      ],
      "metadata": {
        "id": "8NkC2TFi7bAG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Firm Selection"
      ],
      "metadata": {
        "id": "1NgNFA60XtvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Select the frim for the prediction\n",
        "firm_key = [\"AAPL\"]\n",
        "\n",
        "# Apple -> AAPL\n",
        "# Microsoft -> MSFT\n",
        "# Nvidia -> NVDA\n",
        "# Adobe -> ADVE\n",
        "# Tesla -> TSLA\n",
        "# Amazon -> AMZN\n",
        "# Netflix -> NFLX\n",
        "# Meta -> META\n",
        "# Alphabet -> GOOGL"
      ],
      "metadata": {
        "id": "vMB0d5DQRUIB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the firm's full name\n",
        "def get_firm_name(ticker):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    return stock.info[\"longName\"]  # return Full name\n",
        "\n",
        "firm_name = get_firm_name(firm)\n",
        "print(firm_name)  # Full name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "iQBzFDjDVSXK",
        "outputId": "90467162-3601-42cb-c148-bb23560dd57d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'firm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-600768fe8fa1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"longName\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# return Full name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfirm_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_firm_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirm_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Full name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'firm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "oxXd4QSVKgz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stock Price"
      ],
      "metadata": {
        "id": "h9u2ySi0Kml-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "3OnVG3BG_5RU",
        "outputId": "ab53e7a0-2f7a-4690-d117-29a0574959ae"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'firm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c0ed5a2dffd7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fetch data for a single stock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTicker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Get historical data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'firm' is not defined"
          ]
        }
      ],
      "source": [
        "# Fetch data for a single stock\n",
        "ticker = firm\n",
        "stock = yf.Ticker(ticker)\n",
        "\n",
        "# Get historical data\n",
        "data = stock.history(period=\"5y\") #I just set the period 5 years since yahoo finance api offers free microeconomic data about past 5 years\n",
        "\n",
        "# Convert to Dataframe\n",
        "df_stock = pd.DataFrame(data)\n",
        "\n",
        "# Display\n",
        "df_stock"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the closing price\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_stock['Close'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.title(f'{firm} Stock Closing Price Over Time')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "4hJUWBhwGhTf",
        "outputId": "53d81863-76af-4303-f862-15ee5e39154f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_stock' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5fffc188c26c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot the closing price\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Closing Price'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_stock' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Macroeconomic Indicators"
      ],
      "metadata": {
        "id": "K7FUdW0OT2p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fredapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMCAFs-kVPFy",
        "outputId": "a27684d7-68ac-4200-a7a7-3488dee2d2fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fredapi\n",
            "  Downloading fredapi-0.5.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fredapi) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->fredapi) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->fredapi) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->fredapi) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fredapi) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->fredapi) (1.17.0)\n",
            "Downloading fredapi-0.5.2-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: fredapi\n",
            "Successfully installed fredapi-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fredapi import Fred\n",
        "\n",
        "# API key is from https://fred.stlouisfed.org\n",
        "api_key = \"d6ed01a1d424d730c0a92819f41f4c79\"\n",
        "fred = Fred(api_key=api_key)\n",
        "\n",
        "# Macroeconomic Indicators and Their FRED Codes\n",
        "# Indicators can be added or revised\n",
        "indicators = {\n",
        "    \"GDP (Billions USD)\": \"GDP\",\n",
        "    \"Unemployment Rate (%)\": \"UNRATE\",\n",
        "    \"Producer Price Index (PPI)\": \"PPIACO\",\n",
        "    \"Retail Sales (Millions USD)\": \"RSAFS\",\n",
        "    \"Industrial Production Index\": \"INDPRO\",\n",
        "    \"Housing Starts (Thousands)\": \"HOUST\",\n",
        "    \"Personal Consumption Expenditures (PCE)\": \"PCE\",\n",
        "    \"Trade Balance (Billions USD)\": \"BOPGSTB\",\n",
        "    \"M2 Money Supply (Billions USD)\": \"M2\",\n",
        "    \"Consumer Confidence Index\": \"UMCSENT\",\n",
        "}\n",
        "\n",
        "# Fetch Data for Each Indicator\n",
        "data = {}\n",
        "for name, code in indicators.items():\n",
        "    data[name] = fred.get_series(code)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_macro = pd.DataFrame(data)\n",
        "\n",
        "# Display the data\n",
        "df_macro\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "Sx9uKEndUG8W",
        "outputId": "16fc93ef-e3e0-4e18-b168-d884363c0d5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            GDP (Billions USD)  Unemployment Rate (%)  \\\n",
              "1913-01-01                 NaN                    NaN   \n",
              "1913-02-01                 NaN                    NaN   \n",
              "1913-03-01                 NaN                    NaN   \n",
              "1913-04-01                 NaN                    NaN   \n",
              "1913-05-01                 NaN                    NaN   \n",
              "...                        ...                    ...   \n",
              "2024-09-01                 NaN                    4.1   \n",
              "2024-10-01            29700.58                    4.1   \n",
              "2024-11-01                 NaN                    4.2   \n",
              "2024-12-01                 NaN                    4.1   \n",
              "2025-01-01                 NaN                    4.0   \n",
              "\n",
              "            Producer Price Index (PPI)  Retail Sales (Millions USD)  \\\n",
              "1913-01-01                      12.100                          NaN   \n",
              "1913-02-01                      12.000                          NaN   \n",
              "1913-03-01                      12.000                          NaN   \n",
              "1913-04-01                      12.000                          NaN   \n",
              "1913-05-01                      11.900                          NaN   \n",
              "...                                ...                          ...   \n",
              "2024-09-01                     252.682                     716388.0   \n",
              "2024-10-01                     253.081                     720393.0   \n",
              "2024-11-01                     253.229                     725079.0   \n",
              "2024-12-01                     253.590                     730300.0   \n",
              "2025-01-01                     257.302                     723853.0   \n",
              "\n",
              "            Industrial Production Index  Housing Starts (Thousands)  \\\n",
              "1913-01-01                          NaN                         NaN   \n",
              "1913-02-01                          NaN                         NaN   \n",
              "1913-03-01                          NaN                         NaN   \n",
              "1913-04-01                          NaN                         NaN   \n",
              "1913-05-01                          NaN                         NaN   \n",
              "...                                 ...                         ...   \n",
              "2024-09-01                     102.5873                      1355.0   \n",
              "2024-10-01                     102.1219                      1344.0   \n",
              "2024-11-01                     101.9736                      1305.0   \n",
              "2024-12-01                     102.9833                      1515.0   \n",
              "2025-01-01                     103.5110                      1366.0   \n",
              "\n",
              "            Personal Consumption Expenditures (PCE)  \\\n",
              "1913-01-01                                      NaN   \n",
              "1913-02-01                                      NaN   \n",
              "1913-03-01                                      NaN   \n",
              "1913-04-01                                      NaN   \n",
              "1913-05-01                                      NaN   \n",
              "...                                             ...   \n",
              "2024-09-01                                  20044.1   \n",
              "2024-10-01                                  20134.5   \n",
              "2024-11-01                                  20253.6   \n",
              "2024-12-01                                  20387.2   \n",
              "2025-01-01                                      NaN   \n",
              "\n",
              "            Trade Balance (Billions USD)  M2 Money Supply (Billions USD)  \\\n",
              "1913-01-01                           NaN                             NaN   \n",
              "1913-02-01                           NaN                             NaN   \n",
              "1913-03-01                           NaN                             NaN   \n",
              "1913-04-01                           NaN                             NaN   \n",
              "1913-05-01                           NaN                             NaN   \n",
              "...                                  ...                             ...   \n",
              "2024-09-01                      -84333.0                             NaN   \n",
              "2024-10-01                      -74153.0                             NaN   \n",
              "2024-11-01                      -78940.0                             NaN   \n",
              "2024-12-01                      -98431.0                             NaN   \n",
              "2025-01-01                           NaN                             NaN   \n",
              "\n",
              "            Consumer Confidence Index  \n",
              "1913-01-01                        NaN  \n",
              "1913-02-01                        NaN  \n",
              "1913-03-01                        NaN  \n",
              "1913-04-01                        NaN  \n",
              "1913-05-01                        NaN  \n",
              "...                               ...  \n",
              "2024-09-01                       70.1  \n",
              "2024-10-01                       70.5  \n",
              "2024-11-01                       71.8  \n",
              "2024-12-01                       74.0  \n",
              "2025-01-01                        NaN  \n",
              "\n",
              "[3377 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e185f05-12cc-44d8-9af7-2b030c05fd3c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GDP (Billions USD)</th>\n",
              "      <th>Unemployment Rate (%)</th>\n",
              "      <th>Producer Price Index (PPI)</th>\n",
              "      <th>Retail Sales (Millions USD)</th>\n",
              "      <th>Industrial Production Index</th>\n",
              "      <th>Housing Starts (Thousands)</th>\n",
              "      <th>Personal Consumption Expenditures (PCE)</th>\n",
              "      <th>Trade Balance (Billions USD)</th>\n",
              "      <th>M2 Money Supply (Billions USD)</th>\n",
              "      <th>Consumer Confidence Index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1913-01-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1913-02-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1913-03-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1913-04-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1913-05-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.900</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-09-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4.1</td>\n",
              "      <td>252.682</td>\n",
              "      <td>716388.0</td>\n",
              "      <td>102.5873</td>\n",
              "      <td>1355.0</td>\n",
              "      <td>20044.1</td>\n",
              "      <td>-84333.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>70.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-10-01</th>\n",
              "      <td>29700.58</td>\n",
              "      <td>4.1</td>\n",
              "      <td>253.081</td>\n",
              "      <td>720393.0</td>\n",
              "      <td>102.1219</td>\n",
              "      <td>1344.0</td>\n",
              "      <td>20134.5</td>\n",
              "      <td>-74153.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>70.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-11-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4.2</td>\n",
              "      <td>253.229</td>\n",
              "      <td>725079.0</td>\n",
              "      <td>101.9736</td>\n",
              "      <td>1305.0</td>\n",
              "      <td>20253.6</td>\n",
              "      <td>-78940.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>71.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-12-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4.1</td>\n",
              "      <td>253.590</td>\n",
              "      <td>730300.0</td>\n",
              "      <td>102.9833</td>\n",
              "      <td>1515.0</td>\n",
              "      <td>20387.2</td>\n",
              "      <td>-98431.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>74.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-01-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>257.302</td>\n",
              "      <td>723853.0</td>\n",
              "      <td>103.5110</td>\n",
              "      <td>1366.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3377 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e185f05-12cc-44d8-9af7-2b030c05fd3c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6e185f05-12cc-44d8-9af7-2b030c05fd3c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6e185f05-12cc-44d8-9af7-2b030c05fd3c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-606be745-f50e-494b-b51c-9e785d70ff6e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-606be745-f50e-494b-b51c-9e785d70ff6e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-606be745-f50e-494b-b51c-9e785d70ff6e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_33c35381-a051-44de-a1a8-6a40d4419990\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_macro')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_33c35381-a051-44de-a1a8-6a40d4419990 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_macro');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_macro",
              "summary": "{\n  \"name\": \"df_macro\",\n  \"rows\": 3377,\n  \"fields\": [\n    {\n      \"column\": \"GDP (Billions USD)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7776.853522903152,\n        \"min\": 243.164,\n        \"max\": 29700.58,\n        \"num_unique_values\": 312,\n        \"samples\": [\n          11923.447,\n          271.351,\n          555.545\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unemployment Rate (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7089488071636412,\n        \"min\": 2.5,\n        \"max\": 14.8,\n        \"num_unique_values\": 83,\n        \"samples\": [\n          2.9,\n          3.4,\n          5.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Producer Price Index (PPI)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 70.3756922253984,\n        \"min\": 10.3,\n        \"max\": 280.251,\n        \"num_unique_values\": 716,\n        \"samples\": [\n          24.7,\n          100.3,\n          22.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Retail Sales (Millions USD)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 146980.658418889,\n        \"min\": 163721.0,\n        \"max\": 730300.0,\n        \"num_unique_values\": 397,\n        \"samples\": [\n          279504.0,\n          441895.0,\n          389853.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Industrial Production Index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 34.99267961737422,\n        \"min\": 3.6827,\n        \"max\": 104.1038,\n        \"num_unique_values\": 1092,\n        \"samples\": [\n          45.247,\n          9.0857,\n          6.8277\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Housing Starts (Thousands)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 379.5585531204913,\n        \"min\": 478.0,\n        \"max\": 2494.0,\n        \"num_unique_values\": 598,\n        \"samples\": [\n          1769.0,\n          1687.0,\n          1095.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Personal Consumption Expenditures (PCE)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5317.506257452179,\n        \"min\": 306.1,\n        \"max\": 20387.2,\n        \"num_unique_values\": 790,\n        \"samples\": [\n          7212.9,\n          360.4,\n          1176.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Trade Balance (Billions USD)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21301.57636216705,\n        \"min\": -101914.0,\n        \"max\": -831.0,\n        \"num_unique_values\": 396,\n        \"samples\": [\n          -13949.0,\n          -33903.0,\n          -45332.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"M2 Money Supply (Billions USD)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4195.715461324616,\n        \"min\": 1591.4,\n        \"max\": 19420.1,\n        \"num_unique_values\": 2069,\n        \"samples\": [\n          14321.5,\n          1778.4,\n          9265.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Consumer Confidence Index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.941537354716363,\n        \"min\": 50.0,\n        \"max\": 112.0,\n        \"num_unique_values\": 356,\n        \"samples\": [\n          106.5,\n          91.2,\n          103.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Microeconomic Indicators"
      ],
      "metadata": {
        "id": "Q_7f_80wT8Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch company data\n",
        "stock = yf.Ticker(firm)\n",
        "\n",
        "# Fetch Financial Statements\n",
        "financials = stock.financials.T  # Income Statement (Revenue, Profit, Margins)\n",
        "balance_sheet = stock.balance_sheet.T  # Assets, Liabilities, Equity\n",
        "cash_flow = stock.cashflow.T  # Cash from Operations, Investments, Financing\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_financials = pd.DataFrame(financials)\n",
        "df_balance_sheet = pd.DataFrame(balance_sheet)\n",
        "df_cash_flow = pd.DataFrame(cash_flow)\n",
        "\n",
        "df_micro = pd.concat([df_financials, df_balance_sheet, df_cash_flow], axis=1)\n",
        "df_micro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "weB2wMUnUHWY",
        "outputId": "e7f47f84-3370-4fb4-d979-ebd69b61cc40"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'firm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-65bf01bdf12d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fetch company data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTicker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fetch Financial Statements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfinancials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinancials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m  \u001b[0;31m# Income Statement (Revenue, Profit, Margins)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'firm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_micro.columns.tolist() #We can choose the moste relevent indicators"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "tjPEqC2DcEvp",
        "outputId": "48be5d09-5df4-466b-fd2a-31730320f4bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_micro' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-500ff5f3017e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_micro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#We can choose the moste relevent indicators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_micro' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis"
      ],
      "metadata": {
        "id": "oVFq3W_bT8Hv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### yhfinance web scrapping"
      ],
      "metadata": {
        "id": "vptkgbsOGNED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Yahoo Finance web scrapping initialization\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = f\"https://finance.yahoo.com/quote/{firm}/news\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# print(soup.prettify())"
      ],
      "metadata": {
        "id": "NAfabl_mDLGo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "4c015f98-b3c9-44a9-84c9-ce114d0bfadf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'firm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-aac11238c1c1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://finance.yahoo.com/quote/{firm}/news\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m headers = {\n",
            "\u001b[0;31mNameError\u001b[0m: name 'firm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find Articles Title\n",
        "articles_title = soup.find_all(\"h3\", class_=\"clamp\")\n",
        "\n",
        "# Find Posted times\n",
        "articles_time = soup.find_all(class_=\"publishing yf-1weyqlp\")\n",
        "\n",
        "# Find Articles Summaries\n",
        "articles_summary = soup.find_all(\"p\", class_=\"yf-82qtw3\")\n",
        "\n",
        "# Make list for titles\n",
        "article_title_list = []\n",
        "for article_title in articles_title:\n",
        "    article_title_list.append(article_title.text)\n",
        "\n",
        "# Make list for times\n",
        "article_time_list = []\n",
        "for article_time in articles_time:\n",
        "    article_time_list.append(article_time.text.split(\"•\")[1].strip())\n",
        "\n",
        "# Make list for summaries\n",
        "article_summary_list = []\n",
        "for article_summary in articles_summary:\n",
        "    article_summary_list.append(article_summary.text)\n",
        "\n",
        "#Dataframe\n",
        "yh_article_titles_df = pd.DataFrame({\"Time\": article_time_list, \"Title\": article_title_list, \"Summary\": article_summary_list})\n",
        "\n",
        "yh_article_titles_df"
      ],
      "metadata": {
        "id": "VxOQ5sMhThJy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "073df545-2ebd-49d6-cb27-76e90993a74d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'soup' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f67ad7786060>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Find Articles Title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marticles_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"clamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Find Posted times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0marticles_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"publishing yf-1weyqlp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'soup' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncpraw\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import datetime\n",
        "import random\n",
        "import time\n",
        "\n",
        "nest_asyncio.apply()  # Prevent async loop errors\n",
        "\n",
        "# Stock-related keywords\n",
        "# Relevant stock market subreddits\n",
        "stock_subreddits = [\"stocks\", \"StockMarket\"]\n",
        "\n",
        "# Reddit API Function to Get 5 Random Posts Per Day for the Past 14 Days\n",
        "async def fetch_stock_news_past_14_days():\n",
        "    reddit = asyncpraw.Reddit(\n",
        "        client_id=\"ardOQiL60Y2K7KF0V_WMGA\",\n",
        "        client_secret=\"oyNdfuaDlVeSwV7qmgSRP5bFcYru-Q\",\n",
        "        user_agent=\"my_reddit_scraper\"\n",
        "    )\n",
        "\n",
        "    today = datetime.datetime.utcnow()\n",
        "    start_date = today - datetime.timedelta(days=180)  # Start 13 days ago, includes today\n",
        "\n",
        "    data = []\n",
        "\n",
        "    print(f\" Fetching data from {start_date.strftime('%Y-%m-%d')} to {today.strftime('%Y-%m-%d')}...\")\n",
        "\n",
        "    for firm_name in firm_key:  # Iterate over each company\n",
        "        total_found = 0\n",
        "\n",
        "        for day_offset in range(14):  # Loop through the past 14 days\n",
        "            search_date = start_date + datetime.timedelta(days=day_offset)\n",
        "            next_day = search_date + datetime.timedelta(days=1)\n",
        "\n",
        "            for subreddit_name in stock_subreddits:\n",
        "                print(f\" Fetching {firm_name} posts from r/{subreddit_name} on {search_date.strftime('%Y-%m-%d')}...\")\n",
        "\n",
        "                try:\n",
        "                    subreddit_obj = await reddit.subreddit(subreddit_name)\n",
        "                except Exception as e:\n",
        "                    print(f\" Error accessing subreddit {subreddit_name}: {e}\")\n",
        "                    continue  # Skip to the next subreddit if an error occurs\n",
        "\n",
        "                found_posts = []\n",
        "\n",
        "                async for post in subreddit_obj.search(\n",
        "                    query=firm_name,\n",
        "                    sort=\"relevance\",\n",
        "                    time_filter=\"year\",  # Searching across a year but filtering manually\n",
        "                    limit=100\n",
        "                ):\n",
        "                    if not post or not post.title:  # Handle cases where API returns None\n",
        "                        continue\n",
        "\n",
        "                    post_date = datetime.datetime.utcfromtimestamp(post.created_utc)\n",
        "\n",
        "                    # Ensure the post belongs to the correct day\n",
        "                    if search_date.date() <= post_date.date() < next_day.date():\n",
        "                        found_posts.append({\n",
        "                            \"Company\": firm_name,\n",
        "                            \"Subreddit\": subreddit_name,\n",
        "                            \"Title\": post.title,\n",
        "                            \"Score\": post.score,\n",
        "                            \"URL\": post.url.strip(),\n",
        "                            \"Date\": post_date.strftime('%Y-%m-%d'),\n",
        "                            \"Content\": post.selftext[:300]  # Limit content to 300 characters\n",
        "                        })\n",
        "\n",
        "                await asyncio.sleep(1.5)  # Prevent rate-limiting\n",
        "\n",
        "                # Ensure we don't sample more than the available number of posts\n",
        "                if len(found_posts) > 5:\n",
        "                    found_posts = random.sample(found_posts, 5)  # Select 5 at random\n",
        "                elif len(found_posts) > 0:\n",
        "                    found_posts = found_posts  # Take all available posts\n",
        "                else:\n",
        "                    continue  # Skip if no posts were found\n",
        "\n",
        "                data.extend(found_posts)\n",
        "                total_found += len(found_posts)\n",
        "\n",
        "        print(f\" {firm_name}: Total collected over 180 days = {total_found} posts\\n\")\n",
        "\n",
        "    await reddit.close()\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\" No data was collected for the past 180 days. Check API request & filters.\")\n",
        "    else:\n",
        "        print(\"\\n Data Summary:\")\n",
        "        print(df[\"Company\"].value_counts())\n",
        "        print(df[\"Date\"].value_counts())\n",
        "        print(df.sample(min(10, len(df))))  # Show up to 10 random samples\n",
        "\n",
        "    return df\n",
        "\n",
        "# Execution\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(fetch_stock_news_past_14_days())"
      ],
      "metadata": {
        "id": "qDbY3LbvE9H8",
        "outputId": "eea49647-2bc8-438d-8078-1de245b881a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Fetching data from 2024-08-24 to 2025-02-20...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-08-24...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-08-24...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-08-25...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-08-25...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-08-26...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-08-26...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-08-27...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-08-27...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-08-28...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-08-28...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-08-29...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-08-29...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-08-30...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-08-30...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-08-31...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-08-31...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-09-01...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-09-01...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-09-02...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:asyncprawcore:Retrying due to 503 status: GET https://oauth.reddit.com/r/stocks/search/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-09-02...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-09-03...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-09-03...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-09-04...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-09-04...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-09-05...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-09-05...\n",
            "🔄 Fetching AAPL posts from r/stocks on 2024-09-06...\n",
            "🔄 Fetching AAPL posts from r/StockMarket on 2024-09-06...\n",
            "✅ AAPL: Total collected over 180 days = 6 posts\n",
            "\n",
            "\n",
            "📌 Data Summary:\n",
            "Company\n",
            "AAPL    6\n",
            "Name: count, dtype: int64\n",
            "Date\n",
            "2024-09-04    2\n",
            "2024-08-29    1\n",
            "2024-08-30    1\n",
            "2024-09-03    1\n",
            "2024-09-06    1\n",
            "Name: count, dtype: int64\n",
            "  Company    Subreddit                                              Title  \\\n",
            "2    AAPL  StockMarket                                Any helpful advice?   \n",
            "1    AAPL       stocks  r/Stocks Daily Discussion & Fundamentals Frida...   \n",
            "4    AAPL  StockMarket              Been rough here any advice for a 22 M   \n",
            "0    AAPL       stocks  r/Stocks Daily Discussion & Options Trading Th...   \n",
            "3    AAPL       stocks  [Reuters Exclusive] Intel manufacturing busine...   \n",
            "5    AAPL       stocks  r/Stocks Daily Discussion & Fundamentals Frida...   \n",
            "\n",
            "   Score                                                URL        Date  \\\n",
            "2     20             https://www.reddit.com/gallery/1f7oxjp  2024-09-03   \n",
            "1     12  https://www.reddit.com/r/stocks/comments/1f4r0...  2024-08-30   \n",
            "4      0             https://www.reddit.com/gallery/1f8m5su  2024-09-04   \n",
            "0     10  https://www.reddit.com/r/stocks/comments/1f3y5...  2024-08-29   \n",
            "3    192  https://www.reddit.com/r/stocks/comments/1f8sp...  2024-09-04   \n",
            "5     17  https://www.reddit.com/r/stocks/comments/1faaz...  2024-09-06   \n",
            "\n",
            "                                             Content  \n",
            "2  Im pretty diverse. $1000 in each company i own...  \n",
            "1  This is the daily discussion, so anything stoc...  \n",
            "4  Was up 1100 got greedy :( im considering ridin...  \n",
            "0  This is the daily discussion, so anything stoc...  \n",
            "3  Sept 4 (Reuters) - Intel's (INTC.O), contract ...  \n",
            "5  This is the daily discussion, so anything stoc...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "02.10 Ri-on: I successfully set up a system to search for articles related to a specified company and extract the title, posting time, and summary in the yahoo finance website. However, the scraping currently only captures the articles displayed at the top of the webpage. To retrieve older data, further improvements are needed."
      ],
      "metadata": {
        "id": "wc6WIJzAFeGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Reddit seb scrapping"
      ],
      "metadata": {
        "id": "Hq8TV2egW5PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncpraw  # Reddit API Library\n",
        "import asyncio  # Asynchronous execution\n",
        "import nest_asyncio  # Prevent async loop errors\n",
        "import datetime  # For date filtering\n",
        "import random  # For random sampling\n",
        "import time  # Prevent Reddit rate-limiting\n",
        "\n",
        "nest_asyncio.apply()  # Preventing async loop errors\n",
        "\n",
        "# Use only the most relevant stock market subreddits\n",
        "stock_subreddits = [\"stocks\", \"StockMarket\"]\n",
        "\n",
        "# Keywords for refining searches\n",
        "firm_keywords = {\n",
        "    \"Apple\": [\"Apple Inc\", \"AAPL\", \"iPhone\", \"Macbook\", \"Tim Cook\"],\n",
        "    \"Google\": [\"Google LLC\", \"GOOGL\", \"Alphabet\", \"Google AI\", \"Sundar Pichai\"],\n",
        "    \"Tesla\": [\"Tesla Inc\", \"TSLA\", \"Elon Musk\", \"Tesla stock\"],\n",
        "    \"Microsoft\": [\"Microsoft Corp\", \"MSFT\", \"Azure\", \"Satya Nadella\"],\n",
        "    \"Amazon\": [\"Amazon.com\", \"AMZN\", \"AWS\", \"Jeff Bezos\"],\n",
        "    \"Nvidia\": [\"Nvidia Corp\", \"NVDA\", \"GPU\", \"RTX 4090\"],\n",
        "    \"Meta\": [\"Meta Platforms\", \"META\", \"Facebook stock\", \"Mark Zuckerberg\"],\n",
        "}\n",
        "\n",
        "# Reddit API Function to Get a Year's Worth of News (500 Random Posts Per Day)\n",
        "async def fetch_stock_news_randomized():\n",
        "    reddit = asyncpraw.Reddit(\n",
        "        client_id=\"ardOQiL60Y2K7KF0V_WMGA\",\n",
        "        client_secret=\"oyNdfuaDlVeSwV7qmgSRP5bFcYru-Q\",\n",
        "        user_agent=\"my_reddit_scraper\"\n",
        "    )\n",
        "\n",
        "    # Start collecting posts from 1 year ago up to today\n",
        "    today = datetime.datetime.utcnow()\n",
        "    start_date = today - datetime.timedelta(days=365)\n",
        "\n",
        "    # Store data\n",
        "    data = []\n",
        "\n",
        "    for single_date in (start_date + datetime.timedelta(n) for n in range(365)):  # Loop through each day in the past year\n",
        "        print(f\"Fetching data for {single_date.strftime('%Y-%m-%d')}...\")\n",
        "\n",
        "        daily_posts = []  # Temporary storage for that day's posts\n",
        "\n",
        "        for firm_name, keywords in firm_keywords.items():  # Loop through all companies\n",
        "            for subreddit_name in stock_subreddits:  # Loop through selected subreddits\n",
        "                try:\n",
        "                    subreddit = await reddit.subreddit(subreddit_name)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error accessing subreddit {subreddit_name}: {e}\")\n",
        "                    continue  # Skip to the next subreddit if an error occurs\n",
        "\n",
        "                async for post in subreddit.search(\n",
        "                    query=f\"{firm_name} OR {' OR '.join(keywords)}\",\n",
        "                    sort=\"new\",\n",
        "                    time_filter=\"day\",  # Fetches posts from this single day\n",
        "                    limit=1000  # Get as many as possible, then we randomly select 500\n",
        "                ):\n",
        "                    post_date = datetime.datetime.utcfromtimestamp(post.created_utc)\n",
        "                    if post_date.date() != single_date.date():\n",
        "                        continue  # Ignore posts outside the target date\n",
        "\n",
        "                    full_url = post.url.strip()\n",
        "                    if full_url.startswith(\"https://www.reddit.com/\"):  # Ensure only valid Reddit posts\n",
        "                        daily_posts.append({\n",
        "                            \"Company\": firm_name,\n",
        "                            \"Subreddit\": subreddit_name,\n",
        "                            \"Title\": post.title,\n",
        "                            \"Score\": post.score,\n",
        "                            \"URL\": full_url, # optional\n",
        "                            \"Date\": post_date.strftime('%Y-%m-%d'),\n",
        "                            \"Content\": post.selftext[:250]  # Limit content to 250 words\n",
        "                        })\n",
        "\n",
        "                time.sleep(0.5)  # Prevents Reddit from slowing us down\n",
        "\n",
        "        # Randomly sample 50 posts from that day (or take all if less than 50 exist)\n",
        "        if len(daily_posts) > 50:\n",
        "            daily_posts = random.sample(daily_posts, 50)\n",
        "\n",
        "        # Store the sampled posts\n",
        "        data.extend(daily_posts)\n",
        "\n",
        "    await reddit.close()\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Execution\n",
        "if __name__ == \"__main__\":\n",
        "    df = asyncio.run(fetch_stock_news_randomized())  # Fetch all stock discussions for the year\n",
        "    print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liMlP2akOpmR",
        "outputId": "b368976c-4391-4c5d-c448-94e55b1be041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data for 2024-02-18...\n",
            "Fetching data for 2024-02-19...\n",
            "Fetching data for 2024-02-20...\n",
            "Fetching data for 2024-02-21...\n",
            "Fetching data for 2024-02-22...\n",
            "Fetching data for 2024-02-23...\n",
            "Fetching data for 2024-02-24...\n",
            "Fetching data for 2024-02-25...\n",
            "Fetching data for 2024-02-26...\n",
            "Fetching data for 2024-02-27...\n",
            "Fetching data for 2024-02-28...\n",
            "Fetching data for 2024-02-29...\n",
            "Fetching data for 2024-03-01...\n",
            "Fetching data for 2024-03-02...\n",
            "Fetching data for 2024-03-03...\n",
            "Fetching data for 2024-03-04...\n",
            "Fetching data for 2024-03-05...\n",
            "Fetching data for 2024-03-06...\n",
            "Fetching data for 2024-03-07...\n",
            "Fetching data for 2024-03-08...\n",
            "Fetching data for 2024-03-09...\n",
            "Fetching data for 2024-03-10...\n",
            "Fetching data for 2024-03-11...\n",
            "Fetching data for 2024-03-12...\n",
            "Fetching data for 2024-03-13...\n",
            "Fetching data for 2024-03-14...\n",
            "Fetching data for 2024-03-15...\n",
            "Fetching data for 2024-03-16...\n",
            "Fetching data for 2024-03-17...\n",
            "Fetching data for 2024-03-18...\n",
            "Fetching data for 2024-03-19...\n",
            "Fetching data for 2024-03-20...\n",
            "Fetching data for 2024-03-21...\n",
            "Fetching data for 2024-03-22...\n",
            "Fetching data for 2024-03-23...\n",
            "Fetching data for 2024-03-24...\n",
            "Fetching data for 2024-03-25...\n",
            "Fetching data for 2024-03-26...\n",
            "Fetching data for 2024-03-27...\n",
            "Fetching data for 2024-03-28...\n",
            "Fetching data for 2024-03-29...\n",
            "Fetching data for 2024-03-30...\n",
            "Fetching data for 2024-03-31...\n",
            "Fetching data for 2024-04-01...\n",
            "Fetching data for 2024-04-02...\n",
            "Fetching data for 2024-04-03...\n",
            "Fetching data for 2024-04-04...\n",
            "Fetching data for 2024-04-05...\n",
            "Fetching data for 2024-04-06...\n",
            "Fetching data for 2024-04-07...\n",
            "Fetching data for 2024-04-08...\n",
            "Fetching data for 2024-04-09...\n",
            "Fetching data for 2024-04-10...\n",
            "Fetching data for 2024-04-11...\n",
            "Fetching data for 2024-04-12...\n",
            "Fetching data for 2024-04-13...\n",
            "Fetching data for 2024-04-14...\n",
            "Fetching data for 2024-04-15...\n",
            "Fetching data for 2024-04-16...\n",
            "Fetching data for 2024-04-17...\n",
            "Fetching data for 2024-04-18...\n",
            "Fetching data for 2024-04-19...\n",
            "Fetching data for 2024-04-20...\n",
            "Fetching data for 2024-04-21...\n",
            "Fetching data for 2024-04-22...\n",
            "Fetching data for 2024-04-23...\n",
            "Fetching data for 2024-04-24...\n",
            "Fetching data for 2024-04-25...\n",
            "Fetching data for 2024-04-26...\n",
            "Fetching data for 2024-04-27...\n",
            "Fetching data for 2024-04-28...\n",
            "Fetching data for 2024-04-29...\n",
            "Fetching data for 2024-04-30...\n",
            "Fetching data for 2024-05-01...\n",
            "Fetching data for 2024-05-02...\n",
            "Fetching data for 2024-05-03...\n",
            "Fetching data for 2024-05-04...\n",
            "Fetching data for 2024-05-05...\n",
            "Fetching data for 2024-05-06...\n",
            "Fetching data for 2024-05-07...\n",
            "Fetching data for 2024-05-08...\n",
            "Fetching data for 2024-05-09...\n",
            "Fetching data for 2024-05-10...\n",
            "Fetching data for 2024-05-11...\n",
            "Fetching data for 2024-05-12...\n",
            "Fetching data for 2024-05-13...\n",
            "Fetching data for 2024-05-14...\n",
            "Fetching data for 2024-05-15...\n",
            "Fetching data for 2024-05-16...\n",
            "Fetching data for 2024-05-17...\n",
            "Fetching data for 2024-05-18...\n",
            "Fetching data for 2024-05-19...\n",
            "Fetching data for 2024-05-20...\n",
            "Fetching data for 2024-05-21...\n",
            "Fetching data for 2024-05-22...\n",
            "Fetching data for 2024-05-23...\n",
            "Fetching data for 2024-05-24...\n",
            "Fetching data for 2024-05-25...\n",
            "Fetching data for 2024-05-26...\n",
            "Fetching data for 2024-05-27...\n",
            "Fetching data for 2024-05-28...\n",
            "Fetching data for 2024-05-29...\n",
            "Fetching data for 2024-05-30...\n",
            "Fetching data for 2024-05-31...\n",
            "Fetching data for 2024-06-01...\n",
            "Fetching data for 2024-06-02...\n",
            "Fetching data for 2024-06-03...\n",
            "Fetching data for 2024-06-04...\n",
            "Fetching data for 2024-06-05...\n",
            "Fetching data for 2024-06-06...\n",
            "Fetching data for 2024-06-07...\n",
            "Fetching data for 2024-06-08...\n",
            "Fetching data for 2024-06-09...\n",
            "Fetching data for 2024-06-10...\n",
            "Fetching data for 2024-06-11...\n",
            "Fetching data for 2024-06-12...\n",
            "Fetching data for 2024-06-13...\n",
            "Fetching data for 2024-06-14...\n",
            "Fetching data for 2024-06-15...\n",
            "Fetching data for 2024-06-16...\n",
            "Fetching data for 2024-06-17...\n",
            "Fetching data for 2024-06-18...\n",
            "Fetching data for 2024-06-19...\n",
            "Fetching data for 2024-06-20...\n",
            "Fetching data for 2024-06-21...\n",
            "Fetching data for 2024-06-22...\n",
            "Fetching data for 2024-06-23...\n",
            "Fetching data for 2024-06-24...\n",
            "Fetching data for 2024-06-25...\n",
            "Fetching data for 2024-06-26...\n",
            "Fetching data for 2024-06-27...\n",
            "Fetching data for 2024-06-28...\n",
            "Fetching data for 2024-06-29...\n",
            "Fetching data for 2024-06-30...\n",
            "Fetching data for 2024-07-01...\n",
            "Fetching data for 2024-07-02...\n",
            "Fetching data for 2024-07-03...\n",
            "Fetching data for 2024-07-04...\n",
            "Fetching data for 2024-07-05...\n",
            "Fetching data for 2024-07-06...\n",
            "Fetching data for 2024-07-07...\n",
            "Fetching data for 2024-07-08...\n",
            "Fetching data for 2024-07-09...\n",
            "Fetching data for 2024-07-10...\n",
            "Fetching data for 2024-07-11...\n",
            "Fetching data for 2024-07-12...\n",
            "Fetching data for 2024-07-13...\n",
            "Fetching data for 2024-07-14...\n",
            "Fetching data for 2024-07-15...\n",
            "Fetching data for 2024-07-16...\n",
            "Fetching data for 2024-07-17...\n",
            "Fetching data for 2024-07-18...\n",
            "Fetching data for 2024-07-19...\n",
            "Fetching data for 2024-07-20...\n",
            "Fetching data for 2024-07-21...\n",
            "Fetching data for 2024-07-22...\n",
            "Fetching data for 2024-07-23...\n",
            "Fetching data for 2024-07-24...\n",
            "Fetching data for 2024-07-25...\n",
            "Fetching data for 2024-07-26...\n",
            "Fetching data for 2024-07-27...\n",
            "Fetching data for 2024-07-28...\n",
            "Fetching data for 2024-07-29...\n",
            "Fetching data for 2024-07-30...\n",
            "Fetching data for 2024-07-31...\n",
            "Fetching data for 2024-08-01...\n",
            "Fetching data for 2024-08-02...\n",
            "Fetching data for 2024-08-03...\n",
            "Fetching data for 2024-08-04...\n",
            "Fetching data for 2024-08-05...\n",
            "Fetching data for 2024-08-06...\n",
            "Fetching data for 2024-08-07...\n",
            "Fetching data for 2024-08-08...\n",
            "Fetching data for 2024-08-09...\n",
            "Fetching data for 2024-08-10...\n",
            "Fetching data for 2024-08-11...\n",
            "Fetching data for 2024-08-12...\n",
            "Fetching data for 2024-08-13...\n",
            "Fetching data for 2024-08-14...\n",
            "Fetching data for 2024-08-15...\n",
            "Fetching data for 2024-08-16...\n",
            "Fetching data for 2024-08-17...\n",
            "Fetching data for 2024-08-18...\n",
            "Fetching data for 2024-08-19...\n",
            "Fetching data for 2024-08-20...\n",
            "Fetching data for 2024-08-21...\n",
            "Fetching data for 2024-08-22...\n",
            "Fetching data for 2024-08-23...\n",
            "Fetching data for 2024-08-24...\n",
            "Fetching data for 2024-08-25...\n",
            "Fetching data for 2024-08-26...\n",
            "Fetching data for 2024-08-27...\n",
            "Fetching data for 2024-08-28...\n",
            "Fetching data for 2024-08-29...\n",
            "Fetching data for 2024-08-30...\n",
            "Fetching data for 2024-08-31...\n",
            "Fetching data for 2024-09-01...\n",
            "Fetching data for 2024-09-02...\n",
            "Fetching data for 2024-09-03...\n",
            "Fetching data for 2024-09-04...\n",
            "Fetching data for 2024-09-05...\n",
            "Fetching data for 2024-09-06...\n",
            "Fetching data for 2024-09-07...\n",
            "Fetching data for 2024-09-08...\n",
            "Fetching data for 2024-09-09...\n",
            "Fetching data for 2024-09-10...\n",
            "Fetching data for 2024-09-11...\n",
            "Fetching data for 2024-09-12...\n",
            "Fetching data for 2024-09-13...\n",
            "Fetching data for 2024-09-14...\n",
            "Fetching data for 2024-09-15...\n",
            "Fetching data for 2024-09-16...\n",
            "Fetching data for 2024-09-17...\n",
            "Fetching data for 2024-09-18...\n",
            "Fetching data for 2024-09-19...\n",
            "Fetching data for 2024-09-20...\n",
            "Fetching data for 2024-09-21...\n",
            "Fetching data for 2024-09-22...\n",
            "Fetching data for 2024-09-23...\n",
            "Fetching data for 2024-09-24...\n",
            "Fetching data for 2024-09-25...\n",
            "Fetching data for 2024-09-26...\n",
            "Fetching data for 2024-09-27...\n",
            "Fetching data for 2024-09-28...\n",
            "Fetching data for 2024-09-29...\n",
            "Fetching data for 2024-09-30...\n",
            "Fetching data for 2024-10-01...\n",
            "Fetching data for 2024-10-02...\n",
            "Fetching data for 2024-10-03...\n",
            "Fetching data for 2024-10-04...\n",
            "Fetching data for 2024-10-05...\n",
            "Fetching data for 2024-10-06...\n",
            "Fetching data for 2024-10-07...\n",
            "Fetching data for 2024-10-08...\n",
            "Fetching data for 2024-10-09...\n",
            "Fetching data for 2024-10-10...\n",
            "Fetching data for 2024-10-11...\n",
            "Fetching data for 2024-10-12...\n",
            "Fetching data for 2024-10-13...\n",
            "Fetching data for 2024-10-14...\n",
            "Fetching data for 2024-10-15...\n",
            "Fetching data for 2024-10-16...\n",
            "Fetching data for 2024-10-17...\n",
            "Fetching data for 2024-10-18...\n",
            "Fetching data for 2024-10-19...\n",
            "Fetching data for 2024-10-20...\n",
            "Fetching data for 2024-10-21...\n",
            "Fetching data for 2024-10-22...\n",
            "Fetching data for 2024-10-23...\n",
            "Fetching data for 2024-10-24...\n",
            "Fetching data for 2024-10-25...\n",
            "Fetching data for 2024-10-26...\n",
            "Fetching data for 2024-10-27...\n",
            "Fetching data for 2024-10-28...\n",
            "Fetching data for 2024-10-29...\n",
            "Fetching data for 2024-10-30...\n",
            "Fetching data for 2024-10-31...\n",
            "Fetching data for 2024-11-01...\n",
            "Fetching data for 2024-11-02...\n",
            "Fetching data for 2024-11-03...\n",
            "Fetching data for 2024-11-04...\n",
            "Fetching data for 2024-11-05...\n",
            "Fetching data for 2024-11-06...\n",
            "Fetching data for 2024-11-07...\n",
            "Fetching data for 2024-11-08...\n",
            "Fetching data for 2024-11-09...\n",
            "Fetching data for 2024-11-10...\n",
            "Fetching data for 2024-11-11...\n",
            "Fetching data for 2024-11-12...\n",
            "Fetching data for 2024-11-13...\n",
            "Fetching data for 2024-11-14...\n",
            "Fetching data for 2024-11-15...\n",
            "Fetching data for 2024-11-16...\n",
            "Fetching data for 2024-11-17...\n",
            "Fetching data for 2024-11-18...\n",
            "Fetching data for 2024-11-19...\n",
            "Fetching data for 2024-11-20...\n",
            "Fetching data for 2024-11-21...\n",
            "Fetching data for 2024-11-22...\n",
            "Fetching data for 2024-11-23...\n",
            "Fetching data for 2024-11-24...\n",
            "Fetching data for 2024-11-25...\n",
            "Fetching data for 2024-11-26...\n",
            "Fetching data for 2024-11-27...\n",
            "Fetching data for 2024-11-28...\n",
            "Fetching data for 2024-11-29...\n",
            "Fetching data for 2024-11-30...\n",
            "Fetching data for 2024-12-01...\n",
            "Fetching data for 2024-12-02...\n",
            "Fetching data for 2024-12-03...\n",
            "Fetching data for 2024-12-04...\n",
            "Fetching data for 2024-12-05...\n",
            "Fetching data for 2024-12-06...\n",
            "Fetching data for 2024-12-07...\n",
            "Fetching data for 2024-12-08...\n",
            "Fetching data for 2024-12-09...\n",
            "Fetching data for 2024-12-10...\n",
            "Fetching data for 2024-12-11...\n",
            "Fetching data for 2024-12-12...\n",
            "Fetching data for 2024-12-13...\n",
            "Fetching data for 2024-12-14...\n",
            "Fetching data for 2024-12-15...\n",
            "Fetching data for 2024-12-16...\n",
            "Fetching data for 2024-12-17...\n",
            "Fetching data for 2024-12-18...\n",
            "Fetching data for 2024-12-19...\n",
            "Fetching data for 2024-12-20...\n",
            "Fetching data for 2024-12-21...\n",
            "Fetching data for 2024-12-22...\n",
            "Fetching data for 2024-12-23...\n",
            "Fetching data for 2024-12-24...\n",
            "Fetching data for 2024-12-25...\n",
            "Fetching data for 2024-12-26...\n",
            "Fetching data for 2024-12-27...\n",
            "Fetching data for 2024-12-28...\n",
            "Fetching data for 2024-12-29...\n",
            "Fetching data for 2024-12-30...\n",
            "Fetching data for 2024-12-31...\n",
            "Fetching data for 2025-01-01...\n",
            "Fetching data for 2025-01-02...\n",
            "Fetching data for 2025-01-03...\n",
            "Fetching data for 2025-01-04...\n",
            "Fetching data for 2025-01-05...\n",
            "Fetching data for 2025-01-06...\n",
            "Fetching data for 2025-01-07...\n",
            "Fetching data for 2025-01-08...\n",
            "Fetching data for 2025-01-09...\n",
            "Fetching data for 2025-01-10...\n",
            "Fetching data for 2025-01-11...\n",
            "Fetching data for 2025-01-12...\n",
            "Fetching data for 2025-01-13...\n",
            "Fetching data for 2025-01-14...\n",
            "Fetching data for 2025-01-15...\n",
            "Fetching data for 2025-01-16...\n",
            "Fetching data for 2025-01-17...\n",
            "Fetching data for 2025-01-18...\n",
            "Fetching data for 2025-01-19...\n",
            "Fetching data for 2025-01-20...\n",
            "Fetching data for 2025-01-21...\n",
            "Fetching data for 2025-01-22...\n",
            "Fetching data for 2025-01-23...\n",
            "Fetching data for 2025-01-24...\n",
            "Fetching data for 2025-01-25...\n",
            "Fetching data for 2025-01-26...\n",
            "Fetching data for 2025-01-27...\n",
            "Fetching data for 2025-01-28...\n",
            "Fetching data for 2025-01-29...\n",
            "Fetching data for 2025-01-30...\n",
            "Fetching data for 2025-01-31...\n",
            "Fetching data for 2025-02-01...\n",
            "Fetching data for 2025-02-02...\n",
            "Fetching data for 2025-02-03...\n",
            "Fetching data for 2025-02-04...\n",
            "Fetching data for 2025-02-05...\n",
            "Fetching data for 2025-02-06...\n",
            "Fetching data for 2025-02-07...\n",
            "Fetching data for 2025-02-08...\n",
            "Fetching data for 2025-02-09...\n",
            "Fetching data for 2025-02-10...\n",
            "Fetching data for 2025-02-11...\n",
            "Fetching data for 2025-02-12...\n",
            "Fetching data for 2025-02-13...\n",
            "Fetching data for 2025-02-14...\n",
            "Fetching data for 2025-02-15...\n",
            "Fetching data for 2025-02-16...\n",
            "  Company    Subreddit                                              Title  \\\n",
            "0   Tesla       stocks  Did anyone buy palantir stock at IPO/DPO, held...   \n",
            "1   Tesla  StockMarket  Apple, amazing company that’s I just can't bri...   \n",
            "2   Tesla       stocks              Trying to understand AI profitability   \n",
            "3   Tesla  StockMarket  Abnormal Rise in Therapeutic Stocks (CRSP, NTL...   \n",
            "4   Tesla       stocks  Why are cloud stocks trading down despite high...   \n",
            "\n",
            "   Score                                                URL        Date  \\\n",
            "0    195  https://www.reddit.com/r/stocks/comments/1iqmp...  2025-02-16   \n",
            "1      0  https://www.reddit.com/r/StockMarket/comments/...  2025-02-16   \n",
            "2     40  https://www.reddit.com/r/stocks/comments/1iqyd...  2025-02-16   \n",
            "3      1  https://www.reddit.com/r/StockMarket/comments/...  2025-02-16   \n",
            "4    114  https://www.reddit.com/r/stocks/comments/1ir6e...  2025-02-16   \n",
            "\n",
            "                                             Content  \n",
            "0  Did anyone buy palantir stock at IPO/DPO, held...  \n",
            "1  Apple is the world’s most valuable company, th...  \n",
            "2  I’m curious if there are any experts in this f...  \n",
            "3  I recently started to dip my toes back in the ...  \n",
            "4  Hello all,\\n\\nAmazon, Microsoft, and Google ar...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test code\n",
        "\n",
        "import asyncpraw  # Reddit API Library\n",
        "import asyncio  # Asynchronous execution\n",
        "import nest_asyncio  # Prevent async loop errors\n",
        "import pandas as pd  # Handling structured data\n",
        "import datetime  # For date filtering\n",
        "import random  # For random sampling\n",
        "import time  # Prevent Reddit rate-limiting\n",
        "\n",
        "nest_asyncio.apply()  # Preventing async loop errors\n",
        "\n",
        "# ✅ Stock-related keywords\n",
        "firm_keywords = {\n",
        "    \"Apple\": [\"Apple\", \"AAPL\", \"iPhone\", \"Macbook\", \"Tim Cook\"]\n",
        "}\n",
        "\n",
        "# ✅ Relevant stock market subreddits\n",
        "stock_subreddits = [\"stocks\", \"StockMarket\", \"investing\", \"finance\"]\n",
        "\n",
        "# ✅ Reddit API Function to Get a Full Year of News (500 Random Posts Per Day)\n",
        "async def fetch_stock_news_randomized():\n",
        "    reddit = asyncpraw.Reddit(\n",
        "        client_id=\"ardOQiL60Y2K7KF0V_WMGA\",\n",
        "        client_secret=\"oyNdfuaDlVeSwV7qmgSRP5bFcYru-Q\",\n",
        "        user_agent=\"my_reddit_scraper\"\n",
        "    )\n",
        "\n",
        "    today = datetime.datetime.utcnow()\n",
        "    start_date = today - datetime.timedelta(days=365)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    print(f\"🔍 Fetching data from {start_date.strftime('%Y-%m-%d')} to {today.strftime('%Y-%m-%d')}...\")\n",
        "\n",
        "    for firm_name, keywords in firm_keywords.items():\n",
        "        total_found = 0\n",
        "\n",
        "        for subreddit_name in stock_subreddits:\n",
        "            print(f\"🔄 Fetching Reddit API for {firm_name} in r/{subreddit_name}...\")\n",
        "\n",
        "            try:\n",
        "                subreddit_obj = await reddit.subreddit(subreddit_name)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error accessing subreddit {subreddit_name}: {e}\")\n",
        "                continue  # ✅ Skip to the next subreddit if an error occurs\n",
        "\n",
        "            after = None  # ✅ Pagination key\n",
        "            found_posts = []\n",
        "\n",
        "            while len(found_posts) < 5000:  # ✅ Collect up to 5000 posts total\n",
        "                params = {\"after\": after} if after else {}  # ✅ Only add `after` if valid\n",
        "\n",
        "                async for post in subreddit_obj.search(\n",
        "                    query=f'\"{firm_name}\" OR {\" OR \".join(keywords)}',\n",
        "                    sort=\"new\",\n",
        "                    time_filter=\"year\",  # ✅ Get posts from the past year\n",
        "                    limit=100\n",
        "                ):\n",
        "                    if not post or not post.title:  # ✅ Handle cases where API returns None\n",
        "                        continue\n",
        "\n",
        "                    post_date = datetime.datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d')\n",
        "                    found_posts.append({\n",
        "                        \"Company\": firm_name,\n",
        "                        \"Subreddit\": subreddit_name,\n",
        "                        \"Title\": post.title,\n",
        "                        \"Score\": post.score,\n",
        "                        \"URL\": post.url.strip(),\n",
        "                        \"Date\": post_date,\n",
        "                        \"Content\": post.selftext[:500]  # ✅ Limit content to 500 words\n",
        "                    })\n",
        "\n",
        "                    after = post.id  # ✅ Get next batch\n",
        "\n",
        "                await asyncio.sleep(1.0)  # ✅ Prevent rate-limiting\n",
        "\n",
        "                if not after or len(found_posts) >= 5000:  # ✅ Stop after 5000 posts\n",
        "                    break\n",
        "\n",
        "            print(f\"🔹 {firm_name}: Found {len(found_posts)} posts in r/{subreddit_name}.\")\n",
        "\n",
        "            total_found += len(found_posts)\n",
        "\n",
        "            # ✅ Ensure we have 500 posts per day\n",
        "            if len(found_posts) > 500:\n",
        "                found_posts = random.sample(found_posts, 500)\n",
        "\n",
        "            data.extend(found_posts)\n",
        "\n",
        "        print(f\" {firm_name}: Total collected = {total_found} posts\\n\")\n",
        "\n",
        "    await reddit.close()\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"❌ No data was collected. Check API request & filters.\")\n",
        "    else:\n",
        "        print(\"\\n📌 Data Summary:\")\n",
        "        print(df[\"Company\"].value_counts())\n",
        "        print(df[\"Date\"].value_counts())\n",
        "        print(df.sample(10))\n",
        "\n",
        "    return df\n",
        "\n",
        "# ✅ Execution\n",
        "if __name__ == \"__main__\":\n",
        "    df = asyncio.run(fetch_stock_news_randomized())"
      ],
      "metadata": {
        "id": "8QT1LyWY_tZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "02.10 Ri-on: Added draft for web scrapping reddit posts"
      ],
      "metadata": {
        "id": "urwTi8oFXA7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data Merging (Sentiment Analysis)"
      ],
      "metadata": {
        "id": "7YWSK2Y5dzfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Text Quantification"
      ],
      "metadata": {
        "id": "rWHSp9rPdj14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Merging"
      ],
      "metadata": {
        "id": "ffRqNCtOiVk_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_8smSGliX70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "o12hDvJWc0Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ce8JtHjb7HQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "BA5ZadNmdCMC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PZTEqc0NdD44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "Uiry4VlIdHB7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Mewr0mOdJHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "UPyjdmvheM9a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUA98-uieOZp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}