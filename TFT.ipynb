{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManuelBagasina/DATCapstone/blob/ML-Manuel/TFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "KX0C1Yudpor7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vdYkOT0mC3M"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ewai5vAJmiDT"
      },
      "outputs": [],
      "source": [
        "!git clone -b ML-Manuel https://github.com/ManuelBagasina/DATCapstone.git\n",
        "%cd DATCapstone/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-DXlkU5mkTT"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install pytorch-forecasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUgloDa6mI0a"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQuF70IEml4h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import torch\n",
        "import lightning.pytorch as pl\n",
        "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import MAE, RMSE, SMAPE\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from zipfile import ZipFile\n",
        "from pytorch_forecasting.data.encoders import NaNLabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut93Qa4kmnZb"
      },
      "outputs": [],
      "source": [
        "# Extract and load the dataset\n",
        "with ZipFile('_data.csv.zip', 'r') as z:\n",
        "    with z.open('data.csv') as f:  # Ignore macOS metadata files\n",
        "        df = pd.read_csv(f, index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmLcP5JCmneJ"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkwAzhR4mKhP"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CahXmDimrRV"
      },
      "outputs": [],
      "source": [
        "# Display basic info about the dataset\n",
        "print(f\"Original dataframe shape: {df.shape}\")\n",
        "\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nUnique tickers:\")\n",
        "print(df['Ticker'].unique())\n",
        "print(f\"Number of unique tickers: {df['Ticker'].nunique()}\")\n",
        "\n",
        "# Convert date to datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW4N7e44mLyx"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BsyckKqmtQc"
      },
      "outputs": [],
      "source": [
        "# Remove lag features since TFT will handle time dependencies\n",
        "lag_columns = [col for col in df.columns if '_lag' in col]\n",
        "print(f\"\\nRemoving {len(lag_columns)} lag columns from the dataset\")\n",
        "df_no_lag = df.drop(columns=lag_columns)\n",
        "\n",
        "# Handle embedding columns - Either keep them or use PCA to reduce dimensionality\n",
        "# Identify embedding columns\n",
        "emb_columns = [col for col in df_no_lag.columns if col.startswith('emb_')]\n",
        "print(f\"\\nFound {len(emb_columns)} embedding columns\")\n",
        "\n",
        "# Option 1: Remove embedding columns since they might be too many for TFT\n",
        "df_no_emb = df_no_lag.drop(columns=emb_columns)\n",
        "\n",
        "# We'll work with the version without embeddings for simplicity\n",
        "df_processed = df_no_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5Q3MzBSmuNv"
      },
      "outputs": [],
      "source": [
        "# Select target and features for the model\n",
        "target = 'Close'  # Or could be 'Close' , 'Target_1day', 'Target_1week', 'Target_1month', 'Target_1year'\n",
        "\n",
        "# Select relevant features for prediction\n",
        "# Exclude Date, target variables, and other non-predictive columns\n",
        "exclude_columns = ['Date'] + [col for col in df_processed.columns if col.startswith('Target_')]\n",
        "if target not in exclude_columns:\n",
        "    exclude_columns.append(target)\n",
        "features = [col for col in df_processed.columns if col not in exclude_columns]\n",
        "print(f\"\\nUsing {len(features)} features for prediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK2DYAlFmuTO"
      },
      "outputs": [],
      "source": [
        "# Create time index for TFT\n",
        "df_processed['time_idx'] = df_processed.groupby('Ticker')['Date'].rank(method='dense').astype(int) - 1\n",
        "\n",
        "# Verify time_idx is properly set for each ticker\n",
        "for ticker in df_processed['Ticker'].unique():\n",
        "    ticker_data = df_processed[df_processed['Ticker'] == ticker]\n",
        "    print(f\"{ticker}: time_idx from {ticker_data['time_idx'].min()} to {ticker_data['time_idx'].max()}\")\n",
        "\n",
        "# Set parameters for prediction\n",
        "# If predicting Target_1day, max_prediction_length=1\n",
        "# If predicting Target_1week, max_prediction_length=5 (assuming 5 trading days)\n",
        "# If predicting Target_1month, max_prediction_length=20\n",
        "max_prediction_length = 1  # Adjust based on your prediction horizon\n",
        "max_encoder_length = 30    # Use 30 days of history for prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x5Td_OvmRSh"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxXYx1YkmzBp"
      },
      "outputs": [],
      "source": [
        "# Create a training dataset - Use the last 20% of the data for testing\n",
        "val_cutoff = df_processed['time_idx'].max() - max_prediction_length\n",
        "cutoffs = {}\n",
        "for ticker in df_processed['Ticker'].unique():\n",
        "    ticker_data = df_processed[df_processed['Ticker'] == ticker]\n",
        "    cutoffs[ticker] = ticker_data['time_idx'].max() * 0.8\n",
        "\n",
        "df_processed['is_train'] = True\n",
        "for ticker, cutoff in cutoffs.items():\n",
        "    df_processed.loc[(df_processed['Ticker'] == ticker) &\n",
        "                    (df_processed['time_idx'] > cutoff), 'is_train'] = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kn573usJm1Ex"
      },
      "outputs": [],
      "source": [
        "# Check which columns have missing values\n",
        "missing_columns = df_processed.isna().sum()\n",
        "print(\"\\nColumns with missing values:\")\n",
        "print(missing_columns[missing_columns > 0].sort_values(ascending=False))\n",
        "\n",
        "# Check for infinite values\n",
        "df_processed = df_processed.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Handle missing values in Inventory specifically (since that's causing the error)\n",
        "if 'Inventory' in df_processed.columns:\n",
        "    # For each ticker, fill missing Inventory values with median (or 0 if median is NaN)\n",
        "    for ticker in df_processed['Ticker'].unique():\n",
        "        ticker_mask = df_processed['Ticker'] == ticker\n",
        "        ticker_inventory_median = df_processed.loc[ticker_mask, 'Inventory'].median()\n",
        "        if pd.isna(ticker_inventory_median):\n",
        "            ticker_inventory_median = 0\n",
        "        df_processed.loc[ticker_mask, 'Inventory'] = df_processed.loc[ticker_mask, 'Inventory'].fillna(ticker_inventory_median)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqscoTt6m5rR"
      },
      "outputs": [],
      "source": [
        "# Check all features for missing values and fill appropriately\n",
        "for feature in features:\n",
        "    if df_processed[feature].isna().sum() > 0:\n",
        "        print(f\"Filling missing values in {feature}\")\n",
        "        # Fill by ticker\n",
        "        for ticker in df_processed['Ticker'].unique():\n",
        "            ticker_mask = df_processed['Ticker'] == ticker\n",
        "            feature_median = df_processed.loc[ticker_mask, feature].median()\n",
        "            if pd.isna(feature_median):  # If median is NaN (all values are NaN)\n",
        "                feature_median = 0\n",
        "            df_processed.loc[ticker_mask, feature] = df_processed.loc[ticker_mask, feature].fillna(feature_median)\n",
        "\n",
        "# Verify all missing values are fixed\n",
        "remaining_missing = df_processed[features].isna().sum()\n",
        "if remaining_missing.sum() > 0:\n",
        "    print(\"Warning: There are still missing values:\")\n",
        "    print(remaining_missing[remaining_missing > 0])\n",
        "else:\n",
        "    print(\"All missing values have been handled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke28zgd1mSFR"
      },
      "source": [
        "## Create TimeSeriesDataSets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLM2UhLtm3lN"
      },
      "outputs": [],
      "source": [
        "# Reduced feature set for simplicity\n",
        "reduced_features = features[:20]  # Use only the first 20 features to simplify\n",
        "\n",
        "# Create training dataset\n",
        "training = TimeSeriesDataSet(\n",
        "    data=df_processed[df_processed['is_train']],\n",
        "    time_idx=\"time_idx\",\n",
        "    target=target,\n",
        "    group_ids=[\"Ticker\"],\n",
        "    min_encoder_length=15,  # Reduced from 30\n",
        "    max_encoder_length=15,  # Reduced from 30\n",
        "    min_prediction_length=1,\n",
        "    max_prediction_length=1,\n",
        "    static_categoricals=[\"Ticker\"],\n",
        "    static_reals=[],\n",
        "    time_varying_known_categoricals=[],\n",
        "    time_varying_known_reals=[\"time_idx\"],\n",
        "    time_varying_unknown_categoricals=[],\n",
        "    time_varying_unknown_reals=reduced_features,  # Use reduced feature set\n",
        "    target_normalizer=GroupNormalizer(\n",
        "        groups=[\"Ticker\"], transformation=\"softplus\"\n",
        "    ),\n",
        "    categorical_encoders={\n",
        "        \"Ticker\": NaNLabelEncoder(add_nan=True)\n",
        "    },\n",
        "    add_relative_time_idx=True,\n",
        "    add_target_scales=True,\n",
        "    add_encoder_length=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUx6Zuw7m8QG"
      },
      "outputs": [],
      "source": [
        "# Verify there are no lag features in the data\n",
        "print(\"Lag feature verification:\")\n",
        "lag_features = [col for col in df.columns if '_lag' in col]\n",
        "print(f\"Number of lag features in original dataframe: {len(lag_features)}\")\n",
        "if len(lag_features) == 0:\n",
        "    print(\"No lag features found in dataframe. These will be handled by TFT automatically.\")\n",
        "else:\n",
        "    print(\"Lag features found in dataframe. Consider removing them to let TFT handle temporal dependencies.\")\n",
        "    print(f\"First few lag features: {lag_features[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgl3DdIzm8Ws"
      },
      "outputs": [],
      "source": [
        "# Check ticker sequence organization\n",
        "ticker_counts = df_processed.groupby('Ticker').size()\n",
        "print(\"\\nRows per ticker:\")\n",
        "print(ticker_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EckxLjbRm8Z1"
      },
      "outputs": [],
      "source": [
        "# Check if there are enough data points per ticker for the encoder length\n",
        "min_required = max_encoder_length + max_prediction_length\n",
        "print(f\"\\nTickers with insufficient data (<{min_required} points):\")\n",
        "print(ticker_counts[ticker_counts < min_required])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUQhP-X7m8dl"
      },
      "outputs": [],
      "source": [
        "# Show time index consistency by ticker\n",
        "print(\"\\nTime index range by ticker:\")\n",
        "for ticker in df_processed['Ticker'].unique():\n",
        "    ticker_data = df_processed[df_processed['Ticker'] == ticker]\n",
        "    print(f\"{ticker}: {ticker_data['time_idx'].min()} to {ticker_data['time_idx'].max()} ({len(ticker_data)} rows)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU7WU9F_m8hH"
      },
      "outputs": [],
      "source": [
        "# Handle NaN values in target columns\n",
        "print(f\"Number of rows with NA in {target}: {df_processed[target].isna().sum()}\")\n",
        "df_processed[target] = df_processed[target].fillna(0)  # Fill with 0 or another appropriate value\n",
        "print(f\"NAs remaining in {target}: {df_processed[target].isna().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3iTIeAtm8jz"
      },
      "outputs": [],
      "source": [
        "# Create validation dataset and dataloaders\n",
        "validation = TimeSeriesDataSet.from_dataset(\n",
        "    training, df_processed[~df_processed['is_train']], predict=True, stop_randomization=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZYEMh5-m8pw"
      },
      "outputs": [],
      "source": [
        "# Create dataloaders for model training\n",
        "batch_size = 32  # Adjust based on  GPU memory\n",
        "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0, shuffle=False)\n",
        "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNudr8NLmVdb"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kva-aXg9nHea"
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(42)  # For reproducibility\n",
        "\n",
        "# Create the TFT model properly as a LightningModule\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    learning_rate=0.01,\n",
        "    hidden_size=16,\n",
        "    attention_head_size=2,\n",
        "    dropout=0.1,\n",
        "    hidden_continuous_size=8,\n",
        "    loss=RMSE(),\n",
        "    log_interval=10,\n",
        "    reduce_on_plateau_patience=5,\n",
        ")\n",
        "\n",
        "# Confirm it's a LightningModule\n",
        "print(f\"Is LightningModule: {isinstance(tft, pl.LightningModule)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ph5_b22InMVm"
      },
      "outputs": [],
      "source": [
        "# Configure trainer\n",
        "early_stop_callback = pl.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\"\n",
        ")\n",
        "lr_logger = pl.callbacks.LearningRateMonitor()\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=30,\n",
        "    accelerator='auto',  # Use GPU if available\n",
        "    gradient_clip_val=0.1,\n",
        "    limit_train_batches=30,  # Adjust based on dataset size\n",
        "    callbacks=[early_stop_callback, lr_logger],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VPQqDPrCnNiM"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.fit(\n",
        "    tft,\n",
        "    train_dataloaders=train_dataloader,\n",
        "    val_dataloaders=val_dataloader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9x9I8ylmWQj"
      },
      "source": [
        "## Model Evaluation and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XsNHRFpEnPqT"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the validation set\n",
        "predictions = tft.predict(val_dataloader, return_y=True)\n",
        "\n",
        "# Extract raw predictions and actual values\n",
        "raw_predictions = predictions.output.detach().cpu().numpy()\n",
        "raw_actuals = predictions.y[0].detach().cpu().numpy()\n",
        "\n",
        "# Convert predictions to dataframe for easier analysis\n",
        "pred_df = pd.DataFrame({\n",
        "    'prediction': raw_predictions.flatten(),\n",
        "    'actual': raw_actuals.flatten()\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S0nfc1RXnSB2"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "mae = mean_absolute_error(pred_df['actual'], pred_df['prediction'])\n",
        "mse = mean_squared_error(pred_df['actual'], pred_df['prediction'])\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(pred_df['actual'], pred_df['prediction'])\n",
        "\n",
        "print(\"\\nModel Performance:\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"RÂ²: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5p-nNO4mX65"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z0W8u-ZFnW0u"
      },
      "outputs": [],
      "source": [
        "# Visualize predictions vs actuals\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(pred_df['actual'], pred_df['prediction'], alpha=0.5)\n",
        "plt.plot([pred_df['actual'].min(), pred_df['actual'].max()],\n",
        "       [pred_df['actual'].min(), pred_df['actual'].max()],\n",
        "       'r--', lw=2)\n",
        "plt.xlabel('Actual Close Price')\n",
        "plt.ylabel('Predicted Close Price')\n",
        "plt.title('TFT Model Performance: Actual vs Predicted')\n",
        "plt.grid(True)\n",
        "plt.savefig('tft_performance.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RauV2G5LnZIN"
      },
      "outputs": [],
      "source": [
        "# Visualize predictions vs actuals over time for a specific stock\n",
        "ticker_to_plot = df_processed['Ticker'].unique()[0]  # Choose first ticker\n",
        "ticker_val_data = df_processed[(df_processed['Ticker'] == ticker_to_plot) & (~df_processed['is_train'])].copy()\n",
        "\n",
        "# Plot time series data for the selected ticker\n",
        "if len(ticker_val_data) > 0:\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(ticker_val_data['Date'], ticker_val_data[target], label='Actual')\n",
        "    # You would add predictions here after matching them to dates\n",
        "    plt.title(f'TFT Predictions for {ticker_to_plot}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(target)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'tft_predictions_{ticker_to_plot}.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYdrfvF0mamT"
      },
      "source": [
        "## Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E59W-4NNnjuu"
      },
      "outputs": [],
      "source": [
        "# Get feature importance from the TFT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-ohT9-jnoZG"
      },
      "outputs": [],
      "source": [
        "# Variable importance. Show graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3aq07d8mb8t"
      },
      "source": [
        "## Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc09I8opnn1R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}