{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManuelBagasina/DATCapstone/blob/ML-Manuel/TFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "yAlkT6LS4uQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b ML-Manuel https://github.com/ManuelBagasina/DATCapstone.git\n",
        "%cd DATCapstone/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DiRDaCwJ5wIX",
        "outputId": "fc467eae-8cdd-4f95-bf8d-41458a3ce9aa"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DATCapstone'...\n",
            "remote: Enumerating objects: 647, done.\u001b[K\n",
            "remote: Counting objects: 100% (130/130), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 647 (delta 81), reused 25 (delta 25), pack-reused 517 (from 3)\u001b[K\n",
            "Receiving objects: 100% (647/647), 68.11 MiB | 18.77 MiB/s, done.\n",
            "Resolving deltas: 100% (334/334), done.\n",
            "/content/DATCapstone/data/DATCapstone/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing for Random Forest"
      ],
      "metadata": {
        "id": "K9kW5IIJt5a0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "nNJAPfA64rkG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "\n",
        "# Explicitly extract data.csv from ZIP\n",
        "with ZipFile('_data.csv.zip', 'r') as z:\n",
        "    with z.open('data.csv') as f:  # Ignore macOS metadata files\n",
        "        df = pd.read_csv(f, index_col=0)\n",
        "\n",
        "# Convert date and sort\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values(by=['Ticker', 'Date'])\n"
      ],
      "metadata": {
        "id": "M1I8YmpX50kw"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read Data\n",
        "# df = pd.read_csv('/Users/wonminkim/Projects/DATCapstone/data2/data.csv', index_col=0)\n",
        "# Correct path for ZIP file\n",
        "zip_path = '/content/DATCapstone/data/_data.csv.zip'\n",
        "\n",
        "# Extract specific file from ZIP\n",
        "with ZipFile(zip_path, 'r') as z:\n",
        "    with z.open('data.csv') as f:  # Explicitly specify the file to extract\n",
        "        df = pd.read_csv(f, index_col=0)\n",
        "\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "#Forward fill -> micro, macro, stock / fill na -> sentiment\n",
        "df[df.columns[1:40]] = df.groupby('Ticker')[df.columns[1:40]].ffill()\n",
        "df[df.columns[40:-5]] = df[df.columns[40:-5]].fillna(0)\n",
        "\n",
        "df = df.dropna(subset=df.columns[0:-5])\n",
        "\n",
        "# Sorting\n",
        "df = df.sort_values(by=['Ticker', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# Lag feature (Exclude embedding columns)\n",
        "lag_cols = df.columns[1:44]\n",
        "lags = [1, 3, 5, 7, 14, 30]\n",
        "lag_features = []\n",
        "\n",
        "for col in lag_cols:\n",
        "    for lag in lags:\n",
        "        lagged = df.groupby('Ticker')[col].shift(lag)\n",
        "        lag_features.append(lagged.rename(f'{col}_lag{lag}'))\n",
        "\n",
        "lag_df = pd.concat(lag_features, axis=1)\n",
        "\n",
        "df = pd.concat([df.reset_index(drop=True), lag_df.reset_index(drop=True)], axis=1)"
      ],
      "metadata": {
        "id": "tfXrfT8l50mx"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "_8FssTq-50pJ",
        "outputId": "b707475a-eb9a-4c23-f94f-8a8665ce1417"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Date        Open        High         Low       Close     Volume  \\\n",
              "0    2021-09-30  140.983670  141.690258  138.648004  138.863907   89056700   \n",
              "1    2021-10-01  139.256450  140.257452  136.518433  139.992477   94639600   \n",
              "2    2021-10-04  139.119037  139.560665  135.694064  136.547852   98322000   \n",
              "3    2021-10-05  136.891371  139.590139  136.763788  138.481186   80861100   \n",
              "4    2021-10-06  136.871718  139.501783  135.792205  139.354584   83221100   \n",
              "...         ...         ...         ...         ...         ...        ...   \n",
              "5627 2025-01-27  124.788521  128.388181  116.689260  118.409103  818830900   \n",
              "5628 2025-01-28  121.798801  128.988143  116.239315  128.978149  579666400   \n",
              "5629 2025-01-29  126.488367  126.878331  120.038963  123.688622  467120600   \n",
              "5630 2025-01-30  123.088674  124.988501  118.089134  124.638535  392925500   \n",
              "5631 2025-01-31  123.768618  127.838243  119.179044  120.058960  390372900   \n",
              "\n",
              "      Dividends  Stock Splits  GDP (Billions USD)  Unemployment Rate (%)  ...  \\\n",
              "0           0.0           0.0           23921.991                    4.7  ...   \n",
              "1           0.0           0.0           24777.038                    4.5  ...   \n",
              "2           0.0           0.0           24777.038                    4.5  ...   \n",
              "3           0.0           0.0           24777.038                    4.5  ...   \n",
              "4           0.0           0.0           24777.038                    4.5  ...   \n",
              "...         ...           ...                 ...                    ...  ...   \n",
              "5627        0.0           0.0           29723.864                    4.0  ...   \n",
              "5628        0.0           0.0           29723.864                    4.0  ...   \n",
              "5629        0.0           0.0           29723.864                    4.0  ...   \n",
              "5630        0.0           0.0           29723.864                    4.0  ...   \n",
              "5631        0.0           0.0           29723.864                    4.0  ...   \n",
              "\n",
              "      neutral_lag5  neutral_lag7  neutral_lag14  neutral_lag30  positive_lag1  \\\n",
              "0              NaN           NaN            NaN            NaN            NaN   \n",
              "1              NaN           NaN            NaN            NaN       0.000000   \n",
              "2              NaN           NaN            NaN            NaN       0.000000   \n",
              "3              NaN           NaN            NaN            NaN       0.000000   \n",
              "4              NaN           NaN            NaN            NaN       0.000000   \n",
              "...            ...           ...            ...            ...            ...   \n",
              "5627      0.571282      0.000000       0.999946       0.001172       0.000712   \n",
              "5628      0.083897      0.333974       0.256342       0.007833       0.182257   \n",
              "5629      0.140121      0.571282       0.181732       0.032715       0.119110   \n",
              "5630      0.307131      0.083897       0.476841       0.000166       0.001830   \n",
              "5631      0.373307      0.140121       0.001565       0.565207       0.213932   \n",
              "\n",
              "      positive_lag3  positive_lag5  positive_lag7  positive_lag14  \\\n",
              "0               NaN            NaN            NaN             NaN   \n",
              "1               NaN            NaN            NaN             NaN   \n",
              "2               NaN            NaN            NaN             NaN   \n",
              "3          0.000000            NaN            NaN             NaN   \n",
              "4          0.000000            NaN            NaN             NaN   \n",
              "...             ...            ...            ...             ...   \n",
              "5627       0.053954       0.048698       0.000000        0.000014   \n",
              "5628       0.013230       0.374254       0.000178        0.199403   \n",
              "5629       0.000712       0.053954       0.048698        0.013047   \n",
              "5630       0.182257       0.013230       0.374254        0.153314   \n",
              "5631       0.119110       0.000712       0.053954        0.001855   \n",
              "\n",
              "      positive_lag30  \n",
              "0                NaN  \n",
              "1                NaN  \n",
              "2                NaN  \n",
              "3                NaN  \n",
              "4                NaN  \n",
              "...              ...  \n",
              "5627        0.000014  \n",
              "5628        0.000346  \n",
              "5629        0.000541  \n",
              "5630        0.001441  \n",
              "5631        0.007220  \n",
              "\n",
              "[5632 rows x 1075 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-44ab6f13-75ee-4128-8a86-d0153127bdc3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Dividends</th>\n",
              "      <th>Stock Splits</th>\n",
              "      <th>GDP (Billions USD)</th>\n",
              "      <th>Unemployment Rate (%)</th>\n",
              "      <th>...</th>\n",
              "      <th>neutral_lag5</th>\n",
              "      <th>neutral_lag7</th>\n",
              "      <th>neutral_lag14</th>\n",
              "      <th>neutral_lag30</th>\n",
              "      <th>positive_lag1</th>\n",
              "      <th>positive_lag3</th>\n",
              "      <th>positive_lag5</th>\n",
              "      <th>positive_lag7</th>\n",
              "      <th>positive_lag14</th>\n",
              "      <th>positive_lag30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>140.983670</td>\n",
              "      <td>141.690258</td>\n",
              "      <td>138.648004</td>\n",
              "      <td>138.863907</td>\n",
              "      <td>89056700</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23921.991</td>\n",
              "      <td>4.7</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-10-01</td>\n",
              "      <td>139.256450</td>\n",
              "      <td>140.257452</td>\n",
              "      <td>136.518433</td>\n",
              "      <td>139.992477</td>\n",
              "      <td>94639600</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24777.038</td>\n",
              "      <td>4.5</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-10-04</td>\n",
              "      <td>139.119037</td>\n",
              "      <td>139.560665</td>\n",
              "      <td>135.694064</td>\n",
              "      <td>136.547852</td>\n",
              "      <td>98322000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24777.038</td>\n",
              "      <td>4.5</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-10-05</td>\n",
              "      <td>136.891371</td>\n",
              "      <td>139.590139</td>\n",
              "      <td>136.763788</td>\n",
              "      <td>138.481186</td>\n",
              "      <td>80861100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24777.038</td>\n",
              "      <td>4.5</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-10-06</td>\n",
              "      <td>136.871718</td>\n",
              "      <td>139.501783</td>\n",
              "      <td>135.792205</td>\n",
              "      <td>139.354584</td>\n",
              "      <td>83221100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24777.038</td>\n",
              "      <td>4.5</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5627</th>\n",
              "      <td>2025-01-27</td>\n",
              "      <td>124.788521</td>\n",
              "      <td>128.388181</td>\n",
              "      <td>116.689260</td>\n",
              "      <td>118.409103</td>\n",
              "      <td>818830900</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29723.864</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.571282</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.999946</td>\n",
              "      <td>0.001172</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.053954</td>\n",
              "      <td>0.048698</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5628</th>\n",
              "      <td>2025-01-28</td>\n",
              "      <td>121.798801</td>\n",
              "      <td>128.988143</td>\n",
              "      <td>116.239315</td>\n",
              "      <td>128.978149</td>\n",
              "      <td>579666400</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29723.864</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083897</td>\n",
              "      <td>0.333974</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.007833</td>\n",
              "      <td>0.182257</td>\n",
              "      <td>0.013230</td>\n",
              "      <td>0.374254</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.199403</td>\n",
              "      <td>0.000346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5629</th>\n",
              "      <td>2025-01-29</td>\n",
              "      <td>126.488367</td>\n",
              "      <td>126.878331</td>\n",
              "      <td>120.038963</td>\n",
              "      <td>123.688622</td>\n",
              "      <td>467120600</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29723.864</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.140121</td>\n",
              "      <td>0.571282</td>\n",
              "      <td>0.181732</td>\n",
              "      <td>0.032715</td>\n",
              "      <td>0.119110</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.053954</td>\n",
              "      <td>0.048698</td>\n",
              "      <td>0.013047</td>\n",
              "      <td>0.000541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5630</th>\n",
              "      <td>2025-01-30</td>\n",
              "      <td>123.088674</td>\n",
              "      <td>124.988501</td>\n",
              "      <td>118.089134</td>\n",
              "      <td>124.638535</td>\n",
              "      <td>392925500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29723.864</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.307131</td>\n",
              "      <td>0.083897</td>\n",
              "      <td>0.476841</td>\n",
              "      <td>0.000166</td>\n",
              "      <td>0.001830</td>\n",
              "      <td>0.182257</td>\n",
              "      <td>0.013230</td>\n",
              "      <td>0.374254</td>\n",
              "      <td>0.153314</td>\n",
              "      <td>0.001441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5631</th>\n",
              "      <td>2025-01-31</td>\n",
              "      <td>123.768618</td>\n",
              "      <td>127.838243</td>\n",
              "      <td>119.179044</td>\n",
              "      <td>120.058960</td>\n",
              "      <td>390372900</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29723.864</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.373307</td>\n",
              "      <td>0.140121</td>\n",
              "      <td>0.001565</td>\n",
              "      <td>0.565207</td>\n",
              "      <td>0.213932</td>\n",
              "      <td>0.119110</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.053954</td>\n",
              "      <td>0.001855</td>\n",
              "      <td>0.007220</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5632 rows × 1075 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44ab6f13-75ee-4128-8a86-d0153127bdc3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-44ab6f13-75ee-4128-8a86-d0153127bdc3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-44ab6f13-75ee-4128-8a86-d0153127bdc3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5d9a4370-7a45-418c-b9ff-b40b7ad2974e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5d9a4370-7a45-418c-b9ff-b40b7ad2974e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5d9a4370-7a45-418c-b9ff-b40b7ad2974e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4b627780-b92b-4cf4-9cc9-1565488acf22\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4b627780-b92b-4cf4-9cc9-1565488acf22 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Correct path for article CSVs in Google Colab\n",
        "csv_files = glob.glob('/content/DATCapstone/data/*articles*.csv')\n",
        "print(\"Found files:\", csv_files)  # Debugging\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RJ8eRYXp56z-",
        "outputId": "c3efa3a2-2958-48f6-dca0-fa37446669df"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found files: ['/content/DATCapstone/data/AVGO_articles..csv', '/content/DATCapstone/data/META_articles..csv', '/content/DATCapstone/data/TSLA_articles..csv', '/content/DATCapstone/data/ADBE_articles.csv', '/content/DATCapstone/data/NVDA_articles..csv', '/content/DATCapstone/data/MSFT_articles..csv', '/content/DATCapstone/data/AAPL_articles.csv', '/content/DATCapstone/data/NFLX_articles..csv', '/content/DATCapstone/data/GOOGL_articles..csv', '/content/DATCapstone/data/AMZN_articles..csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/DATCapstone/data/*articles*.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PTEz4GIz57k2",
        "outputId": "ddc30f0d-b343-4edd-84f7-ea91c0a8d089"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DATCapstone/data/AAPL_articles.csv\n",
            "/content/DATCapstone/data/ADBE_articles.csv\n",
            "/content/DATCapstone/data/AMZN_articles..csv\n",
            "/content/DATCapstone/data/AVGO_articles..csv\n",
            "/content/DATCapstone/data/GOOGL_articles..csv\n",
            "/content/DATCapstone/data/META_articles..csv\n",
            "/content/DATCapstone/data/MSFT_articles..csv\n",
            "/content/DATCapstone/data/NFLX_articles..csv\n",
            "/content/DATCapstone/data/NVDA_articles..csv\n",
            "/content/DATCapstone/data/TSLA_articles..csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not csv_files:\n",
        "    print(\"No article files found. Initializing empty columns.\")\n",
        "    df_articles = pd.DataFrame(columns=['Date', 'Ticker', 'Articles_Negative', 'Articles_Neutral', 'Articles_positive'])\n",
        "else:\n",
        "    # Your existing processing code\n",
        "    df_articles_list = []\n",
        "    for file in csv_files:\n",
        "        ticker = os.path.basename(file).split('_')[0]\n",
        "        df_articles = pd.read_csv(file)\n",
        "        df_articles['Ticker'] = ticker\n",
        "        df_articles_list.append(df_articles)\n",
        "\n",
        "    df_articles = pd.concat(df_articles_list, ignore_index=True)\n",
        "\n",
        "    # Preprocessing steps...\n",
        "# Articles dataframe preprocessing\n",
        "df_articles[\"Date\"] = pd.to_datetime(df_articles[\"time\"], errors='coerce', utc=True).dt.tz_convert(None).dt.date\n",
        "df_articles = df_articles[['Date', 'Ticker', 'sentiment']]\n",
        "sentiment_dummies = pd.get_dummies(df_articles['sentiment'])\n",
        "df_encoded = pd.concat([df_articles[['Date', 'Ticker']], sentiment_dummies], axis=1)\n",
        "df_articles = df_encoded.groupby(['Date', 'Ticker']).sum().reset_index()\n",
        "df_articles['Date'] = pd.to_datetime(df_articles['Date'])\n",
        "\n",
        "df_articles['articles_avg_sentiment'] = (df_articles['Negative'] * -1 + df_articles['Neutral'] * 0 + df_articles['Positive']) / (df_articles['Negative'] + df_articles['Neutral'] + df_articles['Positive'])\n",
        "df_articles.columns = ['Date', 'Ticker', 'Articles_Negative', 'Articles_Neutral', 'Articles_positive', 'Articles_avg_sentiment']\n",
        "df_articles\n",
        "\n",
        "df = pd.merge(df, df_articles, on=['Date', 'Ticker'], how = 'left')\n",
        "\n",
        "# Fill 0 except the target columns\n",
        "df[[col for col in df.columns if 'Target' not in col]] = df[[col for col in df.columns if 'Target' not in col]].fillna(0)"
      ],
      "metadata": {
        "id": "Ax6G1CGe57sO"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# Read articles df\n",
        "csv_files = glob.glob(os.path.join(\"*_articles*.csv\")) # Updated to reflect file naming pattern\n",
        "\n",
        "df_articles_list = []\n",
        "\n",
        "for file in csv_files:\n",
        "    # get ticker from filename\n",
        "    ticker = os.path.basename(file).split('_')[0]\n",
        "    df_articles = pd.read_csv(file)\n",
        "    df_articles['Ticker'] = ticker\n",
        "    df_articles_list.append(df_articles)\n",
        "\n",
        "# concat articles dataframe\n",
        "df_articles = pd.concat(df_articles_list, ignore_index=True)\n",
        "\n",
        "# Articles dataframe preprocessing\n",
        "df_articles[\"Date\"] = pd.to_datetime(df_articles[\"time\"], errors='coerce', utc=True).dt.tz_convert(None).dt.date\n",
        "df_articles = df_articles[['Date', 'Ticker', 'sentiment']]\n",
        "sentiment_dummies = pd.get_dummies(df_articles['sentiment'])\n",
        "df_encoded = pd.concat([df_articles[['Date', 'Ticker']], sentiment_dummies], axis=1)\n",
        "df_articles = df_encoded.groupby(['Date', 'Ticker']).sum().reset_index()\n",
        "df_articles['Date'] = pd.to_datetime(df_articles['Date'])\n",
        "\n",
        "df_articles['articles_avg_sentiment'] = (df_articles['Negative'] * -1 + df_articles['Neutral'] * 0 + df_articles['Positive']) / (df_articles['Negative'] + df_articles['Neutral'] + df_articles['Positive'])\n",
        "df_articles.columns = ['Date', 'Ticker', 'Articles_Negative', 'Articles_Neutral', 'Articles_positive', 'Articles_avg_sentiment']\n",
        "df_articles\n",
        "\n",
        "df = pd.merge(df, df_articles, on=['Date', 'Ticker'], how = 'left')\n",
        "\n",
        "# Fill 0 except the target columns\n",
        "df[[col for col in df.columns if 'Target' not in col]] = df[[col for col in df.columns if 'Target' not in col]].fillna(0)"
      ],
      "metadata": {
        "id": "JI8knn7E6EWG"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_articles.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BLvjoBL26Edm",
        "outputId": "eb992571-a0f5-40bd-c37c-134805175f02"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Date', 'Ticker', 'Articles_Negative', 'Articles_Neutral',\n",
            "       'Articles_positive', 'Articles_avg_sentiment'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "emb_cols = [col for col in df.columns if col.startswith(\"emb_\")]\n",
        "X_emb = df[emb_cols]\n",
        "X_emb_pca = pca.fit_transform(X_emb)\n",
        "\n",
        "# PCA columns\n",
        "pca_columns = [f\"emb_pca_{i}\" for i in range(X_emb_pca.shape[1])]\n",
        "df_pca_part = pd.DataFrame(X_emb_pca, columns=pca_columns, index=df.index)\n",
        "df_non_pca = df.drop(columns=emb_cols)\n",
        "\n",
        "df_pca = pd.concat([df_non_pca.reset_index(drop=True), df_pca_part.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# 결과 확인\n",
        "df_pca.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "wUNkffOZ6EhB",
        "outputId": "6bc76b74-5af6-4bde-d3f7-0e02849abbec"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Date        Open        High         Low       Close    Volume  \\\n",
              "0 2021-09-30  140.983670  141.690258  138.648004  138.863907  89056700   \n",
              "1 2021-10-01  139.256450  140.257452  136.518433  139.992477  94639600   \n",
              "2 2021-10-04  139.119037  139.560665  135.694064  136.547852  98322000   \n",
              "3 2021-10-05  136.891371  139.590139  136.763788  138.481186  80861100   \n",
              "4 2021-10-06  136.871718  139.501783  135.792205  139.354584  83221100   \n",
              "\n",
              "   Dividends  Stock Splits  GDP (Billions USD)  Unemployment Rate (%)  ...  \\\n",
              "0        0.0           0.0           23921.991                    4.7  ...   \n",
              "1        0.0           0.0           24777.038                    4.5  ...   \n",
              "2        0.0           0.0           24777.038                    4.5  ...   \n",
              "3        0.0           0.0           24777.038                    4.5  ...   \n",
              "4        0.0           0.0           24777.038                    4.5  ...   \n",
              "\n",
              "   emb_pca_74  emb_pca_75  emb_pca_76  emb_pca_77  emb_pca_78  emb_pca_79  \\\n",
              "0   -0.000573   -0.000315   -0.000716   -0.000052    0.000158    0.000176   \n",
              "1   -0.000573   -0.000315   -0.000716   -0.000052    0.000158    0.000176   \n",
              "2   -0.000573   -0.000315   -0.000716   -0.000052    0.000158    0.000176   \n",
              "3   -0.000573   -0.000315   -0.000716   -0.000052    0.000158    0.000176   \n",
              "4   -0.000573   -0.000315   -0.000716   -0.000052    0.000158    0.000176   \n",
              "\n",
              "   emb_pca_80  emb_pca_81  emb_pca_82  emb_pca_83  \n",
              "0    0.000412   -0.000354   -0.000744    0.000022  \n",
              "1    0.000412   -0.000354   -0.000744    0.000022  \n",
              "2    0.000412   -0.000354   -0.000744    0.000022  \n",
              "3    0.000412   -0.000354   -0.000744    0.000022  \n",
              "4    0.000412   -0.000354   -0.000744    0.000022  \n",
              "\n",
              "[5 rows x 399 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c787fdeb-d756-4969-b1d6-7e8025dc41e4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Dividends</th>\n",
              "      <th>Stock Splits</th>\n",
              "      <th>GDP (Billions USD)</th>\n",
              "      <th>Unemployment Rate (%)</th>\n",
              "      <th>...</th>\n",
              "      <th>emb_pca_74</th>\n",
              "      <th>emb_pca_75</th>\n",
              "      <th>emb_pca_76</th>\n",
              "      <th>emb_pca_77</th>\n",
              "      <th>emb_pca_78</th>\n",
              "      <th>emb_pca_79</th>\n",
              "      <th>emb_pca_80</th>\n",
              "      <th>emb_pca_81</th>\n",
              "      <th>emb_pca_82</th>\n",
              "      <th>emb_pca_83</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>140.983670</td>\n",
              "      <td>141.690258</td>\n",
              "      <td>138.648004</td>\n",
              "      <td>138.863907</td>\n",
              "      <td>89056700</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23921.991</td>\n",
              "      <td>4.7</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000573</td>\n",
              "      <td>-0.000315</td>\n",
              "      <td>-0.000716</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.000354</td>\n",
              "      <td>-0.000744</td>\n",
              "      <td>0.000022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-10-01</td>\n",
              "      <td>139.256450</td>\n",
              "      <td>140.257452</td>\n",
              "      <td>136.518433</td>\n",
              "      <td>139.992477</td>\n",
              "      <td>94639600</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24777.038</td>\n",
              "      <td>4.5</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000573</td>\n",
              "      <td>-0.000315</td>\n",
              "      <td>-0.000716</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.000354</td>\n",
              "      <td>-0.000744</td>\n",
              "      <td>0.000022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-10-04</td>\n",
              "      <td>139.119037</td>\n",
              "      <td>139.560665</td>\n",
              "      <td>135.694064</td>\n",
              "      <td>136.547852</td>\n",
              "      <td>98322000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24777.038</td>\n",
              "      <td>4.5</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000573</td>\n",
              "      <td>-0.000315</td>\n",
              "      <td>-0.000716</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.000354</td>\n",
              "      <td>-0.000744</td>\n",
              "      <td>0.000022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-10-05</td>\n",
              "      <td>136.891371</td>\n",
              "      <td>139.590139</td>\n",
              "      <td>136.763788</td>\n",
              "      <td>138.481186</td>\n",
              "      <td>80861100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24777.038</td>\n",
              "      <td>4.5</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000573</td>\n",
              "      <td>-0.000315</td>\n",
              "      <td>-0.000716</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.000354</td>\n",
              "      <td>-0.000744</td>\n",
              "      <td>0.000022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-10-06</td>\n",
              "      <td>136.871718</td>\n",
              "      <td>139.501783</td>\n",
              "      <td>135.792205</td>\n",
              "      <td>139.354584</td>\n",
              "      <td>83221100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24777.038</td>\n",
              "      <td>4.5</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000573</td>\n",
              "      <td>-0.000315</td>\n",
              "      <td>-0.000716</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.000354</td>\n",
              "      <td>-0.000744</td>\n",
              "      <td>0.000022</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 399 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c787fdeb-d756-4969-b1d6-7e8025dc41e4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c787fdeb-d756-4969-b1d6-7e8025dc41e4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c787fdeb-d756-4969-b1d6-7e8025dc41e4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9cdb6cbd-4b9b-473e-be48-03ba80c374ad\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9cdb6cbd-4b9b-473e-be48-03ba80c374ad')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9cdb6cbd-4b9b-473e-be48-03ba80c374ad button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_pca"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing for TFT"
      ],
      "metadata": {
        "id": "r27fmp0Qt-W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing Helper for TFT Model\n",
        "# This script helps prepare your data for the TFT model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def preprocess_data_for_tft(df, target_col='Close', prediction_horizon=1,\n",
        "                           encoder_length=30, use_embeddings=False,\n",
        "                           pca_components=10, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Preprocess data for the Temporal Fusion Transformer model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        The input dataframe containing stock data\n",
        "    target_col : str\n",
        "        Column name to predict (default: 'Close')\n",
        "    prediction_horizon : int\n",
        "        Number of days ahead to predict (default: 1)\n",
        "    encoder_length : int\n",
        "        Number of days to use as context for prediction (default: 30)\n",
        "    use_embeddings : bool\n",
        "        Whether to keep embedding columns (default: False)\n",
        "    pca_components : int\n",
        "        Number of PCA components if reducing embeddings (default: 10)\n",
        "    test_size : float\n",
        "        Fraction of data to use for testing (default: 0.2)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        Processed dataframe ready for TFT model\n",
        "    \"\"\"\n",
        "    print(f\"Input dataframe shape: {df.shape}\")\n",
        "\n",
        "    # Ensure Date is datetime\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    # Sort by Ticker and Date\n",
        "    if 'Ticker' in df.columns:\n",
        "        df = df.sort_values(['Ticker', 'Date'])\n",
        "    else:\n",
        "        raise ValueError(\"DataFrame must have a 'Ticker' column\")\n",
        "\n",
        "    # Remove lag features as TFT handles time dependencies\n",
        "    lag_columns = [col for col in df.columns if '_lag' in col]\n",
        "    print(f\"Removing {len(lag_columns)} lag columns\")\n",
        "    df_no_lag = df.drop(columns=lag_columns)\n",
        "\n",
        "    # Handle embedding columns\n",
        "    emb_columns = [col for col in df_no_lag.columns if col.startswith('emb_')]\n",
        "    print(f\"Found {len(emb_columns)} embedding columns\")\n",
        "\n",
        "    if len(emb_columns) > 0:\n",
        "        if not use_embeddings:\n",
        "            # Option 1: Remove embeddings\n",
        "            df_processed = df_no_lag.drop(columns=emb_columns)\n",
        "            print(\"Embeddings removed\")\n",
        "        else:\n",
        "            # Option 2: Reduce dimensions with PCA\n",
        "            print(f\"Reducing embeddings to {pca_components} components with PCA\")\n",
        "            # Check for null values in embedding columns\n",
        "            null_counts = df_no_lag[emb_columns].isnull().sum()\n",
        "            if null_counts.sum() > 0:\n",
        "                print(f\"Warning: Found {null_counts.sum()} null values in embedding columns\")\n",
        "                # Fill nulls with 0 for PCA\n",
        "                df_no_lag[emb_columns] = df_no_lag[emb_columns].fillna(0)\n",
        "\n",
        "            # Apply PCA\n",
        "            emb_data = df_no_lag[emb_columns].values\n",
        "            pca = PCA(n_components=pca_components)\n",
        "            emb_pca = pca.fit_transform(emb_data)\n",
        "\n",
        "            # Create new dataframe with PCA components\n",
        "            df_processed = df_no_lag.drop(columns=emb_columns)\n",
        "            for i in range(pca_components):\n",
        "                df_processed[f'emb_pca_{i}'] = emb_pca[:, i]\n",
        "\n",
        "            # Print variance explained\n",
        "            var_explained = pca.explained_variance_ratio_.sum()\n",
        "            print(f\"PCA captures {var_explained:.2%} of embedding variance\")\n",
        "    else:\n",
        "        df_processed = df_no_lag\n",
        "\n",
        "    # Create time index for TFT\n",
        "    df_processed['time_idx'] = df_processed.groupby('Ticker')['Date'].rank(method='dense').astype(int) - 1\n",
        "\n",
        "    # Create train/test split\n",
        "    cutoffs = {}\n",
        "    for ticker in df_processed['Ticker'].unique():\n",
        "        ticker_data = df_processed[df_processed['Ticker'] == ticker]\n",
        "        cutoffs[ticker] = ticker_data['time_idx'].max() * (1 - test_size)\n",
        "\n",
        "    df_processed['is_train'] = True\n",
        "    for ticker, cutoff in cutoffs.items():\n",
        "        df_processed.loc[(df_processed['Ticker'] == ticker) &\n",
        "                        (df_processed['time_idx'] > cutoff), 'is_train'] = False\n",
        "\n"
      ],
      "metadata": {
        "id": "y_wrDn2buAuj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TFT Model 1 (Obsolete)"
      ],
      "metadata": {
        "id": "jWhDAp1m6Ooe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "LC36xZJc6v0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "!pip install pytorch-lightning\n",
        "!pip install pytorch-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MX4VTNzX57xX",
        "outputId": "cde89f73-7521-4731-8d5e-24635e5bf7be"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.13.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.14.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Requirement already satisfied: pytorch-forecasting in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy<=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (2.0.2)\n",
            "Requirement already satisfied: torch!=2.0.1,<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (2.6.0+cu124)\n",
            "Requirement already satisfied: lightning<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (2.5.1)\n",
            "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (1.14.1)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.2 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (1.6.1)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2025.3.2)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (0.14.3)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (24.2)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.7.1)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.13.1)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.3.0->pytorch-forecasting) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import torch"
      ],
      "metadata": {
        "id": "kw93xdiz50rW"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Lightning and PyTorch Forecasting\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
        "from pytorch_forecasting.metrics import QuantileLoss, MAE, RMSE\n",
        "from pytorch_forecasting.data import GroupNormalizer, NaNLabelEncoder\n",
        "\n",
        "# Scikit-learn for metrics and preprocessing\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "1QFBS2Y56unK"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "2SE-z1t364Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "pl.seed_everything(42)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "SBOUDLnl6upS",
        "outputId": "9bd3cf0a-bc72-48b7-ecb3-66f9bda9fb20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 2. Resolving the NaN/Inf Target Issue ==========\n",
        "# First, inspect Target_1day before any operations\n",
        "print(f\"Original dataframe shape: {df.shape}\")\n",
        "print(f\"Total NaN in Target_1day: {df['Target_1day'].isna().sum()}\")\n",
        "print(f\"Total inf in Target_1day: {np.isinf(df['Target_1day']).sum()}\")\n",
        "\n",
        "# Create a deep copy to avoid modifying the original df\n",
        "working_df = df.copy()\n",
        "\n",
        "# Very aggressive approach to handle Target_1day\n",
        "# 1. First, let's print some statistics about Target_1day\n",
        "target_val = working_df['Target_1day']\n",
        "print(f\"\\nTarget_1day stats before cleaning:\")\n",
        "print(f\"Min: {target_val.min()}\")\n",
        "print(f\"Max: {target_val.max()}\")\n",
        "print(f\"Mean: {target_val.mean()}\")\n",
        "print(f\"Std: {target_val.std()}\")\n",
        "\n",
        "# 2. Replace NaN and inf with 0 in the target column\n",
        "working_df['Target_1day'] = working_df['Target_1day'].replace([np.nan, np.inf, -np.inf], 0)\n",
        "\n",
        "# 3. Verify the target is free of NaN and inf\n",
        "print(f\"\\nAfter replacement:\")\n",
        "print(f\"NaN in Target_1day: {working_df['Target_1day'].isna().sum()}\")\n",
        "print(f\"Inf in Target_1day: {np.isinf(working_df['Target_1day']).sum()}\")\n",
        "\n",
        "# ========== 3. Additional Feature Checks ==========\n",
        "# Create time index for TFT\n",
        "working_df = working_df.sort_values(['Ticker', 'Date'])\n",
        "working_df['time_idx'] = working_df.groupby('Ticker').cumcount()\n",
        "\n",
        "# Define simple feature sets - using only crucial features that don't have NaN issues\n",
        "categorical_vars = ['Ticker']\n",
        "\n",
        "# Focus on a minimal set of relevant financial features\n",
        "basic_features = [\n",
        "    'Dividends', 'Stock Splits',\n",
        "    'GDP (Billions USD)', 'Unemployment Rate (%)',\n",
        "    'Producer Price Index (PPI)', 'Consumer Confidence Index',\n",
        "    'Total Revenue', 'Operating Income', 'Net Income',\n",
        "    'EBITDA', 'Gross Profit'\n",
        "]\n",
        "\n",
        "# Check which features exist and don't have NaN issues\n",
        "valid_features = []\n",
        "for col in basic_features:\n",
        "    if col in working_df.columns:\n",
        "        nan_count = working_df[col].isna().sum()\n",
        "        inf_count = np.isinf(working_df[col]).sum()\n",
        "\n",
        "        if nan_count > 0 or inf_count > 0:\n",
        "            print(f\"Feature {col}: {nan_count} NaNs, {inf_count} infs - filling with 0\")\n",
        "            working_df[col] = working_df[col].replace([np.nan, np.inf, -np.inf], 0)\n",
        "\n",
        "        valid_features.append(col)\n",
        "\n",
        "# Final features\n",
        "continuous_vars = valid_features\n",
        "\n",
        "print(f\"\\nSelected features:\")\n",
        "print(f\"- Categorical ({len(categorical_vars)}): {categorical_vars}\")\n",
        "print(f\"- Continuous ({len(continuous_vars)}): {continuous_vars}\")\n",
        "print(f\"Total features: {len(categorical_vars) + len(continuous_vars)}\")\n"
      ],
      "metadata": {
        "id": "0ZGhU_uv7xhF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3f19d9da-dcbb-4bfc-bdbc-46d7e59e9bc5"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataframe shape: (5632, 1083)\n",
            "Total NaN in Target_1day: 7\n",
            "Total inf in Target_1day: 0\n",
            "\n",
            "Target_1day stats before cleaning:\n",
            "Min: -42.44155883789063\n",
            "Max: 43.88923645019531\n",
            "Mean: 0.10996016370985243\n",
            "Std: 3.587426056358569\n",
            "\n",
            "After replacement:\n",
            "NaN in Target_1day: 0\n",
            "Inf in Target_1day: 0\n",
            "\n",
            "Selected features:\n",
            "- Categorical (1): ['Ticker']\n",
            "- Continuous (11): ['Dividends', 'Stock Splits', 'GDP (Billions USD)', 'Unemployment Rate (%)', 'Producer Price Index (PPI)', 'Consumer Confidence Index', 'Total Revenue', 'Operating Income', 'Net Income', 'EBITDA', 'Gross Profit']\n",
            "Total features: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 4. Create Training and Validation Sets ==========\n",
        "# Set prediction parameters\n",
        "MAX_PREDICTION_LENGTH = 1  # Predict 1 day ahead\n",
        "MAX_ENCODER_LENGTH = 30    # Use 30 days of history for prediction\n",
        "\n",
        "# Split data into train and validation sets\n",
        "cutoff_date = working_df['Date'].max() - timedelta(days=30)\n",
        "train_df = working_df[working_df['Date'] <= cutoff_date].copy()\n",
        "val_df = working_df[working_df['Date'] > cutoff_date].copy()\n",
        "\n",
        "print(f\"\\nTraining data: {train_df.shape} rows, {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
        "print(f\"Validation data: {val_df.shape} rows, {val_df['Date'].min()} to {val_df['Date'].max()}\")\n",
        "\n",
        "# Final verification - ensure there are absolutely no NaN or inf values in any used column\n",
        "TARGET = \"Target_1week\"\n",
        "\n",
        "print(\"\\nFinal verification:\")\n",
        "for col in [TARGET] + continuous_vars:\n",
        "    nan_count = train_df[col].isna().sum()\n",
        "    inf_count = np.isinf(train_df[col]).sum()\n",
        "    if nan_count > 0 or inf_count > 0:\n",
        "        print(f\"Warning: {col} still has {nan_count} NaNs and {inf_count} infs - fixing\")\n",
        "        train_df[col] = train_df[col].replace([np.nan, np.inf, -np.inf], 0)\n",
        "\n",
        "    nan_count = val_df[col].isna().sum()\n",
        "    inf_count = np.isinf(val_df[col]).sum()\n",
        "    if nan_count > 0 or inf_count > 0:\n",
        "        print(f\"Warning: {col} still has {nan_count} NaNs and {inf_count} infs in validation - fixing\")\n",
        "        val_df[col] = val_df[col].replace([np.nan, np.inf, -np.inf], 0)"
      ],
      "metadata": {
        "id": "94O9W_T26utx",
        "outputId": "d30ccb4d-adbe-43e1-a376-6e59f1ea42bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training data: (5492, 1084) rows, 2021-06-30 00:00:00 to 2024-12-31 00:00:00\n",
            "Validation data: (140, 1084) rows, 2025-01-02 00:00:00 to 2025-01-31 00:00:00\n",
            "\n",
            "Final verification:\n",
            "Warning: Target_1week still has 35 NaNs and 0 infs in validation - fixing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "rJz0MhDR9g2B",
        "outputId": "8c433352-a198-4216-8b54-5698028fbc25"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating TimeSeriesDataSet...\n",
            "Error creating TimeSeriesDataSet: 2481 (45.17%) of Target_1week values were found to be NA or infinite (even after encoding). NA values are not allowed `allow_missing_timesteps` refers to missing rows, not to missing values. Possible strategies to fix the issue are (a) dropping the variable Target_1week, (b) using `NaNLabelEncoder(add_nan=True)` for categorical variables, (c) filling missing values and/or (d) optionally adding a variable indicating filled values\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "2481 (45.17%) of Target_1week values were found to be NA or infinite (even after encoding). NA values are not allowed `allow_missing_timesteps` refers to missing rows, not to missing values. Possible strategies to fix the issue are (a) dropping the variable Target_1week, (b) using `NaNLabelEncoder(add_nan=True)` for categorical variables, (c) filling missing values and/or (d) optionally adding a variable indicating filled values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-8f09de7f82ed>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filled_indicator'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     train_dataset = TimeSeriesDataSet(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtime_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"time_idx\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# check that all tensors are finite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m_check_tensors\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m   1492\u001b[0m                         \u001b[0mcheck_for_nonfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m                     \u001b[0mcheck_for_nonfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36mcheck_for_nonfinite\u001b[0;34m(tensor, names)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mna\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0;34mf\"{na} ({na / tensor.size(0):.2%}) of {name} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;34m\"values were found to be NA or infinite (even after encoding). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 2481 (45.17%) of Target_1week values were found to be NA or infinite (even after encoding). NA values are not allowed `allow_missing_timesteps` refers to missing rows, not to missing values. Possible strategies to fix the issue are (a) dropping the variable Target_1week, (b) using `NaNLabelEncoder(add_nan=True)` for categorical variables, (c) filling missing values and/or (d) optionally adding a variable indicating filled values"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 6. Create and Train TFT Model ==========\n",
        "# Create the TFT model\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    train_dataset,\n",
        "    learning_rate=0.001,\n",
        "    hidden_size=32,       # Reduced size to avoid overfitting\n",
        "    attention_head_size=2,\n",
        "    dropout=0.2,          # Increased dropout for regularization\n",
        "    hidden_continuous_size=16,\n",
        "    loss=QuantileLoss(),\n",
        "    log_interval=10,\n",
        "    reduce_on_plateau_patience=3,\n",
        "    weight_decay=1e-2,\n",
        ")\n",
        "\n",
        "print(f\"TFT model size: {tft.size()/1e3:.1f}k parameters\")\n",
        "\n",
        "# Define callbacks\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=1e-4,\n",
        "    patience=5,\n",
        "    verbose=True,\n",
        "    mode=\"min\"\n",
        ")\n",
        "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=15,        # Reduced epochs for faster training\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "    devices=1,\n",
        "    gradient_clip_val=0.1,\n",
        "    callbacks=[early_stop_callback, lr_monitor],\n",
        "    enable_model_summary=True,\n",
        "    limit_train_batches=50,  # For faster training (remove for full training)\n",
        "    limit_val_batches=20,    # For faster training (remove for full training)\n",
        ")\n",
        "\n",
        "print(\"\\nStarting TFT model training...\")\n",
        "trainer.fit(\n",
        "    tft,\n",
        "    train_dataloaders=train_dataloader,\n",
        "    val_dataloaders=val_dataloader,\n",
        ")\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Save the model\n",
        "model_path = \"tft_stock_model.ckpt\"\n",
        "trainer.save_checkpoint(model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ],
      "metadata": {
        "id": "sZIo-lt-9g-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 7. Evaluate Model ==========\n",
        "# Make predictions\n",
        "predictions = tft.predict(\n",
        "    val_dataloader,\n",
        "    return_x=True,\n",
        "    return_y=True,\n",
        "    mode=\"prediction\"\n",
        ")\n",
        "\n",
        "# Extract actual and predicted values\n",
        "actuals = []\n",
        "predicted = []\n",
        "tickers = []\n",
        "\n",
        "# Get validation data\n",
        "for x, y in iter(val_dataloader):\n",
        "    actuals.extend(y[0].cpu().numpy().flatten())\n",
        "    batch_tickers = x[\"groups\"][\"Ticker\"]\n",
        "    tickers.extend(batch_tickers)\n",
        "\n",
        "# Get median predictions (quantile=0.5)\n",
        "predicted = predictions.output.prediction.cpu().numpy()[:, 0, 0]\n",
        "\n",
        "# Ensure we have the same number of predictions as actuals\n",
        "min_len = min(len(actuals), len(predicted), len(tickers))\n",
        "actuals = actuals[:min_len]\n",
        "predicted = predicted[:min_len]\n",
        "tickers = tickers[:min_len]\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame({\n",
        "    \"Ticker\": tickers,\n",
        "    \"Actual\": actuals,\n",
        "    \"Predicted\": predicted,\n",
        "    \"Error\": predicted - actuals\n",
        "})\n",
        "\n",
        "# Calculate metrics\n",
        "mae = mean_absolute_error(actuals, predicted)\n",
        "mse = mean_squared_error(actuals, predicted)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(actuals, predicted)\n",
        "\n",
        "print(\"\\n----- Validation Metrics -----\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "kzJF_leR9hEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 8. Visualize Results ==========\n",
        "# Plot overall results\n",
        "plt.figure(figsize=(12, 6))\n",
        "sample_size = min(100, len(actuals))\n",
        "plt.plot(actuals[:sample_size], label='Actual')\n",
        "plt.plot(predicted[:sample_size], label='Predicted')\n",
        "plt.title(f'Actual vs Predicted {TARGET}')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Price Change')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('tft_prediction_results.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot results by ticker\n",
        "plt.figure(figsize=(15, 10))\n",
        "ticker_list = list(results_df[\"Ticker\"].unique())[:4]  # First 4 tickers\n",
        "for i, ticker in enumerate(ticker_list):\n",
        "    ticker_df = results_df[results_df[\"Ticker\"] == ticker]\n",
        "    if len(ticker_df) > 0:\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        plt.plot(ticker_df[\"Actual\"].values, label=f'Actual ({ticker})')\n",
        "        plt.plot(ticker_df[\"Predicted\"].values, label=f'Predicted ({ticker})')\n",
        "        plt.title(f'Ticker: {ticker}')\n",
        "        plt.xlabel('Sample Index')\n",
        "        plt.ylabel('Price Change')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('tft_ticker_predictions.png')\n",
        "plt.show()\n",
        "\n",
        "# Analyze feature importance\n",
        "print(\"\\nAnalyzing feature importance...\")\n",
        "interpretation = tft.interpret_output(predictions.x, predictions.output)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(15, 10))\n",
        "tft.plot_interpretation(interpretation)\n",
        "plt.tight_layout()\n",
        "plt.savefig('tft_feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# Print top features\n",
        "if \"encoder_variables\" in interpretation:\n",
        "    encoder_vars = tft.encoder_variables\n",
        "    encoder_importance = interpretation[\"encoder_variables\"][0].mean(0).mean(0).cpu().numpy()\n",
        "\n",
        "    print(\"\\nTop most important features:\")\n",
        "    for i, (var, imp) in enumerate(sorted(zip(encoder_vars, encoder_importance),\n",
        "                                      key=lambda x: x[1], reverse=True)[:10]):\n",
        "        print(f\"{i+1}. {var}: {imp:.4f}\")\n",
        "\n",
        "print(\"\\nTFT model evaluation completed!\")"
      ],
      "metadata": {
        "id": "Hb4Lwr3q6uwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Model (To be built)"
      ],
      "metadata": {
        "id": "hszzp5etkt6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup"
      ],
      "metadata": {
        "id": "TjUtu-ArKe4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b ML-Manuel https://github.com/ManuelBagasina/DATCapstone.git\n",
        "%cd DATCapstone/data"
      ],
      "metadata": {
        "id": "H-GPAxnRKeWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install pytorch-forecasting"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OpHWKrhGIsD9",
        "outputId": "bd051f16-50c3-4e1b-ac5f-e5365c13d0ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.13.1)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.1-py3-none-any.whl (822 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-lightning-2.5.1 torchmetrics-1.7.1\n",
            "Collecting pytorch-forecasting\n",
            "  Downloading pytorch_forecasting-1.3.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (2.0.2)\n",
            "Requirement already satisfied: torch!=2.0.1,<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (2.6.0+cu124)\n",
            "Collecting lightning<3.0.0,>=2.0.0 (from pytorch-forecasting)\n",
            "  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (1.14.1)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.2 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (1.6.1)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2025.3.2)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (0.14.3)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (24.2)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.7.1)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.13.1)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.3.0->pytorch-forecasting) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.10)\n",
            "Downloading pytorch_forecasting-1.3.0-py3-none-any.whl (197 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.7/197.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.1-py3-none-any.whl (818 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning, pytorch-forecasting\n",
            "Successfully installed lightning-2.5.1 pytorch-forecasting-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "hSd4IOyGKjQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import MAE, RMSE, SMAPE\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "jFA9BXjNkxqk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "\n",
        "# Explicitly extract data.csv from ZIP\n",
        "with ZipFile('_data.csv.zip', 'r') as z:\n",
        "    with z.open('data.csv') as f:  # Ignore macOS metadata files\n",
        "        df = pd.read_csv(f, index_col=0)\n"
      ],
      "metadata": {
        "id": "zDo82rCAKKhb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5pfJLVPpKOXu",
        "outputId": "4d7168b5-fb0e-4c8a-84e6-3c437d7a0d82"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: False\n",
            "Using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "mvFgaldEKmAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================\n",
        "# Data Loading and Preprocessing\n",
        "# =====================================================================\n",
        "\n",
        "# Load the dataframe\n",
        "print(f\"Original dataframe shape: {df.shape}\")\n",
        "\n",
        "# Basic data exploration\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nUnique tickers:\")\n",
        "print(df['Ticker'].unique())\n",
        "print(f\"Number of unique tickers: {df['Ticker'].nunique()}\")\n",
        "\n",
        "# Convert date to datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "q27p6YXDKQqI",
        "outputId": "e0d56cfe-5834-4f83-d41a-1ed4e0b29935"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataframe shape: (8103, 817)\n",
            "\n",
            "First few rows:\n",
            "        Date       Open       High        Low      Close    Volume  Dividends  \\\n",
            "0 2021-06-01  75.393341  75.630189  74.351213  75.383865  10485300        0.0   \n",
            "1 2021-06-02  75.507015  76.672309  75.327016  76.047028  12249300        0.0   \n",
            "2 2021-06-03  75.601763  77.174428  75.459653  76.823891  12038700        0.0   \n",
            "3 2021-06-04  77.136525  79.542898  77.089160  78.529190  14502900        0.0   \n",
            "4 2021-06-07  78.576573  79.817653  78.491304  79.523964  10445600        0.0   \n",
            "\n",
            "   Stock Splits  GDP (Billions USD)  Unemployment Rate (%)  ...  emb_763  \\\n",
            "0           0.0           23368.861                    5.9  ...      NaN   \n",
            "1           0.0           23368.861                    5.9  ...      NaN   \n",
            "2           0.0           23368.861                    5.9  ...      NaN   \n",
            "3           0.0           23368.861                    5.9  ...      NaN   \n",
            "4           0.0           23368.861                    5.9  ...      NaN   \n",
            "\n",
            "   emb_764  emb_765  emb_766  emb_767  Target_1day  Target_1week  \\\n",
            "0      NaN      NaN      NaN      NaN     0.663162      4.774864   \n",
            "1      NaN      NaN      NaN      NaN     0.776863      4.026428   \n",
            "2      NaN      NaN      NaN      NaN     1.705299      1.146362   \n",
            "3      NaN      NaN      NaN      NaN     0.994774      0.009506   \n",
            "4      NaN      NaN      NaN      NaN     0.634766     -1.250549   \n",
            "\n",
            "   Target_1month  Target_1year  Ticker  \n",
            "0      -1.392677     -6.483879    ORCL  \n",
            "1      -2.302147     -6.070183    ORCL  \n",
            "2      -1.468452     -7.673927    ORCL  \n",
            "3      -1.013695     -9.686905    ORCL  \n",
            "4      -0.814743     -9.200996    ORCL  \n",
            "\n",
            "[5 rows x 817 columns]\n",
            "\n",
            "Unique tickers:\n",
            "['ORCL' 'MSFT' 'AAPL' 'AVGO' 'AMD' 'AMZN' 'GOOGL' 'META' 'TSLA' 'NVDA']\n",
            "Number of unique tickers: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================\n",
        "# TFT Model Preparation\n",
        "# =====================================================================\n",
        "\n",
        "# Remove lag features since TFT will handle time dependencies\n",
        "lag_columns = [col for col in df.columns if '_lag' in col]\n",
        "print(f\"\\nRemoving {len(lag_columns)} lag columns from the dataset\")\n",
        "df_no_lag = df.drop(columns=lag_columns)\n",
        "\n",
        "# Handle embedding columns - Either keep them or use PCA to reduce dimensionality\n",
        "# Identify embedding columns\n",
        "emb_columns = [col for col in df_no_lag.columns if col.startswith('emb_')]\n",
        "print(f\"\\nFound {len(emb_columns)} embedding columns\")\n",
        "\n",
        "# Option 1: Remove embedding columns since they might be too many for TFT\n",
        "df_no_emb = df_no_lag.drop(columns=emb_columns)\n",
        "\n",
        "# We'll work with the version without embeddings for simplicity\n",
        "df_processed = df_no_emb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AuFlLYGMKuE_",
        "outputId": "9f60fcea-2bf5-4699-f55c-55bb591ee518"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Removing 0 lag columns from the dataset\n",
            "\n",
            "Found 768 embedding columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select target and features for the model\n",
        "target = 'Close'  # Or could be 'Target_1day', etc., for direct prediction of price difference\n",
        "\n",
        "# Select relevant features for prediction\n",
        "# Exclude Date, target variables, and other non-predictive columns\n",
        "exclude_columns = ['Date'] + [col for col in df_processed.columns if col.startswith('Target_')]\n",
        "if target not in exclude_columns:\n",
        "    exclude_columns.append(target)\n",
        "\n",
        "features = [col for col in df_processed.columns if col not in exclude_columns]\n",
        "print(f\"\\nUsing {len(features)} features for prediction\")\n",
        "\n",
        "# Prepare data for TFT\n",
        "# Sort data by ticker and date\n",
        "df_processed = df_processed.sort_values(['Ticker', 'Date'])\n",
        "\n",
        "# Calculate the max prediction length based on your targets\n",
        "# If predicting Target_1day, max_prediction_length=1\n",
        "# If predicting Target_1week, max_prediction_length=5 (assuming 5 trading days)\n",
        "# If predicting Target_1month, max_prediction_length=20\n",
        "max_prediction_length = 1  # Adjust based on your prediction horizon\n",
        "\n",
        "# Set the number of days to use as context for the model\n",
        "max_encoder_length = 30  # Use 30 days of history for prediction\n",
        "\n",
        "# Define time index for TFT\n",
        "# We need to create a time_idx column that increases for each date\n",
        "df_processed['time_idx'] = df_processed.groupby('Ticker')['Date'].rank(method='dense').astype(int) - 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ARjatpvbK735",
        "outputId": "b5610aad-2732-4563-cf25-5a4f2d7dbb7d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using 45 features for prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a training dataset\n",
        "# Use the last 20% of the data for testing\n",
        "val_cutoff = df_processed['time_idx'].max() - max_prediction_length\n",
        "cutoffs = {}\n",
        "for ticker in df_processed['Ticker'].unique():\n",
        "    ticker_data = df_processed[df_processed['Ticker'] == ticker]\n",
        "    cutoffs[ticker] = ticker_data['time_idx'].max() * 0.8\n",
        "\n",
        "df_processed['is_train'] = True\n",
        "for ticker, cutoff in cutoffs.items():\n",
        "    df_processed.loc[(df_processed['Ticker'] == ticker) &\n",
        "                    (df_processed['time_idx'] > cutoff), 'is_train'] = False"
      ],
      "metadata": {
        "id": "xkAha4r2LDt1"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which columns have missing values\n",
        "missing_columns = df_processed.isna().sum()\n",
        "print(\"\\nColumns with missing values:\")\n",
        "print(missing_columns[missing_columns > 0].sort_values(ascending=False))\n",
        "\n",
        "# Check for infinite values\n",
        "df_processed = df_processed.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Handle missing values in Inventory specifically (since that's causing the error)\n",
        "if 'Inventory' in df_processed.columns:\n",
        "    # For each ticker, fill missing Inventory values with median (or 0 if median is NaN)\n",
        "    for ticker in df_processed['Ticker'].unique():\n",
        "        ticker_mask = df_processed['Ticker'] == ticker\n",
        "        ticker_inventory_median = df_processed.loc[ticker_mask, 'Inventory'].median()\n",
        "        if pd.isna(ticker_inventory_median):\n",
        "            ticker_inventory_median = 0\n",
        "        df_processed.loc[ticker_mask, 'Inventory'] = df_processed.loc[ticker_mask, 'Inventory'].fillna(ticker_inventory_median)\n",
        "\n",
        "# Check all features for missing values and fill appropriately\n",
        "for feature in features:\n",
        "    if df_processed[feature].isna().sum() > 0:\n",
        "        print(f\"Filling missing values in {feature}\")\n",
        "        # Fill by ticker\n",
        "        for ticker in df_processed['Ticker'].unique():\n",
        "            ticker_mask = df_processed['Ticker'] == ticker\n",
        "            feature_median = df_processed.loc[ticker_mask, feature].median()\n",
        "            if pd.isna(feature_median):  # If median is NaN (all values are NaN)\n",
        "                feature_median = 0\n",
        "            df_processed.loc[ticker_mask, feature] = df_processed.loc[ticker_mask, feature].fillna(feature_median)\n",
        "\n",
        "# Verify all missing values are fixed\n",
        "remaining_missing = df_processed[features].isna().sum()\n",
        "if remaining_missing.sum() > 0:\n",
        "    print(\"Warning: There are still missing values:\")\n",
        "    print(remaining_missing[remaining_missing > 0])\n",
        "else:\n",
        "    print(\"All missing values have been handled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nLXzlWTjQzu9",
        "outputId": "1ae6c2c4-0763-49ff-a8d3-53bf7e17ed7b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Columns with missing values:\n",
            "Target_1year     2500\n",
            "Target_1month     200\n",
            "Target_1week       50\n",
            "Target_1day        10\n",
            "dtype: int64\n",
            "All missing values have been handled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First create the time_idx column\n",
        "df_processed['time_idx'] = df_processed.groupby('Ticker')['Date'].rank(method='dense').astype(int) - 1\n",
        "\n",
        "# Then check the time index range by ticker\n",
        "print(\"\\nTime index range by ticker:\")\n",
        "for ticker in df_processed['Ticker'].unique():\n",
        "    ticker_data = df_processed[df_processed['Ticker'] == ticker]\n",
        "    print(f\"{ticker}: {ticker_data['time_idx'].min()} to {ticker_data['time_idx'].max()} ({len(ticker_data)} rows)\")"
      ],
      "metadata": {
        "id": "EdR1iVa2SFA1",
        "outputId": "62ba6799-814f-452f-ec62-5ac485e19c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Time index range by ticker:\n",
            "AAPL: 0 to 837 (838 rows)\n",
            "AMD: 0 to 773 (774 rows)\n",
            "AMZN: 0 to 773 (774 rows)\n",
            "AVGO: 0 to 815 (816 rows)\n",
            "GOOGL: 0 to 773 (774 rows)\n",
            "META: 0 to 773 (774 rows)\n",
            "MSFT: 0 to 901 (902 rows)\n",
            "NVDA: 0 to 753 (754 rows)\n",
            "ORCL: 0 to 922 (923 rows)\n",
            "TSLA: 0 to 773 (774 rows)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_forecasting.data.encoders import NaNLabelEncoder # Import NaNLabelEncoder"
      ],
      "metadata": {
        "id": "nj_ygVfVMxtP"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before creating the TimeSeriesDataSet, ensure the time_idx is properly set\n",
        "df_processed['time_idx'] = df_processed.groupby('Ticker')['Date'].rank(method='dense').astype(int) - 1\n",
        "\n",
        "# Verify time_idx is properly set for each ticker\n",
        "for ticker in df_processed['Ticker'].unique():\n",
        "    ticker_data = df_processed[df_processed['Ticker'] == ticker]\n",
        "    print(f\"{ticker}: time_idx from {ticker_data['time_idx'].min()} to {ticker_data['time_idx'].max()}\")\n",
        "\n",
        "# Ensure our TimeSeriesDataSet configuration respects ticker boundaries\n",
        "training = TimeSeriesDataSet(\n",
        "    data=df_processed[df_processed['is_train']],\n",
        "    time_idx=\"time_idx\",\n",
        "    target=target,\n",
        "    group_ids=[\"Ticker\"],  # This ensures predictions are made per ticker\n",
        "    min_encoder_length=max_encoder_length // 2,\n",
        "    max_encoder_length=max_encoder_length,\n",
        "    min_prediction_length=1,\n",
        "    max_prediction_length=max_prediction_length,\n",
        "    static_categoricals=[\"Ticker\"],\n",
        "    static_reals=[],\n",
        "    time_varying_known_categoricals=[],\n",
        "    time_varying_known_reals=[\"time_idx\"],\n",
        "    time_varying_unknown_categoricals=[],\n",
        "    time_varying_unknown_reals=features,\n",
        "    target_normalizer=GroupNormalizer(\n",
        "        groups=[\"Ticker\"], transformation=\"softplus\"\n",
        "    ),\n",
        "    categorical_encoders={\n",
        "        \"Ticker\": NaNLabelEncoder(add_nan=True)\n",
        "    },\n",
        "    add_relative_time_idx=True,\n",
        "    add_target_scales=True,\n",
        "    add_encoder_length=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qWSMpoWcLFyq",
        "outputId": "4a3e90c6-ca22-4f6e-8835-9b94c072aff5"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AAPL: time_idx from 0 to 837\n",
            "AMD: time_idx from 0 to 773\n",
            "AMZN: time_idx from 0 to 773\n",
            "AVGO: time_idx from 0 to 815\n",
            "GOOGL: time_idx from 0 to 773\n",
            "META: time_idx from 0 to 773\n",
            "MSFT: time_idx from 0 to 901\n",
            "NVDA: time_idx from 0 to 753\n",
            "ORCL: time_idx from 0 to 922\n",
            "TSLA: time_idx from 0 to 773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if lag features are already in the dataframe\n",
        "print(\"Lag feature verification:\")\n",
        "lag_features = [col for col in df.columns if '_lag' in col]\n",
        "print(f\"Number of lag features in original dataframe: {len(lag_features)}\")\n",
        "\n",
        "# If dataframe doesn't have lag features already, we need to create them\n",
        "if len(lag_features) == 0:\n",
        "    print(\"No lag features found in dataframe. These will be handled by TFT automatically.\")\n",
        "else:\n",
        "    print(\"Lag features found in dataframe. Consider removing them to let TFT handle temporal dependencies.\")\n",
        "    # Optional: remove lag features\n",
        "    print(f\"First few lag features: {lag_features[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DwJyQs0oOfhy",
        "outputId": "046fbe75-9486-4d18-f932-4b7cc034b399"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lag feature verification:\n",
            "Number of lag features in original dataframe: 0\n",
            "No lag features found in dataframe. These will be handled by TFT automatically.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify ticker sequence organization\n",
        "ticker_counts = df.groupby('Ticker').size()\n",
        "print(\"\\nRows per ticker:\")\n",
        "print(ticker_counts)\n",
        "\n",
        "# Check if there are enough data points per ticker for the encoder length\n",
        "min_required = max_encoder_length + max_prediction_length\n",
        "print(f\"\\nTickers with insufficient data (<{min_required} points):\")\n",
        "print(ticker_counts[ticker_counts < min_required])\n",
        "\n",
        "# Show time index consistency by ticker\n",
        "print(\"\\nTime index range by ticker:\")\n",
        "for ticker in df['Ticker'].unique():\n",
        "    ticker_data = df[df['Ticker'] == ticker]\n",
        "    print(f\"{ticker}: {ticker_data['time_idx'].min()} to {ticker_data['time_idx'].max()} ({len(ticker_data)} rows)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "27ViLJF_OZpz",
        "outputId": "4fadb4f0-76b8-4db5-83eb-c95c2bcc723c"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rows per ticker:\n",
            "Ticker\n",
            "AAPL     838\n",
            "AMD      774\n",
            "AMZN     774\n",
            "AVGO     816\n",
            "GOOGL    774\n",
            "META     774\n",
            "MSFT     902\n",
            "NVDA     754\n",
            "ORCL     923\n",
            "TSLA     774\n",
            "dtype: int64\n",
            "\n",
            "Tickers with insufficient data (<31 points):\n",
            "Series([], dtype: int64)\n",
            "\n",
            "Time index range by ticker:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'time_idx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'time_idx'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-b290d7353db7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ticker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mticker_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ticker'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{ticker}: {ticker_data['time_idx'].min()} to {ticker_data['time_idx'].max()} ({len(ticker_data)} rows)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'time_idx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create validation dataset using the same parameters as training\n",
        "validation = TimeSeriesDataSet.from_dataset(\n",
        "    training, df_processed[~df_processed['is_train']], predict=True, stop_randomization=True\n",
        ")\n",
        "\n",
        "# Create dataloaders for model training\n",
        "batch_size = 64  # Adjust based on your GPU memory\n",
        "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
        "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)"
      ],
      "metadata": {
        "id": "iu-EFPc0LKN1"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "apTmGhcTK__4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================\n",
        "# TFT Model Training\n",
        "# =====================================================================\n",
        "\n",
        "# Create the TFT model\n",
        "pl.seed_everything(42)  # For reproducibility"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8GmhOeiZLP7B",
        "outputId": "fa69cf45-921f-4bf0-b099-bb350a1130d6"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pl module wrapper\n",
        "from pytorch_forecasting.models.base_model import BaseModelWithCovariates\n",
        "\n",
        "# Create the TFT model properly as a LightningModule\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=32,\n",
        "    attention_head_size=4,\n",
        "    dropout=0.1,\n",
        "    hidden_continuous_size=16,\n",
        "    loss=RMSE(),\n",
        "    log_interval=10,\n",
        "    reduce_on_plateau_patience=5,\n",
        ")\n",
        "\n",
        "# This is the key part - properly wrap as a LightningModule\n",
        "print(f\"Is LightningModule: {isinstance(tft, pl.LightningModule)}\")\n",
        "\n",
        "# Verify it's now a LightningModule\n",
        "print(f\"Is LightningModule after conversion: {isinstance(tft, pl.LightningModule)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VjTX4dnPLTYj",
        "outputId": "3db137eb-ea66-43c6-914c-3bb3fbd3e5ac"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is LightningModule: False\n",
            "Is LightningModule after conversion: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure trainer\n",
        "early_stop_callback = pl.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\"\n",
        ")\n",
        "lr_logger = pl.callbacks.LearningRateMonitor()\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=30,\n",
        "    accelerator='auto',  # Use GPU if available\n",
        "    gradient_clip_val=0.1,\n",
        "    limit_train_batches=30,  # Adjust based on dataset size\n",
        "    callbacks=[early_stop_callback, lr_logger],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6SvItHTuLVN3",
        "outputId": "37859753-e0bc-462c-933d-4a1de799ef23"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
            "INFO: GPU available: False, used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.fit(\n",
        "    tft,\n",
        "    train_dataloaders=train_dataloader,\n",
        "    val_dataloaders=val_dataloader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "AByzBbTVLXWW",
        "outputId": "df3c6814-b423-4d03-add0-100e24c9735f"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-0dc97f9f372b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m trainer.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \"\"\"\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_unwrap_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0m_verify_strategy_supports_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/compile.py\u001b[0m in \u001b[0;36m_maybe_unwrap_optimized\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0m_check_mixed_imports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;34mf\"`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `{type(model).__qualname__}`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     )\n",
            "\u001b[0;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation and Prediction"
      ],
      "metadata": {
        "id": "ha_v_INgLY9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the validation set\n",
        "predictions = tft.predict(val_dataloader, return_y=True)\n",
        "\n",
        "# Extract raw predictions and actual values\n",
        "raw_predictions = predictions.output.detach().cpu().numpy()\n",
        "raw_actuals = predictions.y.detach().cpu().numpy()\n",
        "\n",
        "# Convert predictions to dataframe for easier analysis\n",
        "pred_df = pd.DataFrame({\n",
        "    'prediction': raw_predictions.flatten(),\n",
        "    'actual': raw_actuals.flatten()\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "a_UnYBpJLcUt",
        "outputId": "7130b90b-27e6-48b0-e250-920f30223847"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
            "INFO: GPU available: False, used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (10x64 and 54x4)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-04d686cdb55e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make predictions on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Extract raw predictions and actual values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mraw_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/models/base_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, mode, return_index, return_decoder_lengths, batch_size, num_workers, fast_dev_run, return_x, return_y, mode_kwargs, trainer_kwargs, write_interval, output_dir, **kwargs)\u001b[0m\n\u001b[1;32m   1724\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pytorch_lightning\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m         \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_dev_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfast_dev_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrainer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1726\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1727\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lightning\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_level_lighting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pytorch_lightning\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_level_pytorch_lightning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m         return call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    888\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_provided\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         )\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/prediction_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;31m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/prediction_loop.py\u001b[0m in \u001b[0;36m_predict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         )\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predict_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstep_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warning_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict returned None if it was on purpose, ignore this warning...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\u001b[0m in \u001b[0;36mpredict_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predict_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/models/base_model.py\u001b[0m in \u001b[0;36mpredict_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    728\u001b[0m         ][0]\n\u001b[1;32m    729\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m  \u001b[0;31m# need to return output to be able to use predict callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/models/base_model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, x, y, batch_idx, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/models/temporal_fusion_transformer/_tft.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    535\u001b[0m             }\n\u001b[1;32m    536\u001b[0m             static_embedding, static_variable_selection = (\n\u001b[0;32m--> 537\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_variable_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m             )\n\u001b[1;32m    539\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;31m# calculate variable weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mflat_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0msparse_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflattened_grn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0msparse_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context, residual)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x64 and 54x4)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics\n",
        "mae = mean_absolute_error(pred_df['actual'], pred_df['prediction'])\n",
        "mse = mean_squared_error(pred_df['actual'], pred_df['prediction'])\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(pred_df['actual'], pred_df['prediction'])"
      ],
      "metadata": {
        "id": "lCYvVI7KLhrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nModel Performance:\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"R²: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "Fs1_GC4NLlul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "_KcR2tYNLnYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize predictions vs actuals\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(pred_df['actual'], pred_df['prediction'], alpha=0.5)\n",
        "plt.plot([pred_df['actual'].min(), pred_df['actual'].max()],\n",
        "         [pred_df['actual'].min(), pred_df['actual'].max()],\n",
        "         'r--', lw=2)\n",
        "plt.xlabel('Actual Close Price')\n",
        "plt.ylabel('Predicted Close Price')\n",
        "plt.title('TFT Model Performance: Actual vs Predicted')\n",
        "plt.grid(True)\n",
        "plt.savefig('tft_performance.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fCTLKMMoLp8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize predictions vs actuals over time for a specific stock\n",
        "ticker_to_plot = df_processed['Ticker'].unique()[0]  # Choose first ticker\n",
        "ticker_val_data = df_processed[(df_processed['Ticker'] == ticker_to_plot) & (~df_processed['is_train'])].copy()\n",
        "\n",
        "# Get predictions for this ticker\n",
        "# This will be more complex and would require matching predictions to the right dates\n",
        "# Here's a simplified approach - you'll need to adapt this to your specific situation\n",
        "if len(ticker_val_data) > 0:\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(ticker_val_data['Date'], ticker_val_data[target], label='Actual')\n",
        "    # You would add predictions here, after matching them to dates\n",
        "    # plt.plot(ticker_val_data['Date'], ticker_predictions, label='Predicted')\n",
        "    plt.title(f'TFT Predictions for {ticker_to_plot}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(target)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'tft_predictions_{ticker_to_plot}.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CqUeeWglLtGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance"
      ],
      "metadata": {
        "id": "2xBlzDCYLwFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importance from the TFT model\n",
        "importances = tft.interpret_output(val_dataloader, reduction=\"mean\")"
      ],
      "metadata": {
        "id": "YlH8EIukLzbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "order = importances.mean(dim=[\"variable\", \"observation\"]).argsort(descending=True)\n",
        "plt.imshow(importances.mean(dim=\"observation\").index_select(0, order), aspect=\"auto\")\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.colorbar()\n",
        "plt.savefig('tft_feature_importance.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "28mgd9hYL1jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFT Model Visualization Toolkit\n",
        "# This script provides visualization tools for analyzing TFT model predictions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import torch\n",
        "from pytorch_forecasting.models.temporal_fusion_transformer.visualization import plot_attention\n",
        "\n",
        "def visualize_predictions(predictions, actuals, dates=None, ticker=None):\n",
        "    \"\"\"\n",
        "    Visualize model predictions vs actual values.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    predictions : array-like\n",
        "        Model predictions\n",
        "    actuals : array-like\n",
        "        Actual values\n",
        "    dates : array-like, optional\n",
        "        Dates corresponding to predictions/actuals\n",
        "    ticker : str, optional\n",
        "        Ticker symbol for the plot title\n",
        "    \"\"\"\n",
        "    # Convert to numpy arrays if needed\n",
        "    if isinstance(predictions, torch.Tensor):\n",
        "        predictions = predictions.detach().cpu().numpy()\n",
        "    if isinstance(actuals, torch.Tensor):\n",
        "        actuals = actuals.detach().cpu().numpy()\n",
        "\n",
        "    # Reshape if needed\n",
        "    if len(predictions.shape) > 1:\n",
        "        predictions = predictions.flatten()\n",
        "    if len(actuals.shape) > 1:\n",
        "        actuals = actuals.flatten()\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(actuals, predictions)\n",
        "\n",
        "    # Create DataFrame for easier plotting\n",
        "    results_df = pd.DataFrame({\n",
        "        'Actual': actuals,\n",
        "        'Predicted': predictions\n",
        "    })\n",
        "\n",
        "    if dates is not None:\n",
        "        results_df['Date'] = dates\n",
        "\n",
        "    # Create plots\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
        "\n",
        "    # Scatter plot\n",
        "    axes[0].scatter(actuals, predictions, alpha=0.5)\n",
        "    min_val = min(actuals.min(), predictions.min())\n",
        "    max_val = max(actuals.max(), predictions.max())\n",
        "    axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
        "    axes[0].set_xlabel('Actual Values')\n",
        "    axes[0].set_ylabel('Predicted Values')\n",
        "    title = \"Prediction Performance\"\n",
        "    if ticker:\n",
        "        title += f\" for {ticker}\"\n",
        "    axes[0].set_title(title)\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Display metrics on the plot\n",
        "    text = f\"MAE: {mae:.4f}\\nMSE: {mse:.4f}\\nRMSE: {rmse:.4f}\\nR²: {r2:.4f}\"\n",
        "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
        "    axes[0].text(0.05, 0.95, text, transform=axes[0].transAxes,\n",
        "                verticalalignment='top', bbox=props)\n",
        "\n",
        "    # Time series plot if dates are provided\n",
        "    if dates is not None:\n",
        "        axes[1].plot(dates, actuals, label='Actual', marker='o', markersize=4)\n",
        "        axes[1].plot(dates, predictions, label='Predicted', marker='x', markersize=4)\n",
        "        axes[1].set_xlabel('Date')\n",
        "        axes[1].set_ylabel('Value')\n",
        "        axes[1].set_title('Time Series Comparison')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True)\n",
        "        plt.xticks(rotation=45)\n",
        "    else:\n",
        "        # Show residuals if dates not available\n",
        "        residuals = actuals - predictions\n",
        "        sns.histplot(residuals, kde=True, ax=axes[1])\n",
        "        axes[1].set_xlabel('Residuals')\n",
        "        axes[1].set_ylabel('Frequency')\n",
        "        axes[1].set_title('Distribution of Residuals')\n",
        "        axes[1].axvline(x=0, color='r', linestyle='--')\n",
        "        axes[1].grid(True)\n",
        "\n",
        "        # Add residual statistics\n",
        "        res_mean = np.mean(residuals)\n",
        "        res_std = np.std(residuals)\n",
        "        text = f\"Mean: {res_mean:.4f}\\nStd Dev: {res_std:.4f}\"\n",
        "        axes[1].text(0.05, 0.95, text, transform=axes[1].transAxes,\n",
        "                    verticalalignment='top', bbox=props)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig, results_df\n",
        "\n",
        "def plot_prediction_by_ticker(df, predictions, actuals, target_col='Close'):\n",
        "    \"\"\"\n",
        "    Plot predictions for each ticker separately.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        Original dataframe with Date and Ticker info\n",
        "    predictions : array-like\n",
        "        Model predictions\n",
        "    actuals : array-like\n",
        "        Actual values\n",
        "    target_col : str\n",
        "        Name of the target column\n",
        "    \"\"\"\n",
        "    # Ensure df has 'is_train' column to identify test set\n",
        "    if 'is_train' not in df.columns:\n",
        "        print(\"Warning: 'is_train' column not found. Can't identify test set.\")\n",
        "        return\n",
        "\n",
        "    # Get test data\n",
        "    test_df = df[~df['is_train']].copy()\n",
        "\n",
        "    # Check if dimensions match\n",
        "    if len(test_df) != len(predictions):\n",
        "        print(f\"Warning: Test set size ({len(test_df)}) doesn't match predictions size ({len(predictions)})\")\n",
        "        print(\"Will try to align data by index.\")\n",
        "\n",
        "        # Try to match predictions to test set\n",
        "        if isinstance(predictions, pd.Series):\n",
        "            test_df['Predicted'] = predictions.values[:len(test_df)]\n",
        "        else:\n",
        "            test_df['Predicted'] = predictions[:len(test_df)]\n",
        "\n",
        "        if isinstance(actuals, pd.Series):\n",
        "            test_df['Actual'] = actuals.values[:len(test_df)]\n",
        "        else:\n",
        "            test_df['Actual'] = actuals[:len(test_df)]\n",
        "    else:\n",
        "        # Add predictions and actuals to test df\n",
        "        test_df['Predicted'] = predictions\n",
        "        test_df['Actual'] = actuals\n",
        "\n",
        "    # Loop through tickers and create plots\n",
        "    for ticker in test_df['Ticker'].unique():\n",
        "        ticker_df = test_df[test_df['Ticker'] == ticker].sort_values('Date')\n",
        "\n",
        "        if len(ticker_df) == 0:\n",
        "            continue\n",
        "\n",
        "        # Get date range for this ticker\n",
        "        start_date = ticker_df['Date'].min()\n",
        "        end_date = ticker_df['Date'].max()\n",
        "\n",
        "        # Get training data for context\n",
        "        train_df = df[(df['Ticker'] == ticker) &\n",
        "                      (df['Date'] < end_date) &\n",
        "                      (df['Date'] >= start_date - pd.Timedelta(days=60))].copy()\n",
        "\n",
        "        # Create combined dataframe for plotting\n",
        "        plot_df = pd.concat([\n",
        "            train_df[['Date', target_col]].assign(Type='Training'),\n",
        "            ticker_df[['Date', 'Actual']].rename(columns={'Actual': target_col}).assign(Type='Actual'),\n",
        "            ticker_df[['Date', 'Predicted']].rename(columns={'Predicted': target_col}).assign(Type='Predicted')\n",
        "        ])\n",
        "\n",
        "        # Calculate metrics for this ticker\n",
        "        ticker_mae = mean_absolute_error(ticker_df['Actual'], ticker_df['Predicted'])\n",
        "        ticker_rmse = np.sqrt(mean_squared_error(ticker_df['Actual'], ticker_df['Predicted']))\n",
        "        ticker_r2 = r2_score(ticker_df['Actual'], ticker_df['Predicted'])\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        sns.lineplot(data=plot_df, x='Date', y=target_col, hue='Type',\n",
        "                    style='Type', markers=True, dashes=False)\n",
        "\n",
        "        plt.title(f\"{ticker} - {target_col} Prediction (MAE: {ticker_mae:.4f}, RMSE: {ticker_rmse:.4f}, R²: {ticker_r2:.4f})\")\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel(target_col)\n",
        "        plt.grid(True)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Also show detailed statistics\n",
        "        print(f\"\\n{ticker} Prediction Performance:\")\n",
        "        print(f\"MAE: {ticker_mae:.4f}\")\n",
        "        print(f\"RMSE: {ticker_rmse:.4f}\")\n",
        "        print(f\"R²: {ticker_r2:.4f}\")\n",
        "\n",
        "        # Show prediction error distribution\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        errors = ticker_df['Actual'] - ticker_df['Predicted']\n",
        "        sns.histplot(errors, kde=True)\n",
        "        plt.title(f\"{ticker} - Prediction Error Distribution\")\n",
        "        plt.xlabel('Error')\n",
        "        plt.axvline(x=0, color='r', linestyle='--')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "def visualize_feature_importance(tft_model, interpretation, top_n=10):\n",
        "    \"\"\"\n",
        "    Visualize feature importance from a trained TFT model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    tft_model : TemporalFusionTransformer\n",
        "        Trained TFT model\n",
        "    interpretation : dict\n",
        "        Output from tft_model.interpret_output()\n",
        "    top_n : int\n",
        "        Number of top features to display\n",
        "    \"\"\"\n",
        "    # Variable selection weights\n",
        "    static_variables = tft_model.static_variables or []\n",
        "    encoder_variables = tft_model.encoder_variables or []\n",
        "    decoder_variables = tft_model.decoder_variables or []\n",
        "\n",
        "    # Variable importance\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Get mean importance across all samples\n",
        "    importance = interpretation[\"importance\"].mean(axis=0)\n",
        "\n",
        "    # Sort and select top features\n",
        "    sorted_idx = importance.argsort(descending=True)\n",
        "    variable_idx = []\n",
        "    variable_names = []\n",
        "\n",
        "    # Get variable names corresponding to indices\n",
        "    for idx in sorted_idx:\n",
        "        var_idx = idx.item()\n",
        "        if var_idx < len(static_variables):\n",
        "            var_name = static_variables[var_idx]\n",
        "        elif var_idx < len(static_variables) + len(encoder_variables):\n",
        "            var_name = encoder_variables[var_idx - len(static_variables)]\n",
        "        else:\n",
        "            var_name = decoder_variables[var_idx - len(static_variables) - len(encoder_variables)]\n",
        "\n",
        "        variable_idx.append(var_idx)\n",
        "        variable_names.append(var_name)\n",
        "\n",
        "    # Select top N features\n",
        "    if len(variable_idx) > top_n:\n",
        "        variable_idx = variable_idx[:top_n]\n",
        "        variable_names = variable_names[:top_n]\n",
        "\n",
        "    # Plot importance for selected features\n",
        "    importance_values = importance[variable_idx].cpu().numpy()\n",
        "    y_pos = np.arange(len(importance_values))\n",
        "\n",
        "    ax.barh(y_pos, importance_values, align='center')\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(variable_names)\n",
        "    ax.invert_yaxis()  # Labels read top-to-bottom\n",
        "    ax.set_xlabel('Importance')\n",
        "    ax.set_title('Feature Importance')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Also visualize attention patterns if available\n",
        "    if \"attention\" in interpretation:\n",
        "        plot_attention(interpretation[\"attention\"], ax=None)\n",
        "        plt.title(\"Attention Patterns\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return variable_names, importance_values\n",
        "\n",
        "def compare_forecasts_across_tickers(df, predictions, actuals, n_tickers=5):\n",
        "    \"\"\"\n",
        "    Compare forecast quality across different tickers.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        Original dataframe with Ticker info\n",
        "    predictions : array-like\n",
        "        Model predictions\n",
        "    actuals : array-like\n",
        "        Actual values\n",
        "    n_tickers : int\n",
        "        Number of tickers to display (default: 5)\n",
        "    \"\"\"\n",
        "    # Ensure df has Ticker column\n",
        "    if 'Ticker' not in df.columns:\n",
        "        print(\"Error: DataFrame must have a 'Ticker' column\")\n",
        "        return\n",
        "\n",
        "    # Ensure df has 'is_train' column to identify test set\n",
        "    if 'is_train' not in df.columns:\n",
        "        print(\"Warning: 'is_train' column not found. Assuming all data is test.\")\n",
        "        test_df = df.copy()\n",
        "    else:\n",
        "        test_df = df[~df['is_train']].copy()\n",
        "\n",
        "    # Check dimensions\n",
        "    if len(test_df) != len(predictions):\n",
        "        print(f\"Warning: Test set size ({len(test_df)}) doesn't match predictions size ({len(predictions)})\")\n",
        "        # Try to match predictions to test set\n",
        "        test_df = test_df.head(len(predictions))\n",
        "\n",
        "    test_df['Predicted'] = predictions\n",
        "    test_df['Actual'] = actuals\n",
        "    test_df['AbsError'] = np.abs(test_df['Actual'] - test_df['Predicted'])\n",
        "    test_df['PercentError'] = 100 * np.abs(test_df['Actual'] - test_df['Predicted']) / test_df['Actual']\n",
        "\n",
        "    # Calculate metrics by ticker\n",
        "    ticker_metrics = []\n",
        "    for ticker in test_df['Ticker'].unique():\n",
        "        ticker_data = test_df[test_df['Ticker'] == ticker]\n",
        "        mae = mean_absolute_error(ticker_data['Actual'], ticker_data['Predicted'])\n",
        "        rmse = np.sqrt(mean_squared_error(ticker_data['Actual'], ticker_data['Predicted']))\n",
        "        mape = np.mean(ticker_data['PercentError'])\n",
        "        r2 = r2_score(ticker_data['Actual'], ticker_data['Predicted'])\n",
        "\n",
        "        ticker_metrics.append({\n",
        "            'Ticker': ticker,\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'MAPE': mape,\n",
        "            'R²': r2,\n",
        "            'Count': len(ticker_data)\n",
        "        })\n",
        "\n",
        "    metrics_df = pd.DataFrame(ticker_metrics).sort_values('MAE')\n",
        "\n",
        "    # Plot comparisons\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "    # MAE by ticker\n",
        "    top_tickers = metrics_df.head(n_tickers)['Ticker'].tolist()\n",
        "    bottom_tickers = metrics_df.tail(n_tickers)['Ticker'].tolist()\n",
        "\n",
        "    # Filter for top and bottom tickers\n",
        "    plot_df = metrics_df[metrics_df['Ticker'].isin(top_tickers + bottom_tickers)]\n",
        "\n",
        "    # MAE plot\n",
        "    sns.barplot(data=plot_df, x='Ticker', y='MAE', ax=axes[0, 0])\n",
        "    axes[0, 0].set_title('MAE by Ticker')\n",
        "    axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=45)\n",
        "\n",
        "    # RMSE plot\n",
        "    sns.barplot(data=plot_df, x='Ticker', y='RMSE', ax=axes[0, 1])\n",
        "    axes[0, 1].set_title('RMSE by Ticker')\n",
        "    axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=45)\n",
        "\n",
        "    # MAPE plot\n",
        "    sns.barplot(data=plot_df, x='Ticker', y='MAPE', ax=axes[1, 0])\n",
        "    axes[1, 0].set_title('MAPE by Ticker (%)')\n",
        "    axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45)\n",
        "\n",
        "    # R² plot\n",
        "    sns.barplot(data=plot_df, x='Ticker', y='R²', ax=axes[1, 1])\n",
        "    axes[1, 1].set_title('R² by Ticker')\n",
        "    axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Return full metrics dataframe\n",
        "    print(\"Full metrics by ticker:\")\n",
        "    return metrics_df\n",
        "\n",
        "def analyze_prediction_errors(df, predictions, actuals):\n",
        "    \"\"\"\n",
        "    Analyze prediction errors to understand model weaknesses.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        Original dataframe with features\n",
        "    predictions : array-like\n",
        "        Model predictions\n",
        "    actuals : array-like\n",
        "        Actual values\n",
        "    \"\"\"\n",
        "    # Prepare data\n",
        "    if 'is_train' not in df.columns:\n",
        "        test_df = df.copy()\n",
        "    else:\n",
        "        test_df = df[~df['is_train']].copy()\n",
        "\n",
        "    # Adjust dimensions if needed\n",
        "    test_df = test_df.head(len(predictions))\n",
        "\n",
        "    # Calculate errors\n",
        "    test_df['Predicted'] = predictions\n",
        "    test_df['Actual'] = actuals\n",
        "    test_df['Error'] = test_df['Actual'] - test_df['Predicted']\n",
        "    test_df['AbsError'] = np.abs(test_df['Error'])\n",
        "    test_df['PercentError'] = 100 * test_df['AbsError'] / np.abs(test_df['Actual'])\n",
        "\n",
        "    # Basic error statistics\n",
        "    print(\"Error Statistics:\")\n",
        "    print(f\"Mean Error: {test_df['Error'].mean():.4f}\")\n",
        "    print(f\"Std Dev of Error: {test_df['Error'].std():.4f}\")\n",
        "    print(f\"Mean Absolute Error: {test_df['AbsError'].mean():.4f}\")\n",
        "    print(f\"Mean Absolute Percentage Error: {test_df['PercentError'].mean():.4f}%\")\n",
        "\n",
        "    # Plot error distribution\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.histplot(test_df['Error'], kde=True)\n",
        "    plt.title('Distribution of Prediction Errors')\n",
        "    plt.xlabel('Error (Actual - Predicted)')\n",
        "    plt.axvline(x=0, color='r', linestyle='--')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Analyze worst predictions\n",
        "    n_worst = min(20, len(test_df))\n",
        "    worst_predictions = test_df.sort_values('AbsError', ascending=False).head(n_worst)\n",
        "\n",
        "    print(f\"\\nWorst {n_worst} Predictions:\")\n",
        "    for idx, row in worst_predictions.iterrows():\n",
        "        print(f\"Ticker: {row.get('Ticker', 'N/A')}, Date: {row.get('Date', 'N/A')}\")\n",
        "        print(f\"  Actual: {row['Actual']:.4f}, Predicted: {row['Predicted']:.4f}\")\n",
        "        print(f\"  Absolute Error: {row['AbsError']:.4f}, Percent Error: {row['PercentError']:.2f}%\")\n",
        "        print()\n",
        "\n",
        "    # Look for patterns in errors\n",
        "    print(\"\\nAnalyzing error patterns...\")\n",
        "\n",
        "    # Error correlation with features\n",
        "    corr_cols = [col for col in test_df.columns if col not in\n",
        "                ['Error', 'AbsError', 'PercentError', 'Predicted', 'Actual', 'Date', 'is_train']]\n",
        "\n",
        "    error_corr = test_df[corr_cols + ['AbsError']].select_dtypes(include=np.number).corr()['AbsError'].sort_values(ascending=False)\n",
        "\n",
        "    print(\"\\nFeatures most correlated with prediction error:\")\n",
        "    print(error_corr.head(10))\n",
        "\n",
        "    # Plot error vs most correlated feature\n",
        "    if len(error_corr) > 0:\n",
        "        top_feature = error_corr.index[0]\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(test_df[top_feature], test_df['AbsError'], alpha=0.5)\n",
        "        plt.title(f'Prediction Error vs {top_feature}')\n",
        "        plt.xlabel(top_feature)\n",
        "        plt.ylabel('Absolute Error')\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Add trend line\n",
        "        from scipy import stats\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
        "            test_df[top_feature].values, test_df['AbsError'].values)\n",
        "        plt.plot(test_df[top_feature], intercept + slope*test_df[top_feature], 'r')\n",
        "        plt.text(0.05, 0.95, f\"r = {r_value:.4f}, p = {p_value:.4f}\",\n",
        "                transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    return test_df[['Ticker', 'Date', 'Actual', 'Predicted', 'Error', 'AbsError', 'PercentError']]"
      ],
      "metadata": {
        "id": "rpDwwpcgMmDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Model"
      ],
      "metadata": {
        "id": "kPbA2yXYL4vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(tft.state_dict(), \"tft_stock_model.pth\")\n",
        "\n",
        "print(\"\\nModel training and evaluation complete. Check the output directory for visualization plots.\")"
      ],
      "metadata": {
        "id": "7AghZ5Y2L58N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "f5YAiVEtUAsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-forecasting>=0.9.0 pytorch-lightning>=1.5.0"
      ],
      "metadata": {
        "id": "1-TzxDwIVqFf"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# SECTION 1: SETUP AND IMPORTS\n",
        "# =================================================================\n",
        "\n",
        "# Install required packages\n",
        "# !pip install pytorch-forecasting>=1.0.0 lightning>=2.0.0\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import torch\n",
        "import lightning.pytorch as pl\n",
        "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import MAE, RMSE, SMAPE\n",
        "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
        "import os\n",
        "import warnings\n",
        "from zipfile import ZipFile\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check package versions\n",
        "def check_versions():\n",
        "    \"\"\"Check package versions for compatibility\"\"\"\n",
        "    import pkg_resources\n",
        "\n",
        "    packages = {\n",
        "        'pytorch-lightning': None,\n",
        "        'lightning': None,  # Newer name for pytorch-lightning\n",
        "        'pytorch-forecasting': None,\n",
        "        'torch': None\n",
        "    }\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            version = pkg_resources.get_distribution(package).version\n",
        "            packages[package] = version\n",
        "            print(f\"{package}: {version}\")\n",
        "        except pkg_resources.DistributionNotFound:\n",
        "            print(f\"{package}: Not installed\")\n",
        "\n",
        "    return packages\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ],
      "metadata": {
        "id": "INVYVuUSUAR0",
        "outputId": "0586eba9-6710-41f8-8eda-859feeda7480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Seed set to 42\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: False\n",
            "Using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# SECTION 2: DATA LOADING AND EXPLORATION\n",
        "# =================================================================\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Load the stock price dataset\"\"\"\n",
        "    try:\n",
        "        # Extract data.csv from ZIP\n",
        "        with ZipFile('_data.csv.zip', 'r') as z:\n",
        "            with z.open('data.csv') as f:  # Ignore macOS metadata files\n",
        "                df = pd.read_csv(f, index_col=0)\n",
        "        print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        # Create a placeholder dataframe for the rest of the code to work\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def explore_data(df):\n",
        "    \"\"\"Explore the dataset and return basic statistics\"\"\"\n",
        "    print(f\"Original dataframe shape: {df.shape}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nUnique tickers:\")\n",
        "    tickers = df['Ticker'].unique()\n",
        "    print(tickers)\n",
        "    print(f\"Number of unique tickers: {df['Ticker'].nunique()}\")\n",
        "\n",
        "    # Convert date to datetime\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    # Check for missing values\n",
        "    missing_values = df.isna().sum()\n",
        "    print(\"\\nColumns with missing values:\")\n",
        "    print(missing_values[missing_values > 0].sort_values(ascending=False))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "1jcpNrm0ZDSC"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# SECTION 3: DATA PREPROCESSING\n",
        "# =================================================================\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Preprocess the dataset for TFT modeling\"\"\"\n",
        "    # Remove lag features since TFT will handle time dependencies\n",
        "    lag_columns = [col for col in df.columns if '_lag' in col]\n",
        "    print(f\"\\nRemoving {len(lag_columns)} lag columns from the dataset\")\n",
        "    df_no_lag = df.drop(columns=lag_columns)\n",
        "\n",
        "    # Handle embedding columns - Either keep them or use PCA to reduce dimensionality\n",
        "    # Identify embedding columns\n",
        "    emb_columns = [col for col in df_no_lag.columns if col.startswith('emb_')]\n",
        "    print(f\"\\nFound {len(emb_columns)} embedding columns\")\n",
        "\n",
        "    # Remove embedding columns since they might be too many for TFT\n",
        "    df_no_emb = df_no_lag.drop(columns=emb_columns)\n",
        "\n",
        "    # We'll work with the version without embeddings for simplicity\n",
        "    df_processed = df_no_emb\n",
        "\n",
        "    # Select target and features for the model\n",
        "    target = 'Close'  # Or could be 'Target_1day', etc., for direct prediction of price difference\n",
        "\n",
        "    # Select relevant features for prediction (excluding Date, target variables, and other non-predictive columns)\n",
        "    exclude_columns = ['Date'] + [col for col in df_processed.columns if col.startswith('Target_')]\n",
        "    if target not in exclude_columns:\n",
        "        exclude_columns.append(target)\n",
        "    features = [col for col in df_processed.columns if col not in exclude_columns]\n",
        "    print(f\"\\nUsing {len(features)} features for prediction\")\n",
        "\n",
        "    # Sort data by ticker and date\n",
        "    df_processed = df_processed.sort_values(['Ticker', 'Date'])\n",
        "\n",
        "    # Define time index for TFT\n",
        "    # Create a time_idx column that increases for each date within each ticker\n",
        "    df_processed['time_idx'] = df_processed.groupby('Ticker')['Date'].rank(method='dense').astype(int) - 1\n",
        "\n",
        "    # Print time index range by ticker\n",
        "    print(\"\\nTime index range by ticker:\")\n",
        "    for ticker in df_processed['Ticker'].unique():\n",
        "        ticker_data = df_processed[df_processed['Ticker'] == ticker]\n",
        "        print(f\"{ticker}: {ticker_data['time_idx'].min()} to {ticker_data['time_idx'].max()} ({len(ticker_data)} rows)\")\n",
        "\n",
        "    return df_processed, features, target\n"
      ],
      "metadata": {
        "id": "P-r8VpKPZDUJ"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# SECTION 4: TRAIN-TEST SPLIT AND FEATURE CLASSIFICATION\n",
        "# =================================================================\n",
        "\n",
        "def create_train_test_split(df_processed, max_prediction_length=1, max_encoder_length=30):\n",
        "    \"\"\"Create training and testing datasets\"\"\"\n",
        "    # Use the last 20% of the data for testing\n",
        "    cutoffs = {}\n",
        "    for ticker in df_processed['Ticker'].unique():\n",
        "        ticker_data = df_processed[df_processed['Ticker'] == ticker]\n",
        "        cutoffs[ticker] = ticker_data['time_idx'].max() * 0.8\n",
        "\n",
        "    df_processed['is_train'] = True\n",
        "    for ticker, cutoff in cutoffs.items():\n",
        "        df_processed.loc[(df_processed['Ticker'] == ticker) &\n",
        "                          (df_processed['time_idx'] > cutoff), 'is_train'] = False\n",
        "\n",
        "    # Handle missing values\n",
        "    df_processed = df_processed.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Check which columns have missing values\n",
        "    missing_columns = df_processed.isna().sum()\n",
        "    columns_with_missing = missing_columns[missing_columns > 0]\n",
        "\n",
        "    if len(columns_with_missing) > 0:\n",
        "        print(\"\\nColumns with missing values:\")\n",
        "        print(columns_with_missing.sort_values(ascending=False))\n",
        "\n",
        "        # Fill missing values by ticker\n",
        "        for feature in df_processed.columns:\n",
        "            if df_processed[feature].isna().sum() > 0:\n",
        "                print(f\"Filling missing values in {feature}\")\n",
        "                for ticker in df_processed['Ticker'].unique():\n",
        "                    ticker_mask = df_processed['Ticker'] == ticker\n",
        "                    feature_median = df_processed.loc[ticker_mask, feature].median()\n",
        "                    if pd.isna(feature_median):  # If median is NaN (all values are NaN)\n",
        "                        feature_median = 0\n",
        "                    df_processed.loc[ticker_mask, feature] = df_processed.loc[ticker_mask, feature].fillna(feature_median)\n",
        "\n",
        "    # Verify all missing values are fixed\n",
        "    remaining_missing = df_processed.isna().sum()\n",
        "    if remaining_missing.sum() > 0:\n",
        "        print(\"Warning: There are still missing values:\")\n",
        "        print(remaining_missing[remaining_missing > 0])\n",
        "    else:\n",
        "        print(\"All missing values have been handled.\")\n",
        "\n",
        "    return df_processed, max_prediction_length, max_encoder_length"
      ],
      "metadata": {
        "id": "0FtFUHT-ZDYo"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_fixed_timeseries_datasets(df_processed, features, target, max_prediction_length, max_encoder_length):\n",
        "    \"\"\"Create TimeSeriesDatasets with fixed parameters to avoid dimension issues\"\"\"\n",
        "    # Calculate appropriate time_varying feature columns\n",
        "    # Sometimes the error happens because features are classified incorrectly\n",
        "    known_reals = [\"time_idx\"]  # Time index is always a known real\n",
        "    unknown_reals = []\n",
        "\n",
        "    # Determine which features should be known vs unknown\n",
        "    # This is important for proper TFT architecture\n",
        "    for feature in features:\n",
        "        if feature in [\"Open\", \"High\", \"Low\", \"Volume\", \"Dividends\", \"Stock Splits\"]:\n",
        "            # These are typically known at prediction time\n",
        "            known_reals.append(feature)\n",
        "        else:\n",
        "            # Other features are unknown future values\n",
        "            unknown_reals.append(feature)\n",
        "\n",
        "    print(f\"\\nFeature allocation:\")\n",
        "    print(f\"Known real features: {len(known_reals)}\")\n",
        "    print(f\"Unknown real features: {len(unknown_reals)}\")\n",
        "\n",
        "    # Include target in unknown_reals if not already there\n",
        "    if target not in unknown_reals and target not in known_reals:\n",
        "        unknown_reals.append(target)\n",
        "\n",
        "    # Create training dataset with explicit feature categorization\n",
        "    training = TimeSeriesDataSet(\n",
        "        data=df_processed[df_processed['is_train']],\n",
        "        time_idx=\"time_idx\",\n",
        "        target=target,\n",
        "        group_ids=[\"Ticker\"],  # This ensures predictions are made per ticker\n",
        "        min_encoder_length=max_encoder_length // 2,\n",
        "        max_encoder_length=max_encoder_length,\n",
        "        min_prediction_length=1,\n",
        "        max_prediction_length=max_prediction_length,\n",
        "        static_categoricals=[\"Ticker\"],\n",
        "        static_reals=[],\n",
        "        time_varying_known_categoricals=[],\n",
        "        time_varying_known_reals=known_reals,\n",
        "        time_varying_unknown_categoricals=[],\n",
        "        time_varying_unknown_reals=unknown_reals,\n",
        "        target_normalizer=GroupNormalizer(\n",
        "            groups=[\"Ticker\"], transformation=\"softplus\"\n",
        "        ),\n",
        "        categorical_encoders={\n",
        "            \"Ticker\": NaNLabelEncoder(add_nan=True)\n",
        "        },\n",
        "        add_relative_time_idx=True,\n",
        "        add_target_scales=True,\n",
        "        add_encoder_length=True,\n",
        "    )\n",
        "\n",
        "    # Create validation dataset using the same parameters as training\n",
        "    validation = TimeSeriesDataSet.from_dataset(\n",
        "        training, df_processed[~df_processed['is_train']], predict=True, stop_randomization=True\n",
        "    )\n",
        "\n",
        "    # Create dataloaders with smaller batch size to avoid dimension issues\n",
        "    batch_size = 32  # Reduced from 64 to help with dimension issues\n",
        "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
        "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "    return training, validation, train_dataloader, val_dataloader"
      ],
      "metadata": {
        "id": "7Zd0y2hbZDiM"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# SECTION 5: MODEL TRAINING\n",
        "# =================================================================\n",
        "\n",
        "def train_tft_model(training, train_dataloader, val_dataloader, max_encoder_length, max_prediction_length, features, max_epochs=30):\n",
        "    \"\"\"Create and train the TFT model with correct dimensions\"\"\"\n",
        "    # Print some diagnostic information\n",
        "    print(\"\\nDiagnostic Information:\")\n",
        "    print(f\"Number of features: {len(features)}\")\n",
        "    print(f\"Max encoder length: {max_encoder_length}\")\n",
        "    print(f\"Max prediction length: {max_prediction_length}\")\n",
        "\n",
        "    # Try to extract the hidden size dynamically based on feature count\n",
        "    # The key is to ensure the dimensions are compatible\n",
        "    feature_count = len(features)\n",
        "\n",
        "    # Calculate appropriate hidden sizes based on feature count\n",
        "    # These formulas help avoid dimension mismatches\n",
        "    hidden_size = min(32, max(16, feature_count // 2))\n",
        "    hidden_continuous_size = min(16, max(8, feature_count // 4))\n",
        "    attention_head_size = min(4, max(1, hidden_size // 8))\n",
        "\n",
        "    print(f\"Calculated hidden_size: {hidden_size}\")\n",
        "    print(f\"Calculated hidden_continuous_size: {hidden_continuous_size}\")\n",
        "    print(f\"Calculated attention_head_size: {attention_head_size}\")\n",
        "\n",
        "    try:\n",
        "        # Create the TFT model with adjusted parameters\n",
        "        # For pytorch-forecasting 1.3.0, remove hidden_layer_size\n",
        "        tft = TemporalFusionTransformer.from_dataset(\n",
        "            training,\n",
        "            learning_rate=0.03,\n",
        "            hidden_size=hidden_size,\n",
        "            attention_head_size=attention_head_size,\n",
        "            dropout=0.1,\n",
        "            hidden_continuous_size=hidden_continuous_size,\n",
        "            loss=RMSE(),\n",
        "            log_interval=10,\n",
        "            reduce_on_plateau_patience=5\n",
        "            # No hidden_layer_size parameter for version 1.3.0\n",
        "        )\n",
        "\n",
        "        # Check if model is a LightningModule\n",
        "        print(f\"Is LightningModule: {isinstance(tft, pl.LightningModule)}\")\n",
        "\n",
        "        # Configure trainer\n",
        "        early_stop_callback = pl.callbacks.EarlyStopping(\n",
        "            monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\"\n",
        "        )\n",
        "        lr_logger = pl.callbacks.LearningRateMonitor()\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=max_epochs,\n",
        "            accelerator='auto',  # Use GPU if available\n",
        "            gradient_clip_val=0.1,\n",
        "            limit_train_batches=30,  # Adjust based on dataset size\n",
        "            callbacks=[early_stop_callback, lr_logger],\n",
        "        )\n",
        "\n",
        "        # Add debugging to inspect batch shapes before training\n",
        "        print(\"\\nInspecting batch shapes for diagnosis:\")\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            if batch_idx == 0:  # Just look at the first batch\n",
        "                # Print the keys and shapes in the batch\n",
        "                print(\"Batch keys and shapes:\")\n",
        "                for key, value in batch.items():\n",
        "                    if isinstance(value, torch.Tensor):\n",
        "                        print(f\"  {key}: {value.shape}\")\n",
        "                    else:\n",
        "                        print(f\"  {key}: {type(value)}\")\n",
        "                break\n",
        "\n",
        "        # Train the model with proper error handling\n",
        "        try:\n",
        "            print(\"Starting model training...\")\n",
        "            trainer.fit(\n",
        "                model=tft,\n",
        "                train_dataloaders=train_dataloader,\n",
        "                val_dataloaders=val_dataloader,\n",
        "            )\n",
        "            print(\"Model training completed successfully!\")\n",
        "            return tft, trainer\n",
        "        except Exception as e:\n",
        "            print(f\"Error during model training: {e}\")\n",
        "\n",
        "            # Additional diagnostics for matrix shape errors\n",
        "            if \"shapes cannot be multiplied\" in str(e):\n",
        "                print(\"\\nMatrix shape mismatch detected. Trying to debug:\")\n",
        "                try:\n",
        "                    # Try to get more detailed error info\n",
        "                    error_parts = str(e).split(\"(\")[1].split(\")\")[0].split(\" and \")\n",
        "                    shape1 = error_parts[0]\n",
        "                    shape2 = error_parts[1]\n",
        "                    print(f\"Shape 1: {shape1}, Shape 2: {shape2}\")\n",
        "\n",
        "                    # Provide guidance based on the shapes\n",
        "                    if \"x\" in shape1 and \"x\" in shape2:\n",
        "                        dims1 = shape1.split(\"x\")\n",
        "                        dims2 = shape2.split(\"x\")\n",
        "\n",
        "                        # Analyze the dimension mismatch\n",
        "                        print(\"\\nDimension analysis:\")\n",
        "                        if len(dims1) >= 2 and len(dims2) >= 2:\n",
        "                            print(f\"- First tensor dimensions: {dims1}\")\n",
        "                            print(f\"- Second tensor dimensions: {dims2}\")\n",
        "                            print(f\"- For matrix multiplication, need {dims1[-1]} to equal {dims2[0]}\")\n",
        "\n",
        "                            # Try to offer specific advice\n",
        "                            if int(dims1[-1]) > int(dims2[0]):\n",
        "                                print(\"\\nPossible solution: Try reducing hidden_size to match tensor dimensions\")\n",
        "                                suggested_size = int(dims2[0])\n",
        "                                print(f\"Suggested hidden_size: {suggested_size}\")\n",
        "                            elif int(dims1[-1]) < int(dims2[0]):\n",
        "                                print(\"\\nPossible solution: Try increasing hidden_size to match tensor dimensions\")\n",
        "                                suggested_size = int(dims2[0])\n",
        "                                print(f\"Suggested hidden_size: {suggested_size}\")\n",
        "                except Exception as debug_error:\n",
        "                    print(f\"Error during debug: {debug_error}\")\n",
        "\n",
        "            # Further debugging for other error types\n",
        "            if \"parameter\" in str(e).lower() or \"argument\" in str(e).lower():\n",
        "                print(\"\\nThis appears to be a parameter/argument error.\")\n",
        "                print(\"Suggestions:\")\n",
        "                print(\"1. Check pytorch-forecasting version compatibility\")\n",
        "                print(\"2. Try simplifying the model configuration\")\n",
        "                print(\"3. Update packages to the most compatible versions\")\n",
        "\n",
        "            return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating TFT model: {e}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "tLyC6BbeZDpw"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tft_model(training, train_dataloader, val_dataloader, max_encoder_length, max_prediction_length, features, max_epochs=30):\n",
        "    \"\"\"Create and train the TFT model with correct dimensions\"\"\"\n",
        "    # [function content as in the fixed version]\n",
        "    # Create the TFT model WITHOUT the hidden_layer_size parameter\n",
        "    tft = TemporalFusionTransformer.from_dataset(\n",
        "        training,\n",
        "        learning_rate=0.03,\n",
        "        hidden_size=16,  # Use smaller fixed size\n",
        "        attention_head_size=2,  # Use smaller fixed size\n",
        "        dropout=0.1,\n",
        "        hidden_continuous_size=8,  # Use smaller fixed size\n",
        "        loss=RMSE(),\n",
        "    )"
      ],
      "metadata": {
        "id": "9Whhkd8adMnM"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# SECTION 6: MODEL EVALUATION\n",
        "# =================================================================\n",
        "\n",
        "def evaluate_model(tft, val_dataloader, df_processed):\n",
        "    \"\"\"Evaluate the trained model and visualize predictions\"\"\"\n",
        "    if tft is None:\n",
        "        print(\"Cannot evaluate model: No trained model available\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Make predictions on the validation set\n",
        "        predictions = tft.predict(val_dataloader, return_y=True)\n",
        "        print(\"Predictions generated successfully!\")\n",
        "\n",
        "        # Extract raw predictions and actual values\n",
        "        raw_predictions = predictions.output.detach().cpu().numpy()\n",
        "        raw_actuals = predictions.y.detach().cpu().numpy()\n",
        "\n",
        "        # Convert predictions to dataframe for easier analysis\n",
        "        pred_df = pd.DataFrame({\n",
        "            'prediction': raw_predictions.flatten(),\n",
        "            'actual': raw_actuals.flatten()\n",
        "        })\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = mean_absolute_error(pred_df['actual'], pred_df['prediction'])\n",
        "        mse = mean_squared_error(pred_df['actual'], pred_df['prediction'])\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(pred_df['actual'], pred_df['prediction'])\n",
        "\n",
        "        print(\"\\nModel Performance:\")\n",
        "        print(f\"MAE: {mae:.4f}\")\n",
        "        print(f\"MSE: {mse:.4f}\")\n",
        "        print(f\"RMSE: {rmse:.4f}\")\n",
        "        print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "        # Visualize predictions vs actuals\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.scatter(pred_df['actual'], pred_df['prediction'], alpha=0.5)\n",
        "        plt.plot([pred_df['actual'].min(), pred_df['actual'].max()],\n",
        "                [pred_df['actual'].min(), pred_df['actual'].max()],\n",
        "                'r--', lw=2)\n",
        "        plt.xlabel('Actual Close Price')\n",
        "        plt.ylabel('Predicted Close Price')\n",
        "        plt.title('TFT Model Performance: Actual vs Predicted')\n",
        "        plt.grid(True)\n",
        "        plt.savefig('tft_performance.png')\n",
        "        plt.show()\n",
        "\n",
        "        # Feature importance\n",
        "        try:\n",
        "            importances = tft.interpret_output(val_dataloader, reduction=\"mean\")\n",
        "            # Variable importance\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            order = importances.mean(dim=[\"variable\", \"observation\"]).argsort(descending=True)\n",
        "            plt.imshow(importances.mean(dim=\"observation\").index_select(0, order), aspect=\"auto\")\n",
        "            plt.title(\"Feature Importance\")\n",
        "            plt.ylabel(\"Feature\")\n",
        "            plt.colorbar()\n",
        "            plt.savefig('tft_feature_importance.png')\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing feature importance: {e}\")\n",
        "\n",
        "        return pred_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model evaluation: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "zhFmZTIXZDyT"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_fixed_tft_workflow():\n",
        "    \"\"\"Run the fixed TFT workflow to handle compatibility issues\"\"\"\n",
        "    # Load and prepare data as before...\n",
        "    # [code for data loading, preprocessing, etc.]\n",
        "\n",
        "    # Check versions\n",
        "    check_versions()\n",
        "\n",
        "    # Load data\n",
        "    df = load_data()\n",
        "\n",
        "    # Check if dataframe is empty\n",
        "    if df.empty:\n",
        "        print(\"Cannot run workflow: Empty dataframe\")\n",
        "        return\n",
        "\n",
        "    # Explore data\n",
        "    df_with_datetime = explore_data(df)\n",
        "\n",
        "    # Preprocess data\n",
        "    df_processed, features, target = preprocess_data(df_with_datetime)\n",
        "\n",
        "    # Create train-test split\n",
        "    max_prediction_length = 1\n",
        "    max_encoder_length = 30\n",
        "\n",
        "    df_processed, max_prediction_length, max_encoder_length = create_train_test_split(\n",
        "        df_processed, max_prediction_length=max_prediction_length, max_encoder_length=max_encoder_length\n",
        "    )\n",
        "\n",
        "    # Create TimeSeriesDatasets\n",
        "    training, validation, train_dataloader, val_dataloader = create_fixed_timeseries_datasets(\n",
        "        df_processed, features, target, max_prediction_length, max_encoder_length\n",
        "    )\n",
        "\n",
        "    # Try the standard approach first\n",
        "    print(\"\\n=== Attempting standard TFT model training ===\")\n",
        "    tft_model, trainer = train_tft_model(\n",
        "        training, train_dataloader, val_dataloader,\n",
        "        max_encoder_length, max_prediction_length, features,\n",
        "        max_epochs=30\n",
        "    )\n",
        "\n",
        "    # If standard approach fails, try the simplified approach\n",
        "    if tft_model is None:\n",
        "        print(\"\\n=== Standard approach failed, trying simplified model ===\")\n",
        "        tft_model, trainer = train_simple_tft_model(\n",
        "            training, train_dataloader, val_dataloader, max_epochs=30\n",
        "        )\n",
        "\n",
        "    # Evaluate model if training succeeded\n",
        "    if tft_model is not None:\n",
        "        predictions_df = evaluate_model(tft_model, val_dataloader, df_processed)\n",
        "        return df_processed, tft_model, predictions_df\n",
        "    else:\n",
        "        print(\"\\nAll attempts failed - no model available for evaluation.\")\n",
        "        print(\"\\nSuggestions for troubleshooting:\")\n",
        "        print(\"1. Try downgrading pytorch-forecasting to version 0.9.0\")\n",
        "        print(\"2. Try using fewer features (reduce dimensionality)\")\n",
        "        print(\"3. Try different batch sizes (16, 8, etc.)\")\n",
        "        print(\"4. Check for incompatibilities between pytorch and pytorch-lightning versions\")\n",
        "        return df_processed, None, None"
      ],
      "metadata": {
        "id": "C_pKtYghc6B4"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# SECTION 7: MAIN EXECUTION WORKFLOW\n",
        "# =================================================================\n",
        "\n",
        "def run_tft_workflow():\n",
        "    \"\"\"Run the complete TFT workflow from data loading to evaluation\"\"\"\n",
        "    # Check versions\n",
        "    check_versions()\n",
        "\n",
        "    # Load data\n",
        "    df = load_data()\n",
        "\n",
        "    # Check if dataframe is empty\n",
        "    if df.empty:\n",
        "        print(\"Cannot run workflow: Empty dataframe\")\n",
        "        return\n",
        "\n",
        "    # Explore data\n",
        "    df_with_datetime = explore_data(df)\n",
        "\n",
        "    # Preprocess data\n",
        "    df_processed, features, target = preprocess_data(df_with_datetime)\n",
        "\n",
        "    # Create train-test split\n",
        "    max_prediction_length = 1  # Keep simple for now\n",
        "    max_encoder_length = 30    # Standard lookback period\n",
        "\n",
        "    df_processed, max_prediction_length, max_encoder_length = create_train_test_split(\n",
        "        df_processed, max_prediction_length=max_prediction_length, max_encoder_length=max_encoder_length\n",
        "    )\n",
        "\n",
        "    # Create TimeSeriesDatasets\n",
        "    training, validation, train_dataloader, val_dataloader = create_fixed_timeseries_datasets(\n",
        "        df_processed, features, target, max_prediction_length, max_encoder_length\n",
        "    )\n",
        "\n",
        "    # Train TFT model\n",
        "    tft_model, trainer = train_tft_model(\n",
        "        training, train_dataloader, val_dataloader,\n",
        "        max_encoder_length, max_prediction_length, features,\n",
        "        max_epochs=30\n",
        "    )\n",
        "\n",
        "    # Evaluate model if training succeeded\n",
        "    if tft_model is not None:\n",
        "        predictions_df = evaluate_model(tft_model, val_dataloader, df_processed)\n",
        "        return df_processed, tft_model, predictions_df\n",
        "    else:\n",
        "        print(\"Workflow completed with errors - no model available for evaluation.\")\n",
        "        return df_processed, None, None\n",
        "\n",
        "\n",
        "# Run the workflow when script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    run_tft_workflow()"
      ],
      "metadata": {
        "id": "TO3qEzpnZcZg",
        "outputId": "4401b4a0-19a0-4063-bc7d-1f53cc03abd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch-lightning: 2.5.1\n",
            "lightning: 2.5.1\n",
            "pytorch-forecasting: 1.3.0\n",
            "torch: 2.6.0+cu124\n",
            "Dataset loaded successfully. Shape: (8103, 817)\n",
            "Original dataframe shape: (8103, 817)\n",
            "\n",
            "First few rows:\n",
            "         Date       Open       High        Low      Close    Volume  \\\n",
            "0  2021-06-01  75.393341  75.630189  74.351213  75.383865  10485300   \n",
            "1  2021-06-02  75.507015  76.672309  75.327016  76.047028  12249300   \n",
            "2  2021-06-03  75.601763  77.174428  75.459653  76.823891  12038700   \n",
            "3  2021-06-04  77.136525  79.542898  77.089160  78.529190  14502900   \n",
            "4  2021-06-07  78.576573  79.817653  78.491304  79.523964  10445600   \n",
            "\n",
            "   Dividends  Stock Splits  GDP (Billions USD)  Unemployment Rate (%)  ...  \\\n",
            "0        0.0           0.0           23368.861                    5.9  ...   \n",
            "1        0.0           0.0           23368.861                    5.9  ...   \n",
            "2        0.0           0.0           23368.861                    5.9  ...   \n",
            "3        0.0           0.0           23368.861                    5.9  ...   \n",
            "4        0.0           0.0           23368.861                    5.9  ...   \n",
            "\n",
            "   emb_763  emb_764  emb_765  emb_766  emb_767  Target_1day  Target_1week  \\\n",
            "0      NaN      NaN      NaN      NaN      NaN     0.663162      4.774864   \n",
            "1      NaN      NaN      NaN      NaN      NaN     0.776863      4.026428   \n",
            "2      NaN      NaN      NaN      NaN      NaN     1.705299      1.146362   \n",
            "3      NaN      NaN      NaN      NaN      NaN     0.994774      0.009506   \n",
            "4      NaN      NaN      NaN      NaN      NaN     0.634766     -1.250549   \n",
            "\n",
            "   Target_1month  Target_1year  Ticker  \n",
            "0      -1.392677     -6.483879    ORCL  \n",
            "1      -2.302147     -6.070183    ORCL  \n",
            "2      -1.468452     -7.673927    ORCL  \n",
            "3      -1.013695     -9.686905    ORCL  \n",
            "4      -0.814743     -9.200996    ORCL  \n",
            "\n",
            "[5 rows x 817 columns]\n",
            "\n",
            "Unique tickers:\n",
            "['ORCL' 'MSFT' 'AAPL' 'AVGO' 'AMD' 'AMZN' 'GOOGL' 'META' 'TSLA' 'NVDA']\n",
            "Number of unique tickers: 10\n",
            "\n",
            "Columns with missing values:\n",
            "emb_767                        5378\n",
            "emb_766                        5378\n",
            "emb_765                        5378\n",
            "emb_764                        5378\n",
            "emb_748                        5378\n",
            "                               ... \n",
            "Repurchase Of Capital Stock     774\n",
            "Long Term Debt                  252\n",
            "Target_1month                   200\n",
            "Target_1week                     50\n",
            "Target_1day                      10\n",
            "Length: 779, dtype: int64\n",
            "\n",
            "Removing 0 lag columns from the dataset\n",
            "\n",
            "Found 768 embedding columns\n",
            "\n",
            "Using 43 features for prediction\n",
            "\n",
            "Time index range by ticker:\n",
            "AAPL: 0 to 837 (838 rows)\n",
            "AMD: 0 to 773 (774 rows)\n",
            "AMZN: 0 to 773 (774 rows)\n",
            "AVGO: 0 to 815 (816 rows)\n",
            "GOOGL: 0 to 773 (774 rows)\n",
            "META: 0 to 773 (774 rows)\n",
            "MSFT: 0 to 901 (902 rows)\n",
            "NVDA: 0 to 753 (754 rows)\n",
            "ORCL: 0 to 922 (923 rows)\n",
            "TSLA: 0 to 773 (774 rows)\n",
            "\n",
            "Columns with missing values:\n",
            "neutral                        5378\n",
            "negative                       5378\n",
            "vote                           5378\n",
            "positive                       5378\n",
            "Target_1year                   2500\n",
            "Inventory                      1697\n",
            "Repurchase Of Capital Stock     774\n",
            "Long Term Debt                  252\n",
            "Target_1month                   200\n",
            "Target_1week                     50\n",
            "Target_1day                      10\n",
            "dtype: int64\n",
            "Filling missing values in Inventory\n",
            "Filling missing values in Long Term Debt\n",
            "Filling missing values in Repurchase Of Capital Stock\n",
            "Filling missing values in vote\n",
            "Filling missing values in negative\n",
            "Filling missing values in neutral\n",
            "Filling missing values in positive\n",
            "Filling missing values in Target_1day\n",
            "Filling missing values in Target_1week\n",
            "Filling missing values in Target_1month\n",
            "Filling missing values in Target_1year\n",
            "All missing values have been handled.\n",
            "\n",
            "Feature allocation:\n",
            "Known real features: 7\n",
            "Unknown real features: 37\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-74a5d190684d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Run the workflow when script is executed directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mrun_tft_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-138-74a5d190684d>\u001b[0m in \u001b[0;36mrun_tft_workflow\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Train TFT model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     tft_model, trainer = train_tft_model(\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mmax_encoder_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_prediction_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    }
  ]
}